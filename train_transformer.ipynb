{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KUMi2gukB8r3",
   "metadata": {
    "id": "KUMi2gukB8r3"
   },
   "source": [
    "# Extreme Model Distillation\n",
    "\n",
    "This notebook aims to understand how well large language model predictions of multiple classes can be approximated by much smaller models. We explore this using Random Forest, KNN, Decoder-Only Transformer, and an RNN model.\n",
    "\n",
    "The data was collected by asking Llama 3.3:8b for 10,000 descriptions of foods. It was then asked to predict the main ingredient, sweetener, fat or oil, seasoning, allergens, contains allergen. That data was used to train the models in this notebook to determine the performance of the KNN and Random Forest models to predict whether or not it contains an allergen based on the main_ingredient, sweetner, fat or oil, and seasoning factors. The Decoder-Only Transformer and RNN models use the tokens created from the food description to predict whether or not an allergen is present\n",
    "\n",
    "We train a Decoder-Only Transformer and RNN model to learn patterns from food descriptions and associated data stored in a CSV file. The notebook performs the following steps:\n",
    "\n",
    "1.  **Imports & Setup:** Imports necessary libraries and checks for the availability of `torch` and `tokenizers`.\n",
    "2.  **Data Creation:** Creates a placeholder `food_predictions.csv` if it doesn't exist, allowing the notebook to run initially.\n",
    "3.  **Tokenizer:** Defines and trains a Byte-Pair Encoding (BPE) tokenizer on the input text data.\n",
    "4.  **Dataset:** Defines a custom dataset class to load, preprocess, and tokenize the data from the CSV.\n",
    "5.  **Model Definition:** Defines the `DecoderOnlyTransformer` architecture and a wrapper class (`DecoderOnlyModelWrapper`) that includes the model, optimizer, and loss function. Also defines the RNN model.\n",
    "6.  **Training Loop:** Implements the training process, including validation, loss calculation, early stopping, and plotting of training/validation loss.\n",
    "7.  **Testing:** Includes several test functions to verify basic model functionality (forward pass, handling different inputs).\n",
    "8.  **Evaluation:** Defines functions to compute and visualize a confusion matrix and calculate precision, recall, and F1-score.\n",
    "9.  **Main Execution:** Orchestrates the entire workflow: loading data, training the tokenizer, initializing the model, running tests, training the model, evaluating performance, and saving the trained model and reports.\n",
    "10. **Logging:** Redirects output to both the console and a `log.txt` file.\n",
    "\n",
    "This notebook also trains a KNN and Random Forest model to compare classification performance as baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EIDhxhvYB8r4",
   "metadata": {
    "id": "EIDhxhvYB8r4"
   },
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Import necessary libraries. We check if `torch` and `tokenizers` are available and set flags accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WJRvLBKjnEXf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJRvLBKjnEXf",
    "outputId": "dd375bfc-8879-4fb3-bbe9-2c93c69910f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.1.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.6.1)\n",
      "Requirement already satisfied: torch in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.21.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: oauth2client in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.1.3)\n",
      "Requirement already satisfied: gdown in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (5.2.0)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.9.5)\n",
      "Requirement already satisfied: lightning-bolts in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tokenizers) (0.27.0)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (0.22.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (0.4.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pytorch-lightning) (1.7.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pytorch-lightning) (0.14.3)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from lightning-bolts) (0.20.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from lightning-bolts) (2.19.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.11.11)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (5.29.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.18.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib scikit-learn torch tokenizers seaborn oauth2client gdown pytorch-lightning lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "MA7mwQdCB8r5",
   "metadata": {
    "id": "MA7mwQdCB8r5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import json # For writing the notebook itself\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Global variables for device and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xh4XkPHgB8r5",
   "metadata": {
    "id": "Xh4XkPHgB8r5"
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "_MS0nLBwB8r6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MS0nLBwB8r6",
    "outputId": "ae45bc96-ac1b-4d6f-dfc4-d0578c122c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found existing 'food_predictions.csv' locally. Using this file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import io\n",
    "\n",
    "csv_file_path = \"food_predictions.csv\"\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    is_colab = True\n",
    "except ImportError:\n",
    "    is_colab = False\n",
    "\n",
    "if is_colab:\n",
    "    # Colab-specific code\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        from google.colab import auth\n",
    "        from oauth2client.client import GoogleCredentials\n",
    "        \n",
    "        # Mount Google Drive\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Authenticate with Google Drive\n",
    "        auth.authenticate_user()\n",
    "        gauth = GoogleCredentials.get_application_default()\n",
    "        \n",
    "        # File ID from Google Drive URL\n",
    "        file_id = '1RFhhiSFwP0s6Y7y4yWCkegXlVaEYqH0B'  # Replace with the actual file ID\n",
    "        drive_file_path = f'/content/drive/MyDrive/food_predictions.csv'  # Update the path if necessary\n",
    "        \n",
    "        # Check if CSV exists locally, or download from Google Drive\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            try:\n",
    "                # Download file from Google Drive\n",
    "                import subprocess\n",
    "                subprocess.run(['gdown', '--id', file_id, '-O', csv_file_path], check=True)\n",
    "                print(f\"[INFO] Downloaded '{csv_file_path}' from Google Drive.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to download from Google Drive: {e}\")\n",
    "                raise FileNotFoundError(f\"Could not download or find '{csv_file_path}'. Please ensure the file ID and path are correct.\")\n",
    "        else:\n",
    "            print(f\"[INFO] Found existing '{csv_file_path}' in Colab. Using this file.\")\n",
    "    except ImportError:\n",
    "        print(\"[WARN] Google Colab modules not available but detected in Colab environment.\")\n",
    "else:\n",
    "    # Local environment (VS Code, etc.)\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"[INFO] Found existing '{csv_file_path}' locally. Using this file.\")\n",
    "    else:\n",
    "        # Create a simple placeholder CSV if it doesn't exist\n",
    "        try:\n",
    "            with open(csv_file_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['food_description', 'contains_allergen', 'sweetener', 'fats_oils'])\n",
    "                writer.writerow(['Sample food item 1', 'True', 'sugar', 'olive oil'])\n",
    "                writer.writerow(['Sample food item 2', 'False', 'none', 'none'])\n",
    "            print(f\"[INFO] Created placeholder '{csv_file_path}' for local development.\")\n",
    "            print(\"[NOTE] Replace this with your actual data file for meaningful results.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to create placeholder CSV: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F7je9NvONofE",
   "metadata": {
    "id": "F7je9NvONofE"
   },
   "source": [
    "# Data Exploration\n",
    "\n",
    "Let's explore the data to understand the distribution and structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "B_wPg0XWNm_n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "B_wPg0XWNm_n",
    "outputId": "16fe13cc-2bcb-47bf-e199-e757897b0b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Head\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_description</th>\n",
       "      <th>main_ingredient</th>\n",
       "      <th>sweetener</th>\n",
       "      <th>fat_or_oil</th>\n",
       "      <th>seasoning</th>\n",
       "      <th>allergens</th>\n",
       "      <th>contains_allergen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Creamy scrambled eggs, crispy bacon, and toast...</td>\n",
       "      <td>Eggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bacon</td>\n",
       "      <td>Dairy, Eggs</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>omg best pizza i ever had: gooey melted mozzar...</td>\n",
       "      <td>Mozzarella</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tomato sauce, Crispy crust</td>\n",
       "      <td>Dairy, Wheat</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Warm, flaky croissants filled with buttery, ga...</td>\n",
       "      <td>Spinach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Butter</td>\n",
       "      <td>Garlic</td>\n",
       "      <td>Almond, Dairy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decadent chocolate cake, moist and rich, serve...</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fresh catch of the day: pan-seared salmon with...</td>\n",
       "      <td>salmon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lemon, herb</td>\n",
       "      <td>Fish</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    food_description main_ingredient  \\\n",
       "0  Creamy scrambled eggs, crispy bacon, and toast...            Eggs   \n",
       "1  omg best pizza i ever had: gooey melted mozzar...      Mozzarella   \n",
       "2  Warm, flaky croissants filled with buttery, ga...         Spinach   \n",
       "3  Decadent chocolate cake, moist and rich, serve...       Chocolate   \n",
       "4  Fresh catch of the day: pan-seared salmon with...          salmon   \n",
       "\n",
       "  sweetener fat_or_oil                   seasoning      allergens  \\\n",
       "0       NaN        NaN                       Bacon    Dairy, Eggs   \n",
       "1       NaN        NaN  Tomato sauce, Crispy crust   Dairy, Wheat   \n",
       "2       NaN     Butter                      Garlic  Almond, Dairy   \n",
       "3     Sugar        NaN                         NaN          Dairy   \n",
       "4       NaN        NaN                 lemon, herb           Fish   \n",
       "\n",
       "  contains_allergen  \n",
       "0              true  \n",
       "1              true  \n",
       "2              True  \n",
       "3              true  \n",
       "4              true  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(\"DF Head\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "DIPOH1nLO-qi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "DIPOH1nLO-qi",
    "outputId": "55e28eca-215e-48ef-be8f-2000fead8e2b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title contains_allergen\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "df.groupby('contains_allergen').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zafgFwmrQm9C",
   "metadata": {
    "id": "zafgFwmrQm9C"
   },
   "source": [
    "Let's look at the factor data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kREqd0AVQjZT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kREqd0AVQjZT",
    "outputId": "e73fbadc-d5a9-4236-f819-68d59dfdf22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10020 entries, 0 to 10019\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   food_description   10020 non-null  object\n",
      " 1   main_ingredient    9865 non-null   object\n",
      " 2   sweetener          3666 non-null   object\n",
      " 3   fat_or_oil         3501 non-null   object\n",
      " 4   seasoning          7891 non-null   object\n",
      " 5   allergens          7548 non-null   object\n",
      " 6   contains_allergen  10020 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 548.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZOwWsWsfQ5lt",
   "metadata": {
    "id": "ZOwWsWsfQ5lt"
   },
   "source": [
    "Let's look at the number of unique values for each factor. Most noteworthy, the food_descriptions are almost all unique, which is expected. And all of the samples have a value in the contains_allergen column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "VEel3q1jQssf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "VEel3q1jQssf",
    "outputId": "6db628a7-89c5-405a-e30e-e4c74b9a4c80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_description</th>\n",
       "      <th>main_ingredient</th>\n",
       "      <th>sweetener</th>\n",
       "      <th>fat_or_oil</th>\n",
       "      <th>seasoning</th>\n",
       "      <th>allergens</th>\n",
       "      <th>contains_allergen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10020</td>\n",
       "      <td>9865</td>\n",
       "      <td>3666</td>\n",
       "      <td>3501</td>\n",
       "      <td>7891</td>\n",
       "      <td>7548</td>\n",
       "      <td>10020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9988</td>\n",
       "      <td>988</td>\n",
       "      <td>578</td>\n",
       "      <td>469</td>\n",
       "      <td>2778</td>\n",
       "      <td>1587</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Fried chicken tenders with honey mustard dippi...</td>\n",
       "      <td>chicken</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>Butter</td>\n",
       "      <td>none</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>895</td>\n",
       "      <td>497</td>\n",
       "      <td>721</td>\n",
       "      <td>285</td>\n",
       "      <td>1547</td>\n",
       "      <td>5821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         food_description main_ingredient  \\\n",
       "count                                               10020            9865   \n",
       "unique                                               9988             988   \n",
       "top     Fried chicken tenders with honey mustard dippi...         chicken   \n",
       "freq                                                    2             895   \n",
       "\n",
       "       sweetener fat_or_oil seasoning allergens contains_allergen  \n",
       "count       3666       3501      7891      7548             10020  \n",
       "unique       578        469      2778      1587                 5  \n",
       "top        Sugar     Butter      none     Dairy              true  \n",
       "freq         497        721       285      1547              5821  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tUX-yNtbSqRA",
   "metadata": {
    "id": "tUX-yNtbSqRA"
   },
   "source": [
    "There appears to be a majority of items that either don't have a predicted sweetner or fat/oil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "zOA15cyJRL5V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "zOA15cyJRL5V",
    "outputId": "309cbef9-8984-4111-ae8f-e2ceabe5924f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food_description        0\n",
       "main_ingredient       155\n",
       "sweetener            6354\n",
       "fat_or_oil           6519\n",
       "seasoning            2129\n",
       "allergens            2472\n",
       "contains_allergen       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tls0fdxQB8r6",
   "metadata": {
    "id": "tls0fdxQB8r6"
   },
   "source": [
    "## BPE Tokenizer\n",
    "\n",
    "This class handles tokenization. It uses the `tokenizers` library to train a Byte-Pair Encoding (BPE) model on the provided text data. BPE is effective at handling unknown words by breaking them down into subword units.\n",
    "\n",
    "If the `tokenizers` library is unavailable, it falls back to a simple character-level tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ksE9ApGLB8r6",
   "metadata": {
    "id": "ksE9ApGLB8r6"
   },
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\" Wrapper for BPE Tokenizer\"\"\"\n",
    "    def __init__(self, texts):\n",
    "        \"\"\" Initializes and trains the tokenizer.\n",
    "\n",
    "        Args:\n",
    "            texts (iterable): An iterable of strings to train the tokenizer on.\n",
    "        \"\"\"\n",
    "        # Ensure all texts are strings\n",
    "        texts = [str(text) for text in texts]\n",
    "        \n",
    "        # Use Hugging Face tokenizers library\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        # Define special tokens, ensuring <pad> is handled correctly (often ID 0 by convention)\n",
    "        trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "        # Train the tokenizer\n",
    "        self.tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "        # Ensure pad token ID is 0 if possible (it usually is by default with BpeTrainer)\n",
    "        pad_token_id = self.tokenizer.token_to_id(\"<pad>\")\n",
    "        if pad_token_id is None:\n",
    "              print(\"[WARN] <pad> token not found after training!\")\n",
    "              # Handle this case if necessary, maybe re-train or add manually\n",
    "        elif pad_token_id != 0:\n",
    "              print(f\"[WARN] <pad> token ID is {pad_token_id}, not 0. CrossEntropyLoss might need ignore_index adjustment if not using 0.\")\n",
    "        print(f\"[INFO] Trained BPE tokenizer. Vocab size: {self.tokenizer.get_vocab_size()}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Ensure input is a string\n",
    "        text = str(text)\n",
    "        \n",
    "        # Encode with BOS and EOS tokens implicitly handled via format string during encoding\n",
    "        bos_token = self.tokenizer.token_to_id(\"<bos>\")\n",
    "        eos_token = self.tokenizer.token_to_id(\"<eos>\")\n",
    "\n",
    "        encoded = self.tokenizer.encode(text) # Encode the main text\n",
    "\n",
    "        # Manually add BOS and EOS if not added automatically or if specific placement is needed\n",
    "        output_ids = []\n",
    "        if bos_token is not None:\n",
    "            output_ids.append(bos_token)\n",
    "        output_ids.extend(encoded.ids)\n",
    "        if eos_token is not None:\n",
    "              output_ids.append(eos_token)\n",
    "        return output_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\" Decodes a list of token IDs back into a string. \"\"\"\n",
    "        # Ensure ids is a list of integers\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.cpu().tolist()\n",
    "        # Use the tokenizer's decode method\n",
    "        return self.tokenizer.decode(ids, skip_special_tokens=False) # Keep special tokens for clarity if needed\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\" Returns the size of the vocabulary. \"\"\"\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\" Converts a token string to its ID.\"\"\"\n",
    "        return self.tokenizer.token_to_id(token)\n",
    "\n",
    "    def id_to_token(self, id):\n",
    "        \"\"\" Converts a token ID to its string representation.\"\"\"\n",
    "        return self.tokenizer.id_to_token(id)\n",
    "\n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        \"\"\" Returns the ID of the padding token.\"\"\"\n",
    "        return self.token_to_id(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z91DvXm3B8r6",
   "metadata": {
    "id": "z91DvXm3B8r6"
   },
   "source": [
    "## Dataset and Collation\n",
    "\n",
    "### `VectorizedFoodDataset` Class\n",
    "Reads the `food_prediction.csv` file. It takes the `food_description` column and vectorizes it. The `contains_allergen` is then encoded to use 1 and 0 instead of true and false.\n",
    "\n",
    "### `FoodDataset` Class\n",
    "Reads the `food_predictions.csv` file. It assumes the first column is `food_description` and concatenates all other columns into a structured `OUTPUT:` section. It then tokenizes this combined text.\n",
    "\n",
    "### `collate_fn` Function\n",
    "Takes a batch of sequences (lists of token IDs) from the dataset and pads them to the length of the longest sequence in the batch. It creates an attention mask to indicate which tokens are real and which are padding. This is necessary for batch processing in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uRy601ptB8r7",
   "metadata": {
    "id": "uRy601ptB8r7"
   },
   "outputs": [],
   "source": [
    "class VectorizedFoodDataset:\n",
    "    def __init__(self, csv_path, vectorizer):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Convert descriptions to strings to ensure they can be processed by vectorizer\n",
    "        descriptions = [str(desc) for desc in df[\"food_description\"].tolist()]\n",
    "        \n",
    "        # Convert allergen information to boolean values more reliably\n",
    "        bool_array = np.array([(str(val).lower() == \"true\") for val in df[\"contains_allergen\"].tolist()], dtype=int)\n",
    "\n",
    "        self.targets = bool_array\n",
    "        self.features = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "    def __clean_text(text):\n",
    "        \"\"\"Cleans the input text by removing irrelevant characters and converting to lowercase.\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s]', '', str(text))  # Remove punctuation, ensure text is string\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        return text\n",
    "\n",
    "\n",
    "class FoodDataset:\n",
    "    \"\"\" Loads and preprocesses data from the food CSV file.\"\"\"\n",
    "    def __init__(self, csv_path, max_len=128):\n",
    "        \"\"\" Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the input CSV file.\n",
    "            max_len (int): Maximum sequence length after tokenization. Longer sequences will be truncated.\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self._tokenizer = None # Tokenizer will be set later\n",
    "        self.max_len = max_len\n",
    "\n",
    "        try:\n",
    "            with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[ERROR] CSV file not found at {csv_path}. Please ensure it exists.\")\n",
    "            rows = [] # Initialize with empty list to prevent further errors\n",
    "        except Exception as e:\n",
    "             print(f\"[ERROR] Failed to read CSV file {csv_path}: {e}\")\n",
    "             rows = []\n",
    "\n",
    "        for row in rows:\n",
    "            desc = row.get(\"food_description\", \"\") # Get food description, default to empty string if missing\n",
    "            other_cols = []\n",
    "            for k, v in row.items():\n",
    "                if k == \"food_description\": # Skip the description itself\n",
    "                    continue\n",
    "                other_cols.append(f\"{k}: {v}\") # Format other columns as 'key: value'\n",
    "\n",
    "            # Combine description and other info into a single string\n",
    "            output_section = \"\\n\".join(other_cols)\n",
    "            # Using a separator like ' OUTPUT:' helps the model distinguish input from target\n",
    "            self.samples.append(desc.strip() + \"\\nOUTPUT:\\n\" + output_section.strip())\n",
    "\n",
    "        if not self.samples:\n",
    "            print(\"[WARN] No samples loaded from the CSV. The dataset is empty.\")\n",
    "        else:\n",
    "             print(f\"[INFO] Loaded {len(self.samples)} samples from {csv_path}.\")\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        \"\"\" Sets the tokenizer to be used for encoding samples. \"\"\"\n",
    "        self._tokenizer = tokenizer\n",
    "        print(\"[INFO] Tokenizer set for the dataset.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the number of samples in the dataset. \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Retrieves a single sample by index.\n",
    "\n",
    "        If a tokenizer is set, it returns the tokenized and truncated sequence.\n",
    "        Otherwise, it returns the raw text sample.\n",
    "        \"\"\"\n",
    "        text = self.samples[idx]\n",
    "        if not self._tokenizer:\n",
    "            # Return raw text if tokenizer is not set (e.g., during tokenizer training)\n",
    "            return text\n",
    "\n",
    "        # Encode the text using the tokenizer\n",
    "        enc = self._tokenizer.encode(text)\n",
    "\n",
    "        # Truncate if the encoded sequence exceeds max_len\n",
    "        if len(enc) > self.max_len:\n",
    "            # Truncate, but ensure EOS token is preserved if it was originally included\n",
    "            eos_id = self._tokenizer.token_to_id(\"<eos>\")\n",
    "            enc = enc[:self.max_len -1 ] + [eos_id]\n",
    "\n",
    "        return enc\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    \"\"\" Collates a batch of tokenized sequences into padded tensors. \"\"\"\n",
    "    if not batch:\n",
    "        # Handle empty batch case\n",
    "        return {\"input_ids\": torch.empty((0, 0), dtype=torch.long),\n",
    "                \"attention_mask\": torch.empty((0, 0), dtype=torch.long)}\n",
    "\n",
    "    # Check if the batch contains raw strings (shouldn't happen if used after tokenization)\n",
    "    if isinstance(batch[0], str):\n",
    "        print(\"[WARN] collate_fn received strings, expected token IDs.\")\n",
    "        return {\"input_ids\": batch, \"attention_mask\": [None]*len(batch)} # Basic handling for unexpected strings\n",
    "\n",
    "    # Determine the maximum length in the batch\n",
    "    lengths = [len(x) for x in batch]\n",
    "    max_batch_len = max(lengths) if lengths else 0\n",
    "\n",
    "    # Look up the padding token ID from the tokenizer\n",
    "    pad_token_id = tokenizer.pad_id\n",
    "\n",
    "    # Create padded tensors initialized with the padding token ID\n",
    "    padded = torch.full((len(batch), max_batch_len), pad_token_id, dtype=torch.long)\n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    mask = torch.zeros((len(batch), max_batch_len), dtype=torch.long)\n",
    "\n",
    "    # Fill the tensors with data from the batch\n",
    "    for i, seq in enumerate(batch):\n",
    "        seqlen = len(seq)\n",
    "        padded[i, :seqlen] = torch.tensor(seq, dtype=torch.long)\n",
    "        mask[i, :seqlen] = 1 # Mark the actual tokens in the mask\n",
    "\n",
    "    return {\"input_ids\": padded, \"attention_mask\": mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qyhZ1PgyB8r7",
   "metadata": {
    "id": "qyhZ1PgyB8r7"
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "### `DecoderOnlyTransformer`\n",
    "Implements a standard Transformer Decoder stack. It includes:\n",
    "-   An embedding layer (`nn.Embedding`) to convert token IDs into vectors.\n",
    "-   A stack of Transformer Decoder Layers (`nn.TransformerDecoderLayer`, `nn.TransformerDecoder`).\n",
    "-   A final linear layer (`nn.Linear`) to project the decoder output back to the vocabulary size, producing logits.\n",
    "-   It uses a causal mask (`generate_square_subsequent_mask`) to ensure that predictions for a position can only depend on previous positions.\n",
    "\n",
    "### `DecoderOnlyModelWrapper`\n",
    "A wrapper class that contains the `DecoderOnlyTransformer` model, the Adam optimizer, and the cross-entropy loss function (`nn.CrossEntropyLoss`). It provides methods for:\n",
    "-   Running the forward pass.\n",
    "-   Calculating the loss (using teacher forcing: predicting the next token based on the ground truth previous tokens).\n",
    "-   Accessing the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7vfVMbRqB8r7",
   "metadata": {
    "id": "7vfVMbRqB8r7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\" Simple Decoder-Only Transformer model. \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=5, dim_feedforward=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # Embedding layer: maps token IDs to dense vectors\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        # Positional Encoding (Add this for better performance, simple example omits it)\n",
    "        self.pos_encoder = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Standard Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                    dim_feedforward=dim_feedforward,\n",
    "                                                    batch_first=True) # Use batch_first=True\n",
    "        # Stack multiple decoder layers\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output layer: maps decoder output back to vocabulary size (logits)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\" Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "            attention_mask (Tensor, optional): Mask for padding tokens. Shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch_size, seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        positions = torch.arange(0, x.size(1), dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        # Add positional encoding here if implemented\n",
    "        pos_emb = self.pos_encoder(positions)\n",
    "        emb = self.emb(x) + pos_emb\n",
    "\n",
    "        # 2. Generate Causal Mask\n",
    "        seq_len = x.size(1)\n",
    "        # Mask to prevent attention to future tokens\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "\n",
    "        # 3. Generate Padding Mask from attention_mask\n",
    "        # TransformerDecoderLayer expects mask where True indicates masking\n",
    "        # Our `attention_mask` is 1 for tokens, 0 for padding. Need to invert it.\n",
    "        if attention_mask is not None:\n",
    "            # Shape: (batch_size, seq_len)\n",
    "            padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        # 4. Pass through Decoder\n",
    "        # Note: TransformerDecoder uses target (tgt) and memory. For decoder-only, memory is the same as target.\n",
    "        # `batch_first=True` means input shape is (batch, seq, feature)\n",
    "        dec_output = self.decoder(tgt=emb, memory=emb,\n",
    "                                tgt_mask=tgt_mask,\n",
    "                                tgt_key_padding_mask=padding_mask,\n",
    "                                memory_key_padding_mask=padding_mask) # Apply padding mask to memory as well\n",
    "\n",
    "        # 5. Final Linear Layer (Output Logits)\n",
    "        # Output shape: (batch_size, seq_len, vocab_size)\n",
    "        logits = self.fc(dec_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\" Counts the total number of trainable parameters in a PyTorch model. \"\"\"\n",
    "    # Ensure we are counting parameters of the actual nn.Module\n",
    "    actual_model = model.model if isinstance(model, DecoderOnlyModelWrapper) else model\n",
    "    if isinstance(actual_model, nn.Module):\n",
    "        return sum(p.numel() for p in actual_model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return 0 # Should not happen with real model\n",
    "\n",
    "class DecoderOnlyModelWrapper(nn.Module):\n",
    "    \"\"\" Wraps the Transformer model, optimizer, and loss function. \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=5, dim_feedforward=512, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.lr = lr\n",
    "        # Adam optimizer for training\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        # Cross Entropy Loss, ignoring padding token (assuming ID 0)\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        print(\"[INFO] Initialized PyTorch DecoderOnlyModelWrapper.\")\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\" Forward pass through the underlying model. \"\"\"\n",
    "        # Ensure input tensor is on the same device as the model\n",
    "        # device = next(self.model.parameters()).device # Get device from model parameters\n",
    "        # x = x.to(device)\n",
    "        # if attention_mask is not None:\n",
    "        #     attention_mask = attention_mask.to(device)\n",
    "        return self.model(x, attention_mask)\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        \"\"\" Computes the loss for a given batch. \"\"\"\n",
    "        inp = batch[\"input_ids\"] # Shape: (batch_size, seq_len)\n",
    "        attn_mask = batch.get(\"attention_mask\") # Shape: (batch_size, seq_len) or None\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        inp = inp.to(device)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "        # Get model predictions (logits)\n",
    "        # Input `inp` has shape (batch, seq_len)\n",
    "        logits = self(inp, attention_mask=attn_mask) # Shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Prepare for loss calculation:\n",
    "        # Predict the token at step `t` based on tokens `0..t-1`\n",
    "        # Logits for prediction need to exclude the last token's output\n",
    "        # Target labels need to exclude the first token (BOS)\n",
    "        pred_logits = logits[:, :-1, :].contiguous() # Shape: (batch, seq_len-1, vocab_size)\n",
    "        target_ids = inp[:, 1:].contiguous()       # Shape: (batch, seq_len-1)\n",
    "\n",
    "        # Flatten logits and targets for CrossEntropyLoss\n",
    "        # Input shape for loss: (N*C), Target shape: (N)\n",
    "        # N = batch_size * (seq_len - 1), C = vocab_size\n",
    "        loss = self.crit(pred_logits.view(-1, pred_logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\" Returns the optimizer instance. \"\"\"\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d80a8",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "The `train_loop` function orchestrates the training process over multiple epochs:\n",
    "-   Iterates through the training data loader.\n",
    "-   Computes the loss for each batch.\n",
    "-   Performs backpropagation and updates model weights using the optimizer.\n",
    "-   Optionally, evaluates the model on a validation set after each epoch.\n",
    "-   Implements early stopping: training halts if the validation loss doesn't improve for a specified number of `patience` epochs.\n",
    "-   Tracks and prints training and validation losses.\n",
    "-   Plots the losses and saves the plot as `training_validation_loss.png`.\n",
    "-   Generates a basic `report.html` containing the loss plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d8aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader_func, val_loader_func, epochs=30, device=\"cpu\", patience=5, report_filename='report.html', loss_plot_filename='training_validation_loss.png'):\n",
    "    \"\"\" Trains the model, performs validation, and handles early stopping. \"\"\"\n",
    "\n",
    "    # Move model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Enable mixed precision training if CUDA is available\n",
    "    use_amp = device.type == 'cuda'\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    print(f\"[INFO] Model moved to {device}. Mixed precision training: {use_amp}\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"--- Starting Training --- Epochs: {epochs}, Device: {device}, Patience: {patience} ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set model to training mode\n",
    "        total_train_loss = 0.0\n",
    "        train_steps = 0\n",
    "        train_loader = train_loader_func() # Get fresh iterator for the epoch\n",
    "\n",
    "        print(\"  Training...\")\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # training step\n",
    "            try:\n",
    "                # Calculate loss\n",
    "                loss = model.compute_loss(batch)\n",
    "                loss_item = loss.item()\n",
    "                total_train_loss += loss_item\n",
    "\n",
    "                # Backpropagation\n",
    "                model.get_optimizer().zero_grad() # Clear previous gradients\n",
    "                loss.backward()                   # Compute gradients\n",
    "                model.get_optimizer().step()      # Update weights\n",
    "            except Exception as e:\n",
    "                  print(f\"[ERROR] Exception during training step {i}: {e}\")\n",
    "                  # Optionally skip batch or break\n",
    "                  continue\n",
    "\n",
    "            train_steps += 1\n",
    "            if (i + 1) % 10 == 0: # Print progress every 10 steps\n",
    "                 print(f\"    Step {i+1}: current batch loss = {loss_item:.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / train_steps if train_steps > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"  Epoch {epoch+1} Average Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        val_loader = val_loader_func() # Get validation loader\n",
    "        if val_loader:\n",
    "            print(\"  Validating...\")\n",
    "            model.eval() # Set model to evaluation mode\n",
    "\n",
    "            total_val_loss = 0.0\n",
    "            val_steps = 0\n",
    "\n",
    "            # Use torch.no_grad() for validation to save memory and computation\n",
    "            maybe_no_grad = torch.no_grad()\n",
    "\n",
    "            with maybe_no_grad:\n",
    "                for i, val_batch in enumerate(val_loader):\n",
    "                    try:\n",
    "                      vloss = model.compute_loss(val_batch)\n",
    "                      vloss_item = vloss.item()\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Exception during validation step {i}: {e}\")\n",
    "                        vloss_item = float('nan') # Indicate error\n",
    "                        continue\n",
    "\n",
    "                    if not math.isnan(vloss_item):\n",
    "                         total_val_loss += vloss_item\n",
    "                         val_steps += 1\n",
    "                         if (i + 1) % 10 == 0:\n",
    "                             print(f\"    Validation Step {i+1}: current batch loss = {vloss_item:.4f}\")\n",
    "\n",
    "            avg_val_loss = total_val_loss / val_steps if val_steps > 0 else float('inf') # Handle case with no validation steps\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print(f\"  Epoch {epoch+1} Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            # --- Early Stopping Check ---\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0 # Reset patience counter\n",
    "                print(f\"    New best validation loss: {best_val_loss:.4f}. Patience reset.\")\n",
    "                # Save model state\n",
    "                model_state = {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': model.get_optimizer().state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_losses': val_losses\n",
    "                }\n",
    "                torch.save(model_state, 'best_model.pth')\n",
    "                print(\"    Best model checkpoint saved with additional training state.\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                improvement = best_val_loss - avg_val_loss\n",
    "                print(f\"    Validation loss did not improve. Current: {avg_val_loss:.4f}, Best: {best_val_loss:.4f}, Delta: {improvement:.6f}\")\n",
    "                print(f\"    Early stopping patience: {patience_counter}/{patience}\")\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\n--- Early Stopping triggered at epoch {epoch+1} ---\")\n",
    "                    print(f\"--- Best validation loss: {best_val_loss:.4f} achieved at epoch {epoch+1 - patience_counter} ---\")\n",
    "                    break # Stop training\n",
    "        else:\n",
    "             print(\"  No validation loader provided, skipping validation.\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "    # --- Plotting and Reporting ---\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        if val_losses: # Only plot validation loss if it was calculated\n",
    "            plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(loss_plot_filename)\n",
    "        plt.show() # Display the plot in the notebook\n",
    "        print(f\"[INFO] Loss plot saved as '{loss_plot_filename}'\")\n",
    "\n",
    "        # Generate HTML Report (overwrite or create)\n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write('<html><head><title>Training Report</title></head><body>\\n')\n",
    "            f.write('<h1>Training Report</h1>\\n')\n",
    "            f.write('<h2>Training and Validation Loss</h2>\\n')\n",
    "            # Use relative path for image source\n",
    "            f.write(f'<img src=\"{os.path.basename(loss_plot_filename)}\" alt=\"Training and Validation Loss\"><br>\\n')\n",
    "            # Table for losses (optional)\n",
    "            f.write('<h3>Loss Values per Epoch</h3>\\n')\n",
    "            f.write('<table border=\"1\"><tr><th>Epoch</th><th>Train Loss</th>')\n",
    "            if val_losses:\n",
    "                f.write('<th>Validation Loss</th>')\n",
    "            f.write('</tr>\\n')\n",
    "            for i in range(len(train_losses)):\n",
    "                 f.write(f'<tr><td>{i+1}</td><td>{train_losses[i]:.4f}</td>')\n",
    "                 if i < len(val_losses):\n",
    "                     f.write(f'<td>{val_losses[i]:.4f}</td>')\n",
    "                 elif val_losses: # If val exists but stopped early\n",
    "                      f.write('<td>N/A</td>')\n",
    "                 f.write('</tr>\\n')\n",
    "            f.write('</table>\\n')\n",
    "            # Placeholder for closing tags, will be added to by evaluation metrics\n",
    "            # f.write('</body></html>')\n",
    "        print(f\"[INFO] Basic HTML report started in '{report_filename}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"[ERROR] Failed to generate plot or report: {e}\")\n",
    "\n",
    "    return train_losses, val_losses # Return recorded losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "djSjSBJqzoG_",
   "metadata": {
    "id": "djSjSBJqzoG_"
   },
   "source": [
    "## Create an RNN Model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "XqIS7ll2zVJf",
   "metadata": {
    "id": "XqIS7ll2zVJf"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RNNModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_units=128, dropout_rate=0.2, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Automatically saves the hyperparameters\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_units, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_units, 1) # For binary classification\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x) # Get the last hidden state\n",
    "        x = self.dropout(x[:, -1, :]) # Apply dropout to the last hidden state\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(logits.squeeze(), y.float()) # Binary cross-entropy loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(logits.squeeze(), y.float())\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mbES9GnwB8r9",
   "metadata": {
    "id": "mbES9GnwB8r9"
   },
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "These functions evaluate the trained model's performance on a dataset (typically the validation or a separate test set).\n",
    "\n",
    "### `compute_confusion_matrix`\n",
    "-   Iterates through the evaluation dataset.\n",
    "-   Gets model predictions (logits) for each batch.\n",
    "-   Determines the predicted token ID (argmax) for each position.\n",
    "-   Compares predicted IDs against the true next token IDs (gold labels).\n",
    "-   Aggregates these comparisons into a confusion matrix (tensor).\n",
    "-   Visualizes the confusion matrix using `matplotlib` and saves it as `confusion_matrix.png`.\n",
    "\n",
    "### `compute_metrics`\n",
    "-   Similar to the confusion matrix computation, it iterates through the data and gets predictions vs. gold labels.\n",
    "-   Flattens the predictions and labels across all batches (ignoring padding).\n",
    "-   Uses `scikit-learn`'s `precision_score`, `recall_score`, and `f1_score` functions to calculate weighted metrics across all token classes.\n",
    "-   Returns the computed precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uk6d4hZUB8r9",
   "metadata": {
    "id": "uk6d4hZUB8r9"
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, eval_dataset, tokenizer, device=\"cpu\", batch_size=8, conf_matrix_filename='confusion_matrix.png'):\n",
    "    \"\"\" Computes and saves the confusion matrix for token predictions. \"\"\"\n",
    "    print(\"--- Computing Confusion Matrix ---\")\n",
    "\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "    model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "    vocab_size = model.model.fc.out_features\n",
    "    # Initialize confusion matrix on CPU to avoid potential GPU memory issues for large vocabs\n",
    "    confusion = torch.zeros((vocab_size, vocab_size), dtype=torch.long, device='cpu')\n",
    "\n",
    "    # Create a simple data loader for the evaluation dataset\n",
    "    def eval_loader_func():\n",
    "        for i in range(0, len(eval_dataset), batch_size):\n",
    "            batch_data = eval_dataset[i : i + batch_size]\n",
    "            # Collate the batch manually or using the collate_fn\n",
    "            collated_batch = collate_fn(batch_data, tokenizer)\n",
    "            yield collated_batch\n",
    "\n",
    "    processed_tokens = 0\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for batch in eval_loader_func():\n",
    "            inp = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch.get(\"attention_mask\", None)\n",
    "            if attn_mask is not None:\n",
    "                 attn_mask = attn_mask.to(device)\n",
    "\n",
    "            if inp.numel() == 0: continue # Skip empty batches\n",
    "\n",
    "            # Get model predictions\n",
    "            logits = model(inp, attention_mask=attn_mask) # (batch, seq_len, vocab_size)\n",
    "\n",
    "            # Get predicted token IDs (argmax along the vocab dimension)\n",
    "            # We predict the next token, so compare logits[:, :-1, :] with targets[:, 1:]\n",
    "            pred_logits = logits[:, :-1, :]\n",
    "            predicted_ids = pred_logits.argmax(dim=-1) # (batch, seq_len-1)\n",
    "\n",
    "            # Get gold standard (actual) token IDs\n",
    "            gold_ids = inp[:, 1:] # (batch, seq_len-1)\n",
    "\n",
    "            # Create a mask to ignore padding tokens in the gold standard\n",
    "            # Assuming pad_id is 0\n",
    "            mask = (gold_ids != tokenizer.pad_id) # (batch, seq_len-1)\n",
    "\n",
    "            # Flatten tensors and apply mask\n",
    "            gold_flat = torch.masked_select(gold_ids, mask)\n",
    "            pred_flat = torch.masked_select(predicted_ids, mask)\n",
    "\n",
    "            # Move tensors to CPU for confusion matrix update\n",
    "            gold_flat_cpu = gold_flat.cpu()\n",
    "            pred_flat_cpu = pred_flat.cpu()\n",
    "\n",
    "            # Update confusion matrix\n",
    "            for gold_tok, pred_tok in zip(gold_flat_cpu, pred_flat_cpu):\n",
    "                # Ensure indices are within bounds (should be guaranteed by vocab size)\n",
    "                if 0 <= gold_tok.item() < vocab_size and 0 <= pred_tok.item() < vocab_size:\n",
    "                     confusion[gold_tok.item(), pred_tok.item()] += 1\n",
    "                     processed_tokens += 1\n",
    "                else:\n",
    "                     print(f\"[WARN] Token ID out of bounds: Gold={gold_tok.item()}, Pred={pred_tok.item()}. Vocab size={vocab_size}. Skipping.\")\n",
    "            print(f\"  Processed batch. Total tokens considered so far: {processed_tokens}\")\n",
    "\n",
    "    print(f\"--- Confusion Matrix Calculation Complete. Total tokens analyzed: {processed_tokens} ---\")\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        # Display a subset if the vocab is too large\n",
    "        matrix_to_plot = confusion\n",
    "        max_display_size = 50 # Limit display size for readability\n",
    "        if vocab_size > max_display_size:\n",
    "            print(f\"[INFO] Vocab size ({vocab_size}) is large, plotting only top {max_display_size}x{max_display_size} part of the matrix.\")\n",
    "            matrix_to_plot = confusion[:max_display_size, :max_display_size]\n",
    "\n",
    "        plt.imshow(matrix_to_plot.log1p(), interpolation='nearest', cmap='Blues') # Use log scale for better visibility\n",
    "        plt.title(f'Confusion Matrix (Log Scale) - First {matrix_to_plot.shape[0]} Tokens')\n",
    "        plt.xlabel('Predicted Token ID')\n",
    "        plt.ylabel('Actual Token ID')\n",
    "        plt.colorbar()\n",
    "        # Add ticks if the matrix is small enough\n",
    "        if matrix_to_plot.shape[0] <= 20:\n",
    "             tick_marks = torch.arange(matrix_to_plot.shape[0])\n",
    "             plt.xticks(tick_marks, tick_marks)\n",
    "             plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(conf_matrix_filename)\n",
    "        plt.show()\n",
    "        print(f\"[INFO] Confusion matrix plot saved as '{conf_matrix_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to plot confusion matrix: {e}\")\n",
    "\n",
    "    # Optionally, return the matrix itself\n",
    "    return confusion\n",
    "\n",
    "\n",
    "def compute_metrics(model, eval_dataset, tokenizer, device=\"cpu\", batch_size=8):\n",
    "    \"\"\" Computes precision, recall, and F1 score for token predictions. \"\"\"\n",
    "    print(\"--- Computing Metrics (Precision, Recall, F1) ---\")\n",
    "\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "    model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Create a simple data loader for the evaluation dataset\n",
    "    def eval_loader_func():\n",
    "        for i in range(0, len(eval_dataset), batch_size):\n",
    "            batch_data = []\n",
    "            for j in range(i, min(i + batch_size, len(eval_dataset))):\n",
    "                item = eval_dataset[j]\n",
    "                # Ensure we're working with sequence data, not scalars\n",
    "                if isinstance(item, (list, tuple)) or (hasattr(item, \"__len__\") and not isinstance(item, (str, int, float))):\n",
    "                    batch_data.append(item)\n",
    "                else:\n",
    "                    # Skip scalar values or convert them if needed\n",
    "                    print(f\"[WARN] Skipping non-sequence item at index {j}: {item}\")\n",
    "            \n",
    "            if not batch_data:\n",
    "                continue  # Skip empty batches\n",
    "                \n",
    "            # Process the batch through collate_fn\n",
    "            try:\n",
    "                collated_batch = collate_fn(batch_data, tokenizer)\n",
    "                yield collated_batch\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to process batch {i//batch_size}: {e}\")\n",
    "                continue\n",
    "\n",
    "    processed_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader_func():\n",
    "            inp = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch.get(\"attention_mask\", None)\n",
    "            if attn_mask is not None:\n",
    "                 attn_mask = attn_mask.to(device)\n",
    "\n",
    "            if inp.numel() == 0: continue # Skip empty batches\n",
    "\n",
    "            logits = model(inp, attention_mask=attn_mask)\n",
    "            pred_logits = logits[:, :-1, :]\n",
    "            predicted_ids = pred_logits.argmax(dim=-1)\n",
    "            gold_ids = inp[:, 1:]\n",
    "            mask = (gold_ids != tokenizer.pad_id)\n",
    "\n",
    "            gold_flat = torch.masked_select(gold_ids, mask)\n",
    "            pred_flat = torch.masked_select(predicted_ids, mask)\n",
    "\n",
    "            # Append flattened results (move to CPU list for scikit-learn)\n",
    "            all_labels.extend(gold_flat.cpu().tolist())\n",
    "            all_preds.extend(pred_flat.cpu().tolist())\n",
    "            processed_tokens += len(gold_flat)\n",
    "            print(f\"  Processed batch. Total tokens considered so far: {processed_tokens}\")\n",
    "\n",
    "    print(f\"--- Metrics Calculation Complete. Total tokens analyzed: {processed_tokens} ---\")\n",
    "\n",
    "    if not all_labels: # Handle case where no valid tokens were processed\n",
    "        print(\"[WARN] No valid tokens found for metric calculation. Returning zero metrics.\")\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Compute metrics using scikit-learn\n",
    "    # 'weighted' average accounts for label imbalance\n",
    "    # `zero_division=0` handles cases where a class might have no predictions/labels\n",
    "    try:\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        print(f\"  Calculated Metrics - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    except Exception as e:\n",
    "         print(f\"[ERROR] Failed to compute metrics using sklearn: {e}\")\n",
    "         precision, recall, f1 = 0.0, 0.0, 0.0 # Default to zero on error\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nwFZ_9VpB8r9",
   "metadata": {
    "id": "nwFZ_9VpB8r9"
   },
   "source": [
    "## Logging Utility\n",
    "\n",
    "The `Tee` class redirects `stdout` and `stderr` streams. Any output printed to the console will also be written to a specified log file (`log.txt` in this case). This is useful for keeping a persistent record of the entire process, including print statements, warnings, and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ULw2HiO9B8r9",
   "metadata": {
    "id": "ULw2HiO9B8r9"
   },
   "outputs": [],
   "source": [
    "class Tee:\n",
    "    \"\"\" Utility class to redirect stdout/stderr to both console and a file.\"\"\"\n",
    "    def __init__(self, console, logfile):\n",
    "        self.console = console\n",
    "        self.logfile = logfile\n",
    "\n",
    "    def write(self, data):\n",
    "        self.console.write(data)\n",
    "        self.logfile.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        # This flush method is needed for compatibility with sys.stdout\n",
    "        self.console.flush()\n",
    "        self.logfile.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HUBTQeEiB8r-",
   "metadata": {
    "id": "HUBTQeEiB8r-"
   },
   "source": [
    "## Main Execution Block\n",
    "\n",
    "This is the main part of the notebook that orchestrates the entire process:\n",
    "\n",
    "1.  **Setup Logging:** Redirects output using the `Tee` class to `log.txt`.\n",
    "2.  **Load Dataset:** Creates an instance of `FoodDataset` using `food_predictions.csv`.\n",
    "3.  **Test Dataset:** Runs `test_dataset_length`.\n",
    "4.  **Initialize & Train Tokenizer:** Creates `BPETokenizer` and trains it on the dataset samples.\n",
    "5.  **Set Tokenizer for Dataset:** Assigns the trained tokenizer to the dataset instance.\n",
    "6.  **Data Split:** Splits dataset indices into training and validation sets (using a simple 90/10 split here).\n",
    "7.  **Define Data Loaders:** Creates functions (`train_loader`, `val_loader`) that generate batches of data using the specified indices and the `collate_fn`.\n",
    "8.  **Initialize Model:** Creates an instance of `DecoderOnlyModelWrapper` with the vocabulary size from the tokenizer and hyperparameters.\n",
    "9.  **Set Device:** Determines whether to use CUDA (GPU) if available, otherwise CPU.\n",
    "10. **Count Parameters & Run Tests:** Prints the number of trainable parameters and runs the basic model tests.\n",
    "11. **Train Model:** Calls the `train_loop` function to train the model.\n",
    "12. **Evaluate Model:** After training, calls `compute_confusion_matrix` and `compute_metrics` on the validation set (used here as a test set).\n",
    "13. **Save Model & Report:** Saves the trained model's state dictionary to `trained_model.pth` and appends the evaluation metrics to `report.html`.\n",
    "14. **Cleanup Logging:** Restores the original `stdout` and `stderr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "M32LQqt4ngVT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M32LQqt4ngVT",
    "outputId": "74f96f28-7463-41d0-d3f4-e368e3659c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleared log file: log.txt\n"
     ]
    }
   ],
   "source": [
    "# Define filenames\n",
    "log_filename = \"log.txt\"\n",
    "csv_filename = \"food_predictions.csv\" # Assumed to be created or exist\n",
    "report_filename = \"report.html\"\n",
    "loss_plot_filename = \"training_validation_loss.png\"\n",
    "conf_matrix_filename = \"confusion_matrix.png\"\n",
    "model_save_filename = \"trained_model.pth\"\n",
    "\n",
    "# Clear log file at the start\n",
    "try:\n",
    "    with open(log_filename, \"w\") as f:\n",
    "        f.write(\"--- Log Start ---\\n\")\n",
    "    print(f\"[INFO] Cleared log file: {log_filename}\")\n",
    "except IOError as e:\n",
    "     print(f\"[WARN] Could not clear log file {log_filename}: {e}\")\n",
    "\n",
    "# Keep original stdout/stderr\n",
    "original_stdout = sys.stdout\n",
    "original_stderr = sys.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cAOwJam9d6o_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAOwJam9d6o_",
    "outputId": "c9d3e483-fb1e-4440-da8f-a2a477b40d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Main Process ---\n",
      "\n",
      "[Phase 1] Loading dataset from 'food_predictions.csv'...\n",
      "\n",
      "[Phase 2] Vectorizing dataset...\n",
      "[INFO] Dataset length: 10020\n",
      "\n",
      "[Phase 3] Check vectorized data\n",
      "  Features: <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 13 stored elements and shape (1, 2214)>\n",
      "  Coords\tValues\n",
      "  (0, 486)\t0.18906573466080354\n",
      "  (0, 1692)\t0.36280099125122656\n",
      "  (0, 656)\t0.34251137829537215\n",
      "  (0, 499)\t0.18004423698562447\n",
      "  (0, 103)\t0.2591121087630078\n",
      "  (0, 52)\t0.10731739598299057\n",
      "  (0, 2003)\t0.25985567632738704\n",
      "  (0, 106)\t0.49475648317838145\n",
      "  (0, 2162)\t0.10547004900525823\n",
      "  (0, 1742)\t0.2961612146933974\n",
      "  (0, 1338)\t0.1997704180635649\n",
      "  (0, 798)\t0.22337156390945978\n",
      "  (0, 1875)\t0.3213970858820761\n",
      "  Target: 1\n",
      "\n",
      "[Phase 4a] Splitting data into training and testing sets...\n",
      "  Training Features shape: (8016, 2214)\n",
      "  Training Targets shape: (8016,)\n",
      "  Testing Features shape: (2004, 2214)\n",
      "  Testing Targets shape: (2004,)\n",
      "\n",
      "[Phase 4b] Scaling features for kNN...\n",
      "\n",
      "[Phase 5a] Hyperparameter tuning for Random Forest...\n",
      "Best Random Forest parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "\n",
      "[Phase 5b] Hyperparameter tuning for K-Nearest Neighbors...\n",
      "Best kNN parameters: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'distance'}\n",
      "\n",
      "[Phase 6a] Running prediction on Random Forest...\n",
      "\n",
      "[Phase 6b] Running prediction on K-Nearest Neighbors...\n",
      "\n",
      "[Phase 7a] Evaluate performance of Random Forest...\n",
      "Random Forest Accuracy: 0.7455089820359282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.42      0.52       649\n",
      "           1       0.76      0.90      0.83      1355\n",
      "\n",
      "    accuracy                           0.75      2004\n",
      "   macro avg       0.72      0.66      0.67      2004\n",
      "weighted avg       0.73      0.75      0.73      2004\n",
      "\n",
      "\n",
      "[Phase 7b] Evaluate performance of K-Nearest Neighbors...\n",
      "K-Nearest Neighbor Accuracy: 0.6951097804391217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.40      0.46       649\n",
      "           1       0.75      0.83      0.79      1355\n",
      "\n",
      "    accuracy                           0.70      2004\n",
      "   macro avg       0.64      0.62      0.62      2004\n",
      "weighted avg       0.68      0.70      0.68      2004\n",
      "\n",
      "[INFO] Restored standard output/error streams.\n"
     ]
    }
   ],
   "source": [
    "def test_dataset_length(dataset):\n",
    "    \"\"\" A simple test function to check the length of the dataset. \"\"\"\n",
    "    print(f\"[INFO] Dataset length: {len(dataset)}\")\n",
    "\n",
    "# Open log file in append mode and start Tee redirection\n",
    "try:\n",
    "    log_file = open(log_filename, \"a\", encoding='utf-8')\n",
    "    sys.stdout = Tee(original_stdout, log_file)\n",
    "    sys.stderr = Tee(original_stderr, log_file)\n",
    "\n",
    "    print(\"\\n--- Starting Main Process ---\")\n",
    "\n",
    "    # 1. Load Dataset\n",
    "    print(f\"\\n[Phase 1] Loading dataset from '{csv_filename}'...\")\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vector_dset = VectorizedFoodDataset(csv_filename, vectorizer)\n",
    "\n",
    "    # 2. Test Vectorized Dataset Length\n",
    "    print(f\"\\n[Phase 2] Vectorizing dataset...\")\n",
    "    test_dataset_length(vector_dset)\n",
    "    if len(vector_dset) == 0:\n",
    "        raise ValueError(\"Dataset is empty. Cannot proceed. Check CSV file and path.\")\n",
    "\n",
    "    # 3. Print Features and Targets for first sample\n",
    "    print(f\"\\n[Phase 3] Check vectorized data\")\n",
    "    features, target = vector_dset[0]\n",
    "    print(f\"  Features: {features}\")\n",
    "    print(f\"  Target: {target}\")\n",
    "\n",
    "    # 4a. Split data into training and testing sets\n",
    "    print(\"\\n[Phase 4a] Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        vector_dset.features, vector_dset.targets, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"  Training Features shape: {X_train.shape}\")\n",
    "    print(f\"  Training Targets shape: {y_train.shape}\")\n",
    "    print(f\"  Testing Features shape: {X_test.shape}\")\n",
    "    print(f\"  Testing Targets shape: {y_test.shape}\")\n",
    "\n",
    "    # 4b. Scale features for kNN\n",
    "    print(\"\\n[Phase 4b] Scaling features for kNN...\")\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 5a. Initialize and train Random Forest\n",
    "    random_forest_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    print(\"\\n[Phase 5a] Hyperparameter tuning for Random Forest...\")\n",
    "    random_forest_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), random_forest_param_grid, cv=5, scoring='accuracy')\n",
    "    random_forest_grid_search.fit(X_train, y_train)\n",
    "    random_forest_model = random_forest_grid_search.best_estimator_\n",
    "    print(f\"Best Random Forest parameters: {random_forest_grid_search.best_params_}\")\n",
    "\n",
    "    # 5b. Initialize and train kNN\n",
    "    kNN_param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "\n",
    "    print(\"\\n[Phase 5b] Hyperparameter tuning for K-Nearest Neighbors...\")\n",
    "    kNN_grid_search = GridSearchCV(KNeighborsClassifier(), kNN_param_grid, cv=5, scoring='accuracy')\n",
    "    kNN_grid_search.fit(X_train_scaled, y_train)\n",
    "    kNN_model = kNN_grid_search.best_estimator_\n",
    "    print(f\"Best kNN parameters: {kNN_grid_search.best_params_}\")\n",
    "\n",
    "    # 6a. Make predictions on testing set for Random Forest\n",
    "    print(\"\\n[Phase 6a] Running prediction on Random Forest...\")\n",
    "    random_forest_y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "    # 6b. Make predictions on testing set for kNN\n",
    "    print(\"\\n[Phase 6b] Running prediction on K-Nearest Neighbors...\")\n",
    "    kNN_y_pred = kNN_model.predict(X_test_scaled)\n",
    "\n",
    "    # 7a. Evaluate performance of Random Forest\n",
    "    print(\"\\n[Phase 7a] Evaluate performance of Random Forest...\")\n",
    "    random_forest_accuracy = accuracy_score(y_test, random_forest_y_pred)\n",
    "    print(f\"Random Forest Accuracy: {random_forest_accuracy}\")\n",
    "    print(classification_report(y_test, random_forest_y_pred))\n",
    "\n",
    "    # 7b. Evaluate performance of kNN\n",
    "    print(\"\\n[Phase 7b] Evaluate performance of K-Nearest Neighbors...\")\n",
    "    kNN_accuracy = accuracy_score(y_test, kNN_y_pred)\n",
    "    print(f\"K-Nearest Neighbor Accuracy: {kNN_accuracy}\")\n",
    "    print(classification_report(y_test, kNN_y_pred))\n",
    "\n",
    "finally:\n",
    "    # 13. Cleanup Logging: Always restore original stdout/stderr\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "    if 'log' in locals() and log_file:\n",
    "        log_file.close()\n",
    "    print(\"[INFO] Restored standard output/error streams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rxHDwvjB7OvK",
   "metadata": {
    "id": "rxHDwvjB7OvK"
   },
   "source": [
    "## Function for training the RNN, including hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "no0KBrz-65v6",
   "metadata": {
    "id": "no0KBrz-65v6"
   },
   "outputs": [],
   "source": [
    "def train_rnn(train_loader, val_loader, vocab_size, max_length, tokenizer=None):\n",
    "    \"\"\"Trains the RNN model, saves the best model, and evaluates its performance.\"\"\"\n",
    "    \n",
    "    if tokenizer is None:\n",
    "        print(\"[ERROR] Tokenizer not provided to train_rnn function\")\n",
    "        return\n",
    "    \n",
    "    # Create a proper binary classification dataset for the RNN\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    # Import pytorch lightning's Trainer if not imported at the top\n",
    "    try:\n",
    "        from pytorch_lightning import Trainer\n",
    "    except ImportError:\n",
    "        print(\"[ERROR] pytorch_lightning is not installed. Please install it with pip install pytorch-lightning\")\n",
    "        return\n",
    "    \n",
    "    # Extract data from the existing loader and create tensors for binary classification\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for batch in train_loader:\n",
    "        # For binary classification, we'll use the 'contains_allergen' label\n",
    "        # First ensure inputs are tensors of integers (not strings)\n",
    "        inputs = batch[\"input_ids\"]  # Shape: [batch_size, seq_len]\n",
    "        # For simplicity, we'll convert the target based on if \"true\" appears in the text\n",
    "        # Make sure to convert each tensor to a list of integers before passing to tokenizer.decode\n",
    "        labels = torch.tensor([1 if \"true\" in tokenizer.decode(ids.tolist()).lower() else 0 \n",
    "                              for ids in inputs], dtype=torch.float)\n",
    "        \n",
    "        train_inputs.append(inputs)\n",
    "        train_labels.append(labels)\n",
    "    \n",
    "    # Ensure all tensors have the same sequence length before concatenation\n",
    "    if train_inputs:\n",
    "        try:\n",
    "            # Find the minimum sequence length across all batches\n",
    "            min_seq_len = min(inp.size(1) for inp in train_inputs)\n",
    "            \n",
    "            # Truncate all tensors to the minimum length\n",
    "            truncated_inputs = [inp[:, :min_seq_len] for inp in train_inputs]\n",
    "            \n",
    "            # Now concatenate the truncated tensors\n",
    "            all_train_inputs = torch.cat(truncated_inputs, dim=0)\n",
    "            all_train_labels = torch.cat(train_labels, dim=0)\n",
    "            \n",
    "            # Create new tensor datasets and loaders\n",
    "            train_dataset = TensorDataset(all_train_inputs, all_train_labels)\n",
    "            rnn_train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "            \n",
    "            # Do the same for validation\n",
    "            val_inputs = []\n",
    "            val_labels = []\n",
    "            for batch in val_loader:\n",
    "                inputs = batch[\"input_ids\"]\n",
    "                # Convert tensors to lists before decoding\n",
    "                labels = torch.tensor([1 if \"true\" in tokenizer.decode(x.cpu().tolist()).lower() else 0 \n",
    "                                     for x in inputs], dtype=torch.float)\n",
    "                val_inputs.append(inputs)\n",
    "                val_labels.append(labels)\n",
    "            \n",
    "            if val_inputs:\n",
    "                # Make sure validation tensors also have consistent sequence length\n",
    "                val_min_seq_len = min(inp.size(1) for inp in val_inputs)\n",
    "                # Use the smaller of train and validation min lengths to ensure compatibility\n",
    "                final_seq_len = min(min_seq_len, val_min_seq_len)\n",
    "                \n",
    "                # Truncate all tensors to the consistent length\n",
    "                truncated_train_inputs = [inp[:, :final_seq_len] for inp in train_inputs]\n",
    "                all_train_inputs = torch.cat(truncated_train_inputs, dim=0)\n",
    "                \n",
    "                # Truncate all validation tensors to the consistent length\n",
    "                truncated_val_inputs = [inp[:, :final_seq_len] for inp in val_inputs]\n",
    "                \n",
    "                # Now concatenate the truncated validation tensors\n",
    "                all_val_inputs = torch.cat(truncated_val_inputs, dim=0)\n",
    "                all_val_labels = torch.cat(val_labels, dim=0)\n",
    "                \n",
    "                val_dataset = TensorDataset(all_val_inputs, all_val_labels)\n",
    "                rnn_val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "            else:\n",
    "                rnn_val_loader = None\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Error preparing data for RNN: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "    else:\n",
    "        print(\"[ERROR] No training data available for RNN\")\n",
    "        return\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        # Import early stopping callback\n",
    "        from pytorch_lightning.callbacks import EarlyStopping\n",
    "        \n",
    "        # Create early stopping callback\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.00,\n",
    "            patience=3,\n",
    "            verbose=True,\n",
    "            mode='min'\n",
    "        )\n",
    "        \n",
    "        # Train the final RNN model with the best hyperparameters\n",
    "        rnn_model = RNNModel(vocab_size, embedding_dim=128, hidden_units=128, dropout_rate=0.2, lr=1e-3)\n",
    "        \n",
    "        # Enable mixed precision training for faster computation\n",
    "        from pytorch_lightning.plugins import MixedPrecisionPlugin\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            max_epochs=10, \n",
    "            gpus=1 if torch.cuda.is_available() else 0,\n",
    "            callbacks=[early_stop_callback],\n",
    "            precision=16 if torch.cuda.is_available() else 32,  # Use FP16 if GPU is available\n",
    "            accelerator='gpu' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        trainer.fit(rnn_model, rnn_train_loader, rnn_val_loader)\n",
    "\n",
    "        # Save the best RNN model\n",
    "        torch.save(rnn_model.state_dict(), 'rnn_model.pth')\n",
    "        print(\"Best RNN model saved to rnn_model.pth\")\n",
    "\n",
    "        # Evaluate the RNN model\n",
    "        rnn_model.eval()  # Set to evaluation mode\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in rnn_val_loader:\n",
    "                x, y = batch\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                logits = rnn_model(x)  # Use rnn_model instead of best_rnn_model\n",
    "                preds = torch.round(torch.sigmoid(logits)).squeeze()  # Round predictions to 0/1\n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "                all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "        # Calculate accuracy, precision, recall, and F1-score\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "        print(f\"RNN Evaluation Metrics:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-score: {f1:.4f}\")\n",
    "\n",
    "        # Append evaluation metrics to HTML report\n",
    "        try:\n",
    "            with open('report.html', 'a') as f:\n",
    "                f.write('<h2>RNN Evaluation Metrics</h2>\\n')\n",
    "                f.write(f'<p>Accuracy: {accuracy:.4f}</p>\\n')\n",
    "                f.write(f'<p>Precision: {precision:.4f}</p>\\n')\n",
    "                f.write(f'<p>Recall: {recall:.4f}</p>\\n')\n",
    "                f.write(f'<p>F1-score: {f1:.4f}</p>\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to append RNN metrics to report: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error during RNN training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zURs9Zde7ayx",
   "metadata": {
    "id": "zURs9Zde7ayx"
   },
   "source": [
    "## Function for training the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ao_MzbdZ672R",
   "metadata": {
    "id": "Ao_MzbdZ672R"
   },
   "outputs": [],
   "source": [
    "def train_transformer(train_loader_func, val_loader_func, vocab_size, tokenizer=None):\n",
    "    \"\"\"Trains the Transformer model, saves the best model, and evaluates its performance.\"\"\"\n",
    "    \n",
    "    if tokenizer is None:\n",
    "        print(\"[ERROR] Tokenizer not provided to train_transformer function\")\n",
    "        return\n",
    "        \n",
    "    # Create and train the Transformer model with default parameters\n",
    "    print(\"[INFO] Creating transformer model with default parameters\")\n",
    "    model = DecoderOnlyModelWrapper(\n",
    "        vocab_size,\n",
    "        d_model=128,\n",
    "        nhead=4,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=512,\n",
    "        lr=1e-3\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"[INFO] Training transformer model\")\n",
    "    train_losses, val_losses = train_loop(model, train_loader_func, val_loader_func, \n",
    "                                        epochs=20, device=device, patience=5)\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "    print(\"Transformer model saved to best_transformer_model.pth\")\n",
    "\n",
    "    # Evaluate the Transformer model\n",
    "    print(\"[INFO] Creating evaluation dataset\")\n",
    "    eval_dataset = FoodDataset(csv_file_path)\n",
    "    eval_dataset.set_tokenizer(tokenizer)\n",
    "\n",
    "    try:\n",
    "        # Add debug information to diagnose dataset issues\n",
    "        print(f\"[DEBUG] Evaluation dataset length: {len(eval_dataset)}\")\n",
    "        if len(eval_dataset) > 0:\n",
    "            sample_item = eval_dataset[0]\n",
    "            print(f\"[DEBUG] Sample item from evaluation dataset: {type(sample_item)}\")\n",
    "            print(f\"[DEBUG] Is sequence? {isinstance(sample_item, (list, tuple))}\")\n",
    "            if hasattr(sample_item, \"__len__\"):\n",
    "                print(f\"[DEBUG] Item length: {len(sample_item)}\")\n",
    "        \n",
    "        precision, recall, f1 = compute_metrics(model, eval_dataset, tokenizer, device=device, batch_size=8)\n",
    "\n",
    "        print(f\"Transformer Evaluation Metrics:\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1-score: {f1:.4f}\")\n",
    "\n",
    "        # Append evaluation metrics to HTML report\n",
    "        with open('report.html', 'a') as f:\n",
    "            f.write('<h2>Transformer Evaluation Metrics</h2>\\n')\n",
    "            f.write(f'<p>Precision: {precision:.4f}</p>\\n')\n",
    "            f.write(f'<p>Recall: {recall:.4f}</p>\\n')\n",
    "            f.write(f'<p>F1-score: {f1:.4f}</p>\\n')\n",
    "            f.write('</body></html>')  # Close the HTML tags\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to compute or append Transformer metrics: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RN-Ev7O4_PHc",
   "metadata": {
    "id": "RN-Ev7O4_PHc"
   },
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Jpr4ww0sB8r-",
   "metadata": {
    "id": "Jpr4ww0sB8r-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleared log file: log.txt\n",
      "\n",
      "--- Starting Main Process ---\n",
      "\n",
      "[Phase 1] Loading dataset from 'food_predictions.csv'...\n",
      "[INFO] Loaded 10020 samples from food_predictions.csv.\n",
      "[INFO] Dataset length: 10020\n",
      "\n",
      "[Phase 2] Initializing and training tokenizer...\n",
      "[INFO] Trained BPE tokenizer. Vocab size: 5817\n",
      "  Tokenizer vocabulary size: 5817\n",
      "[INFO] Tokenizer set for the dataset.\n",
      "[INFO] Trained BPE tokenizer. Vocab size: 5817\n",
      "  Tokenizer vocabulary size: 5817\n",
      "[INFO] Tokenizer set for the dataset.\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pilchj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 744 K \n",
      "1 | rnn       | GRU       | 99.1 K\n",
      "2 | dropout   | Dropout   | 0     \n",
      "3 | fc        | Linear    | 129   \n",
      "----------------------------------------\n",
      "843 K     Trainable params\n",
      "0         Non-trainable params\n",
      "843 K     Total params\n",
      "3.375     Total estimated model params size (MB)\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 744 K \n",
      "1 | rnn       | GRU       | 99.1 K\n",
      "2 | dropout   | Dropout   | 0     \n",
      "3 | fc        | Linear    | 129   \n",
      "----------------------------------------\n",
      "843 K     Trainable params\n",
      "0         Non-trainable params\n",
      "843 K     Total params\n",
      "3.375     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267b818248c84ddfa26ca8c06b1eb536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pilchj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\pilchj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2328910d28554b82bc1540fba5c8c2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a52702da88548d18346b276f936673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31644193a1e47959b78168ce06cf57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977b485923cd4ec380ff06f59f270c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c933c0e4f69a4292a66d344f683c9023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c5ecd53fb741baa74eeb876333db40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b829b125a62941df9b542290ad03445a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 0.053. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RNN model saved to rnn_model.pth\n",
      "RNN Evaluation Metrics:\n",
      "  Accuracy: 0.9800\n",
      "  Precision: 0.9851\n",
      "  Recall: 0.9851\n",
      "  F1-score: 0.9851\n",
      "[INFO] Creating transformer model with default parameters\n",
      "[INFO] Initialized PyTorch DecoderOnlyModelWrapper.\n",
      "[INFO] Training transformer model\n",
      "[INFO] Model moved to cpu. Mixed precision training: False\n",
      "--- Starting Training --- Epochs: 20, Device: cpu, Patience: 5 ---\n",
      "\n",
      "=== Epoch 1/20 ===\n",
      "  Training...\n",
      "RNN Evaluation Metrics:\n",
      "  Accuracy: 0.9800\n",
      "  Precision: 0.9851\n",
      "  Recall: 0.9851\n",
      "  F1-score: 0.9851\n",
      "[INFO] Creating transformer model with default parameters\n",
      "[INFO] Initialized PyTorch DecoderOnlyModelWrapper.\n",
      "[INFO] Training transformer model\n",
      "[INFO] Model moved to cpu. Mixed precision training: False\n",
      "--- Starting Training --- Epochs: 20, Device: cpu, Patience: 5 ---\n",
      "\n",
      "=== Epoch 1/20 ===\n",
      "  Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pilchj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step 10: current batch loss = 6.4518\n",
      "    Step 20: current batch loss = 5.1953\n",
      "    Step 20: current batch loss = 5.1953\n",
      "    Step 30: current batch loss = 4.4539\n",
      "    Step 30: current batch loss = 4.4539\n",
      "    Step 40: current batch loss = 4.0750\n",
      "    Step 40: current batch loss = 4.0750\n",
      "    Step 50: current batch loss = 3.8579\n",
      "    Step 50: current batch loss = 3.8579\n",
      "    Step 60: current batch loss = 3.4100\n",
      "    Step 60: current batch loss = 3.4100\n",
      "    Step 70: current batch loss = 3.3039\n",
      "    Step 70: current batch loss = 3.3039\n",
      "    Step 80: current batch loss = 3.0295\n",
      "    Step 80: current batch loss = 3.0295\n",
      "    Step 90: current batch loss = 3.0408\n",
      "    Step 90: current batch loss = 3.0408\n",
      "    Step 100: current batch loss = 2.6943\n",
      "    Step 100: current batch loss = 2.6943\n",
      "    Step 110: current batch loss = 2.4782\n",
      "    Step 110: current batch loss = 2.4782\n",
      "    Step 120: current batch loss = 2.4456\n",
      "    Step 120: current batch loss = 2.4456\n",
      "    Step 130: current batch loss = 2.4080\n",
      "    Step 130: current batch loss = 2.4080\n",
      "    Step 140: current batch loss = 2.0024\n",
      "    Step 140: current batch loss = 2.0024\n",
      "    Step 150: current batch loss = 1.8468\n",
      "    Step 150: current batch loss = 1.8468\n",
      "    Step 160: current batch loss = 1.6993\n",
      "    Step 160: current batch loss = 1.6993\n",
      "    Step 170: current batch loss = 1.4812\n",
      "    Step 170: current batch loss = 1.4812\n",
      "    Step 180: current batch loss = 1.3662\n",
      "    Step 180: current batch loss = 1.3662\n",
      "    Step 190: current batch loss = 1.0910\n",
      "    Step 190: current batch loss = 1.0910\n",
      "    Step 200: current batch loss = 0.9788\n",
      "    Step 200: current batch loss = 0.9788\n",
      "    Step 210: current batch loss = 0.9896\n",
      "    Step 210: current batch loss = 0.9896\n",
      "    Step 220: current batch loss = 1.1234\n",
      "    Step 220: current batch loss = 1.1234\n",
      "    Step 230: current batch loss = 0.7287\n",
      "    Step 230: current batch loss = 0.7287\n",
      "    Step 240: current batch loss = 0.6214\n",
      "    Step 240: current batch loss = 0.6214\n",
      "    Step 250: current batch loss = 0.7258\n",
      "  Epoch 1 Average Train Loss: 2.6151\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.7258\n",
      "  Epoch 1 Average Train Loss: 2.6151\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.6408\n",
      "    Validation Step 10: current batch loss = 0.6408\n",
      "    Validation Step 20: current batch loss = 0.5523\n",
      "    Validation Step 20: current batch loss = 0.5523\n",
      "    Validation Step 30: current batch loss = 0.5376\n",
      "    Validation Step 30: current batch loss = 0.5376\n",
      "    Validation Step 40: current batch loss = 0.8642\n",
      "    Validation Step 40: current batch loss = 0.8642\n",
      "    Validation Step 50: current batch loss = 0.4814\n",
      "    Validation Step 50: current batch loss = 0.4814\n",
      "    Validation Step 60: current batch loss = 0.4453\n",
      "    Validation Step 60: current batch loss = 0.4453\n",
      "  Epoch 1 Average Validation Loss: 0.6203\n",
      "    New best validation loss: 0.6203. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 2/20 ===\n",
      "  Training...\n",
      "  Epoch 1 Average Validation Loss: 0.6203\n",
      "    New best validation loss: 0.6203. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 2/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.6374\n",
      "    Step 10: current batch loss = 0.6374\n",
      "    Step 20: current batch loss = 0.5854\n",
      "    Step 20: current batch loss = 0.5854\n",
      "    Step 30: current batch loss = 0.4100\n",
      "    Step 30: current batch loss = 0.4100\n",
      "    Step 40: current batch loss = 0.4328\n",
      "    Step 40: current batch loss = 0.4328\n",
      "    Step 50: current batch loss = 0.4300\n",
      "    Step 50: current batch loss = 0.4300\n",
      "    Step 60: current batch loss = 0.4635\n",
      "    Step 60: current batch loss = 0.4635\n",
      "    Step 70: current batch loss = 0.4085\n",
      "    Step 70: current batch loss = 0.4085\n",
      "    Step 80: current batch loss = 0.3281\n",
      "    Step 80: current batch loss = 0.3281\n",
      "    Step 90: current batch loss = 0.2652\n",
      "    Step 90: current batch loss = 0.2652\n",
      "    Step 100: current batch loss = 0.2619\n",
      "    Step 100: current batch loss = 0.2619\n",
      "    Step 110: current batch loss = 0.3163\n",
      "    Step 110: current batch loss = 0.3163\n",
      "    Step 120: current batch loss = 0.2194\n",
      "    Step 120: current batch loss = 0.2194\n",
      "    Step 130: current batch loss = 0.1336\n",
      "    Step 130: current batch loss = 0.1336\n",
      "    Step 140: current batch loss = 0.2683\n",
      "    Step 140: current batch loss = 0.2683\n",
      "    Step 150: current batch loss = 0.2152\n",
      "    Step 150: current batch loss = 0.2152\n",
      "    Step 160: current batch loss = 0.2872\n",
      "    Step 160: current batch loss = 0.2872\n",
      "    Step 170: current batch loss = 0.2185\n",
      "    Step 170: current batch loss = 0.2185\n",
      "    Step 180: current batch loss = 0.2062\n",
      "    Step 180: current batch loss = 0.2062\n",
      "    Step 190: current batch loss = 0.2109\n",
      "    Step 190: current batch loss = 0.2109\n",
      "    Step 200: current batch loss = 0.1933\n",
      "    Step 200: current batch loss = 0.1933\n",
      "    Step 210: current batch loss = 0.1537\n",
      "    Step 210: current batch loss = 0.1537\n",
      "    Step 220: current batch loss = 0.1270\n",
      "    Step 220: current batch loss = 0.1270\n",
      "    Step 230: current batch loss = 0.1209\n",
      "    Step 230: current batch loss = 0.1209\n",
      "    Step 240: current batch loss = 0.3535\n",
      "    Step 240: current batch loss = 0.3535\n",
      "    Step 250: current batch loss = 0.1438\n",
      "  Epoch 2 Average Train Loss: 0.3169\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.1438\n",
      "  Epoch 2 Average Train Loss: 0.3169\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.1785\n",
      "    Validation Step 10: current batch loss = 0.1785\n",
      "    Validation Step 20: current batch loss = 0.1193\n",
      "    Validation Step 20: current batch loss = 0.1193\n",
      "    Validation Step 30: current batch loss = 0.1477\n",
      "    Validation Step 30: current batch loss = 0.1477\n",
      "    Validation Step 40: current batch loss = 0.2560\n",
      "    Validation Step 40: current batch loss = 0.2560\n",
      "    Validation Step 50: current batch loss = 0.1108\n",
      "    Validation Step 50: current batch loss = 0.1108\n",
      "    Validation Step 60: current batch loss = 0.0717\n",
      "    Validation Step 60: current batch loss = 0.0717\n",
      "  Epoch 2 Average Validation Loss: 0.1496\n",
      "    New best validation loss: 0.1496. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 3/20 ===\n",
      "  Training...\n",
      "  Epoch 2 Average Validation Loss: 0.1496\n",
      "    New best validation loss: 0.1496. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 3/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.1270\n",
      "    Step 10: current batch loss = 0.1270\n",
      "    Step 20: current batch loss = 0.1145\n",
      "    Step 20: current batch loss = 0.1145\n",
      "    Step 30: current batch loss = 0.0860\n",
      "    Step 30: current batch loss = 0.0860\n",
      "    Step 40: current batch loss = 0.1498\n",
      "    Step 40: current batch loss = 0.1498\n",
      "    Step 50: current batch loss = 0.0914\n",
      "    Step 50: current batch loss = 0.0914\n",
      "    Step 60: current batch loss = 0.0675\n",
      "    Step 60: current batch loss = 0.0675\n",
      "    Step 70: current batch loss = 0.1332\n",
      "    Step 70: current batch loss = 0.1332\n",
      "    Step 80: current batch loss = 0.0916\n",
      "    Step 80: current batch loss = 0.0916\n",
      "    Step 90: current batch loss = 0.0630\n",
      "    Step 90: current batch loss = 0.0630\n",
      "    Step 100: current batch loss = 0.0895\n",
      "    Step 100: current batch loss = 0.0895\n",
      "    Step 110: current batch loss = 0.1028\n",
      "    Step 110: current batch loss = 0.1028\n",
      "    Step 120: current batch loss = 0.0839\n",
      "    Step 120: current batch loss = 0.0839\n",
      "    Step 130: current batch loss = 0.0579\n",
      "    Step 130: current batch loss = 0.0579\n",
      "    Step 140: current batch loss = 0.0890\n",
      "    Step 140: current batch loss = 0.0890\n",
      "    Step 150: current batch loss = 0.0540\n",
      "    Step 150: current batch loss = 0.0540\n",
      "    Step 160: current batch loss = 0.0840\n",
      "    Step 160: current batch loss = 0.0840\n",
      "    Step 170: current batch loss = 0.0796\n",
      "    Step 170: current batch loss = 0.0796\n",
      "    Step 180: current batch loss = 0.1229\n",
      "    Step 180: current batch loss = 0.1229\n",
      "    Step 190: current batch loss = 0.0688\n",
      "    Step 190: current batch loss = 0.0688\n",
      "    Step 200: current batch loss = 0.0486\n",
      "    Step 200: current batch loss = 0.0486\n",
      "    Step 210: current batch loss = 0.0599\n",
      "    Step 210: current batch loss = 0.0599\n",
      "    Step 220: current batch loss = 0.0403\n",
      "    Step 220: current batch loss = 0.0403\n",
      "    Step 230: current batch loss = 0.0745\n",
      "    Step 230: current batch loss = 0.0745\n",
      "    Step 240: current batch loss = 0.0897\n",
      "    Step 240: current batch loss = 0.0897\n",
      "    Step 250: current batch loss = 0.0768\n",
      "  Epoch 3 Average Train Loss: 0.0953\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0768\n",
      "  Epoch 3 Average Train Loss: 0.0953\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.1158\n",
      "    Validation Step 10: current batch loss = 0.1158\n",
      "    Validation Step 20: current batch loss = 0.0586\n",
      "    Validation Step 20: current batch loss = 0.0586\n",
      "    Validation Step 30: current batch loss = 0.0880\n",
      "    Validation Step 30: current batch loss = 0.0880\n",
      "    Validation Step 40: current batch loss = 0.1376\n",
      "    Validation Step 40: current batch loss = 0.1376\n",
      "    Validation Step 50: current batch loss = 0.0727\n",
      "    Validation Step 50: current batch loss = 0.0727\n",
      "    Validation Step 60: current batch loss = 0.0271\n",
      "    Validation Step 60: current batch loss = 0.0271\n",
      "  Epoch 3 Average Validation Loss: 0.0819\n",
      "    New best validation loss: 0.0819. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 4/20 ===\n",
      "  Training...\n",
      "  Epoch 3 Average Validation Loss: 0.0819\n",
      "    New best validation loss: 0.0819. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 4/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0407\n",
      "    Step 10: current batch loss = 0.0407\n",
      "    Step 20: current batch loss = 0.0422\n",
      "    Step 20: current batch loss = 0.0422\n",
      "    Step 30: current batch loss = 0.0529\n",
      "    Step 30: current batch loss = 0.0529\n",
      "    Step 40: current batch loss = 0.0318\n",
      "    Step 40: current batch loss = 0.0318\n",
      "    Step 50: current batch loss = 0.0564\n",
      "    Step 50: current batch loss = 0.0564\n",
      "    Step 60: current batch loss = 0.0702\n",
      "    Step 60: current batch loss = 0.0702\n",
      "    Step 70: current batch loss = 0.0701\n",
      "    Step 70: current batch loss = 0.0701\n",
      "    Step 80: current batch loss = 0.0430\n",
      "    Step 80: current batch loss = 0.0430\n",
      "    Step 90: current batch loss = 0.0401\n",
      "    Step 90: current batch loss = 0.0401\n",
      "    Step 100: current batch loss = 0.0422\n",
      "    Step 100: current batch loss = 0.0422\n",
      "    Step 110: current batch loss = 0.0559\n",
      "    Step 110: current batch loss = 0.0559\n",
      "    Step 120: current batch loss = 0.0271\n",
      "    Step 120: current batch loss = 0.0271\n",
      "    Step 130: current batch loss = 0.0576\n",
      "    Step 130: current batch loss = 0.0576\n",
      "    Step 140: current batch loss = 0.0575\n",
      "    Step 140: current batch loss = 0.0575\n",
      "    Step 150: current batch loss = 0.0391\n",
      "    Step 150: current batch loss = 0.0391\n",
      "    Step 160: current batch loss = 0.0623\n",
      "    Step 160: current batch loss = 0.0623\n",
      "    Step 170: current batch loss = 0.0393\n",
      "    Step 170: current batch loss = 0.0393\n",
      "    Step 180: current batch loss = 0.0404\n",
      "    Step 180: current batch loss = 0.0404\n",
      "    Step 190: current batch loss = 0.0277\n",
      "    Step 190: current batch loss = 0.0277\n",
      "    Step 200: current batch loss = 0.0375\n",
      "    Step 200: current batch loss = 0.0375\n",
      "    Step 210: current batch loss = 0.0321\n",
      "    Step 210: current batch loss = 0.0321\n",
      "    Step 220: current batch loss = 0.0487\n",
      "    Step 220: current batch loss = 0.0487\n",
      "    Step 230: current batch loss = 0.0313\n",
      "    Step 230: current batch loss = 0.0313\n",
      "    Step 240: current batch loss = 0.0191\n",
      "    Step 240: current batch loss = 0.0191\n",
      "    Step 250: current batch loss = 0.0387\n",
      "  Epoch 4 Average Train Loss: 0.0433\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0387\n",
      "  Epoch 4 Average Train Loss: 0.0433\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0789\n",
      "    Validation Step 10: current batch loss = 0.0789\n",
      "    Validation Step 20: current batch loss = 0.0407\n",
      "    Validation Step 20: current batch loss = 0.0407\n",
      "    Validation Step 30: current batch loss = 0.0607\n",
      "    Validation Step 30: current batch loss = 0.0607\n",
      "    Validation Step 40: current batch loss = 0.0997\n",
      "    Validation Step 40: current batch loss = 0.0997\n",
      "    Validation Step 50: current batch loss = 0.0561\n",
      "    Validation Step 50: current batch loss = 0.0561\n",
      "    Validation Step 60: current batch loss = 0.0104\n",
      "    Validation Step 60: current batch loss = 0.0104\n",
      "  Epoch 4 Average Validation Loss: 0.0588\n",
      "    New best validation loss: 0.0588. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 5/20 ===\n",
      "  Training...\n",
      "  Epoch 4 Average Validation Loss: 0.0588\n",
      "    New best validation loss: 0.0588. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 5/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0184\n",
      "    Step 10: current batch loss = 0.0184\n",
      "    Step 20: current batch loss = 0.0194\n",
      "    Step 20: current batch loss = 0.0194\n",
      "    Step 30: current batch loss = 0.0295\n",
      "    Step 30: current batch loss = 0.0295\n",
      "    Step 40: current batch loss = 0.0121\n",
      "    Step 40: current batch loss = 0.0121\n",
      "    Step 50: current batch loss = 0.0224\n",
      "    Step 50: current batch loss = 0.0224\n",
      "    Step 60: current batch loss = 0.0324\n",
      "    Step 60: current batch loss = 0.0324\n",
      "    Step 70: current batch loss = 0.0119\n",
      "    Step 70: current batch loss = 0.0119\n",
      "    Step 80: current batch loss = 0.0464\n",
      "    Step 80: current batch loss = 0.0464\n",
      "    Step 90: current batch loss = 0.0084\n",
      "    Step 90: current batch loss = 0.0084\n",
      "    Step 100: current batch loss = 0.0190\n",
      "    Step 100: current batch loss = 0.0190\n",
      "    Step 110: current batch loss = 0.0156\n",
      "    Step 110: current batch loss = 0.0156\n",
      "    Step 120: current batch loss = 0.0105\n",
      "    Step 120: current batch loss = 0.0105\n",
      "    Step 130: current batch loss = 0.0197\n",
      "    Step 130: current batch loss = 0.0197\n",
      "    Step 140: current batch loss = 0.0148\n",
      "    Step 140: current batch loss = 0.0148\n",
      "    Step 150: current batch loss = 0.0130\n",
      "    Step 150: current batch loss = 0.0130\n",
      "    Step 160: current batch loss = 0.0309\n",
      "    Step 160: current batch loss = 0.0309\n",
      "    Step 170: current batch loss = 0.0077\n",
      "    Step 170: current batch loss = 0.0077\n",
      "    Step 180: current batch loss = 0.0253\n",
      "    Step 180: current batch loss = 0.0253\n",
      "    Step 190: current batch loss = 0.0158\n",
      "    Step 190: current batch loss = 0.0158\n",
      "    Step 200: current batch loss = 0.0072\n",
      "    Step 200: current batch loss = 0.0072\n",
      "    Step 210: current batch loss = 0.0164\n",
      "    Step 210: current batch loss = 0.0164\n",
      "    Step 220: current batch loss = 0.0187\n",
      "    Step 220: current batch loss = 0.0187\n",
      "    Step 230: current batch loss = 0.0231\n",
      "    Step 230: current batch loss = 0.0231\n",
      "    Step 240: current batch loss = 0.0110\n",
      "    Step 240: current batch loss = 0.0110\n",
      "    Step 250: current batch loss = 0.0261\n",
      "  Epoch 5 Average Train Loss: 0.0220\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0261\n",
      "  Epoch 5 Average Train Loss: 0.0220\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0645\n",
      "    Validation Step 10: current batch loss = 0.0645\n",
      "    Validation Step 20: current batch loss = 0.0358\n",
      "    Validation Step 20: current batch loss = 0.0358\n",
      "    Validation Step 30: current batch loss = 0.0583\n",
      "    Validation Step 30: current batch loss = 0.0583\n",
      "    Validation Step 40: current batch loss = 0.0807\n",
      "    Validation Step 40: current batch loss = 0.0807\n",
      "    Validation Step 50: current batch loss = 0.0506\n",
      "    Validation Step 50: current batch loss = 0.0506\n",
      "    Validation Step 60: current batch loss = 0.0048\n",
      "    Validation Step 60: current batch loss = 0.0048\n",
      "  Epoch 5 Average Validation Loss: 0.0518\n",
      "    New best validation loss: 0.0518. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 6/20 ===\n",
      "  Training...\n",
      "  Epoch 5 Average Validation Loss: 0.0518\n",
      "    New best validation loss: 0.0518. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 6/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0131\n",
      "    Step 10: current batch loss = 0.0131\n",
      "    Step 20: current batch loss = 0.0117\n",
      "    Step 20: current batch loss = 0.0117\n",
      "    Step 30: current batch loss = 0.0050\n",
      "    Step 30: current batch loss = 0.0050\n",
      "    Step 40: current batch loss = 0.0081\n",
      "    Step 40: current batch loss = 0.0081\n",
      "    Step 50: current batch loss = 0.0218\n",
      "    Step 50: current batch loss = 0.0218\n",
      "    Step 60: current batch loss = 0.0215\n",
      "    Step 60: current batch loss = 0.0215\n",
      "    Step 70: current batch loss = 0.0243\n",
      "    Step 70: current batch loss = 0.0243\n",
      "    Step 80: current batch loss = 0.0127\n",
      "    Step 80: current batch loss = 0.0127\n",
      "    Step 90: current batch loss = 0.0121\n",
      "    Step 90: current batch loss = 0.0121\n",
      "    Step 100: current batch loss = 0.0246\n",
      "    Step 100: current batch loss = 0.0246\n",
      "    Step 110: current batch loss = 0.0125\n",
      "    Step 110: current batch loss = 0.0125\n",
      "    Step 120: current batch loss = 0.0134\n",
      "    Step 120: current batch loss = 0.0134\n",
      "    Step 130: current batch loss = 0.0057\n",
      "    Step 130: current batch loss = 0.0057\n",
      "    Step 140: current batch loss = 0.0154\n",
      "    Step 140: current batch loss = 0.0154\n",
      "    Step 150: current batch loss = 0.0282\n",
      "    Step 150: current batch loss = 0.0282\n",
      "    Step 160: current batch loss = 0.0179\n",
      "    Step 160: current batch loss = 0.0179\n",
      "    Step 170: current batch loss = 0.0090\n",
      "    Step 170: current batch loss = 0.0090\n",
      "    Step 180: current batch loss = 0.0066\n",
      "    Step 180: current batch loss = 0.0066\n",
      "    Step 190: current batch loss = 0.0150\n",
      "    Step 190: current batch loss = 0.0150\n",
      "    Step 200: current batch loss = 0.0108\n",
      "    Step 200: current batch loss = 0.0108\n",
      "    Step 210: current batch loss = 0.0170\n",
      "    Step 210: current batch loss = 0.0170\n",
      "    Step 220: current batch loss = 0.0328\n",
      "    Step 220: current batch loss = 0.0328\n",
      "    Step 230: current batch loss = 0.0307\n",
      "    Step 230: current batch loss = 0.0307\n",
      "    Step 240: current batch loss = 0.0156\n",
      "    Step 240: current batch loss = 0.0156\n",
      "    Step 250: current batch loss = 0.0215\n",
      "  Epoch 6 Average Train Loss: 0.0143\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0215\n",
      "  Epoch 6 Average Train Loss: 0.0143\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0574\n",
      "    Validation Step 10: current batch loss = 0.0574\n",
      "    Validation Step 20: current batch loss = 0.0311\n",
      "    Validation Step 20: current batch loss = 0.0311\n",
      "    Validation Step 30: current batch loss = 0.0630\n",
      "    Validation Step 30: current batch loss = 0.0630\n",
      "    Validation Step 40: current batch loss = 0.0939\n",
      "    Validation Step 40: current batch loss = 0.0939\n",
      "    Validation Step 50: current batch loss = 0.0496\n",
      "    Validation Step 50: current batch loss = 0.0496\n",
      "    Validation Step 60: current batch loss = 0.0066\n",
      "    Validation Step 60: current batch loss = 0.0066\n",
      "  Epoch 6 Average Validation Loss: 0.0511\n",
      "    New best validation loss: 0.0511. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 7/20 ===\n",
      "  Training...\n",
      "  Epoch 6 Average Validation Loss: 0.0511\n",
      "    New best validation loss: 0.0511. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 7/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0113\n",
      "    Step 10: current batch loss = 0.0113\n",
      "    Step 20: current batch loss = 0.0173\n",
      "    Step 20: current batch loss = 0.0173\n",
      "    Step 30: current batch loss = 0.0199\n",
      "    Step 30: current batch loss = 0.0199\n",
      "    Step 40: current batch loss = 0.0079\n",
      "    Step 40: current batch loss = 0.0079\n",
      "    Step 50: current batch loss = 0.0085\n",
      "    Step 50: current batch loss = 0.0085\n",
      "    Step 60: current batch loss = 0.0141\n",
      "    Step 60: current batch loss = 0.0141\n",
      "    Step 70: current batch loss = 0.0039\n",
      "    Step 70: current batch loss = 0.0039\n",
      "    Step 80: current batch loss = 0.0086\n",
      "    Step 80: current batch loss = 0.0086\n",
      "    Step 90: current batch loss = 0.0115\n",
      "    Step 90: current batch loss = 0.0115\n",
      "    Step 100: current batch loss = 0.0089\n",
      "    Step 100: current batch loss = 0.0089\n",
      "    Step 110: current batch loss = 0.0103\n",
      "    Step 110: current batch loss = 0.0103\n",
      "    Step 120: current batch loss = 0.0051\n",
      "    Step 120: current batch loss = 0.0051\n",
      "    Step 130: current batch loss = 0.0057\n",
      "    Step 130: current batch loss = 0.0057\n",
      "    Step 140: current batch loss = 0.0066\n",
      "    Step 140: current batch loss = 0.0066\n",
      "    Step 150: current batch loss = 0.0073\n",
      "    Step 150: current batch loss = 0.0073\n",
      "    Step 160: current batch loss = 0.0125\n",
      "    Step 160: current batch loss = 0.0125\n",
      "    Step 170: current batch loss = 0.0079\n",
      "    Step 170: current batch loss = 0.0079\n",
      "    Step 180: current batch loss = 0.0043\n",
      "    Step 180: current batch loss = 0.0043\n",
      "    Step 190: current batch loss = 0.0051\n",
      "    Step 190: current batch loss = 0.0051\n",
      "    Step 200: current batch loss = 0.0049\n",
      "    Step 200: current batch loss = 0.0049\n",
      "    Step 210: current batch loss = 0.0054\n",
      "    Step 210: current batch loss = 0.0054\n",
      "    Step 220: current batch loss = 0.0057\n",
      "    Step 220: current batch loss = 0.0057\n",
      "    Step 230: current batch loss = 0.0026\n",
      "    Step 230: current batch loss = 0.0026\n",
      "    Step 240: current batch loss = 0.0048\n",
      "    Step 240: current batch loss = 0.0048\n",
      "    Step 250: current batch loss = 0.0031\n",
      "  Epoch 7 Average Train Loss: 0.0078\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0031\n",
      "  Epoch 7 Average Train Loss: 0.0078\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0530\n",
      "    Validation Step 10: current batch loss = 0.0530\n",
      "    Validation Step 20: current batch loss = 0.0305\n",
      "    Validation Step 20: current batch loss = 0.0305\n",
      "    Validation Step 30: current batch loss = 0.0480\n",
      "    Validation Step 30: current batch loss = 0.0480\n",
      "    Validation Step 40: current batch loss = 0.0877\n",
      "    Validation Step 40: current batch loss = 0.0877\n",
      "    Validation Step 50: current batch loss = 0.0492\n",
      "    Validation Step 50: current batch loss = 0.0492\n",
      "    Validation Step 60: current batch loss = 0.0012\n",
      "    Validation Step 60: current batch loss = 0.0012\n",
      "  Epoch 7 Average Validation Loss: 0.0450\n",
      "    New best validation loss: 0.0450. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 8/20 ===\n",
      "  Training...\n",
      "  Epoch 7 Average Validation Loss: 0.0450\n",
      "    New best validation loss: 0.0450. Patience reset.\n",
      "    Best model checkpoint saved with additional training state.\n",
      "\n",
      "=== Epoch 8/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0020\n",
      "    Step 10: current batch loss = 0.0020\n",
      "    Step 20: current batch loss = 0.0028\n",
      "    Step 20: current batch loss = 0.0028\n",
      "    Step 30: current batch loss = 0.0036\n",
      "    Step 30: current batch loss = 0.0036\n",
      "    Step 40: current batch loss = 0.0024\n",
      "    Step 40: current batch loss = 0.0024\n",
      "    Step 50: current batch loss = 0.0020\n",
      "    Step 50: current batch loss = 0.0020\n",
      "    Step 60: current batch loss = 0.0015\n",
      "    Step 60: current batch loss = 0.0015\n",
      "    Step 70: current batch loss = 0.0076\n",
      "    Step 70: current batch loss = 0.0076\n",
      "    Step 80: current batch loss = 0.0050\n",
      "    Step 80: current batch loss = 0.0050\n",
      "    Step 90: current batch loss = 0.0038\n",
      "    Step 90: current batch loss = 0.0038\n",
      "    Step 100: current batch loss = 0.0027\n",
      "    Step 100: current batch loss = 0.0027\n",
      "    Step 110: current batch loss = 0.0049\n",
      "    Step 110: current batch loss = 0.0049\n",
      "    Step 120: current batch loss = 0.0038\n",
      "    Step 120: current batch loss = 0.0038\n",
      "    Step 130: current batch loss = 0.0040\n",
      "    Step 130: current batch loss = 0.0040\n",
      "    Step 140: current batch loss = 0.0029\n",
      "    Step 140: current batch loss = 0.0029\n",
      "    Step 150: current batch loss = 0.0063\n",
      "    Step 150: current batch loss = 0.0063\n",
      "    Step 160: current batch loss = 0.0031\n",
      "    Step 160: current batch loss = 0.0031\n",
      "    Step 170: current batch loss = 0.0052\n",
      "    Step 170: current batch loss = 0.0052\n",
      "    Step 180: current batch loss = 0.0022\n",
      "    Step 180: current batch loss = 0.0022\n",
      "    Step 190: current batch loss = 0.0029\n",
      "    Step 190: current batch loss = 0.0029\n",
      "    Step 200: current batch loss = 0.0022\n",
      "    Step 200: current batch loss = 0.0022\n",
      "    Step 210: current batch loss = 0.0030\n",
      "    Step 210: current batch loss = 0.0030\n",
      "    Step 220: current batch loss = 0.0037\n",
      "    Step 220: current batch loss = 0.0037\n",
      "    Step 230: current batch loss = 0.0025\n",
      "    Step 230: current batch loss = 0.0025\n",
      "    Step 240: current batch loss = 0.0028\n",
      "    Step 240: current batch loss = 0.0028\n",
      "    Step 250: current batch loss = 0.0035\n",
      "  Epoch 8 Average Train Loss: 0.0043\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0035\n",
      "  Epoch 8 Average Train Loss: 0.0043\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0525\n",
      "    Validation Step 10: current batch loss = 0.0525\n",
      "    Validation Step 20: current batch loss = 0.0304\n",
      "    Validation Step 20: current batch loss = 0.0304\n",
      "    Validation Step 30: current batch loss = 0.0450\n",
      "    Validation Step 30: current batch loss = 0.0450\n",
      "    Validation Step 40: current batch loss = 0.0715\n",
      "    Validation Step 40: current batch loss = 0.0715\n",
      "    Validation Step 50: current batch loss = 0.0485\n",
      "    Validation Step 50: current batch loss = 0.0485\n",
      "    Validation Step 60: current batch loss = 0.0008\n",
      "    Validation Step 60: current batch loss = 0.0008\n",
      "  Epoch 8 Average Validation Loss: 0.0451\n",
      "    Validation loss did not improve. Current: 0.0451, Best: 0.0450, Delta: -0.000093\n",
      "    Early stopping patience: 1/5\n",
      "\n",
      "=== Epoch 9/20 ===\n",
      "  Training...\n",
      "  Epoch 8 Average Validation Loss: 0.0451\n",
      "    Validation loss did not improve. Current: 0.0451, Best: 0.0450, Delta: -0.000093\n",
      "    Early stopping patience: 1/5\n",
      "\n",
      "=== Epoch 9/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0026\n",
      "    Step 10: current batch loss = 0.0026\n",
      "    Step 20: current batch loss = 0.0020\n",
      "    Step 20: current batch loss = 0.0020\n",
      "    Step 30: current batch loss = 0.0017\n",
      "    Step 30: current batch loss = 0.0017\n",
      "    Step 40: current batch loss = 0.0033\n",
      "    Step 40: current batch loss = 0.0033\n",
      "    Step 50: current batch loss = 0.0015\n",
      "    Step 50: current batch loss = 0.0015\n",
      "    Step 60: current batch loss = 0.0011\n",
      "    Step 60: current batch loss = 0.0011\n",
      "    Step 70: current batch loss = 0.0013\n",
      "    Step 70: current batch loss = 0.0013\n",
      "    Step 80: current batch loss = 0.0012\n",
      "    Step 80: current batch loss = 0.0012\n",
      "    Step 90: current batch loss = 0.0013\n",
      "    Step 90: current batch loss = 0.0013\n",
      "    Step 100: current batch loss = 0.0016\n",
      "    Step 100: current batch loss = 0.0016\n",
      "    Step 110: current batch loss = 0.0025\n",
      "    Step 110: current batch loss = 0.0025\n",
      "    Step 120: current batch loss = 0.0017\n",
      "    Step 120: current batch loss = 0.0017\n",
      "    Step 130: current batch loss = 0.0021\n",
      "    Step 130: current batch loss = 0.0021\n",
      "    Step 140: current batch loss = 0.0021\n",
      "    Step 140: current batch loss = 0.0021\n",
      "    Step 150: current batch loss = 0.0019\n",
      "    Step 150: current batch loss = 0.0019\n",
      "    Step 160: current batch loss = 0.0022\n",
      "    Step 160: current batch loss = 0.0022\n",
      "    Step 170: current batch loss = 0.0114\n",
      "    Step 170: current batch loss = 0.0114\n",
      "    Step 180: current batch loss = 0.0016\n",
      "    Step 180: current batch loss = 0.0016\n",
      "    Step 190: current batch loss = 0.0015\n",
      "    Step 190: current batch loss = 0.0015\n",
      "    Step 200: current batch loss = 0.0010\n",
      "    Step 200: current batch loss = 0.0010\n",
      "    Step 210: current batch loss = 0.0017\n",
      "    Step 210: current batch loss = 0.0017\n",
      "    Step 220: current batch loss = 0.0030\n",
      "    Step 220: current batch loss = 0.0030\n",
      "    Step 230: current batch loss = 0.0027\n",
      "    Step 230: current batch loss = 0.0027\n",
      "    Step 240: current batch loss = 0.0013\n",
      "    Step 240: current batch loss = 0.0013\n",
      "    Step 250: current batch loss = 0.0074\n",
      "  Epoch 9 Average Train Loss: 0.0023\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0074\n",
      "  Epoch 9 Average Train Loss: 0.0023\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0725\n",
      "    Validation Step 10: current batch loss = 0.0725\n",
      "    Validation Step 20: current batch loss = 0.0305\n",
      "    Validation Step 20: current batch loss = 0.0305\n",
      "    Validation Step 30: current batch loss = 0.0527\n",
      "    Validation Step 30: current batch loss = 0.0527\n",
      "    Validation Step 40: current batch loss = 0.0829\n",
      "    Validation Step 40: current batch loss = 0.0829\n",
      "    Validation Step 50: current batch loss = 0.0461\n",
      "    Validation Step 50: current batch loss = 0.0461\n",
      "    Validation Step 60: current batch loss = 0.0009\n",
      "    Validation Step 60: current batch loss = 0.0009\n",
      "  Epoch 9 Average Validation Loss: 0.0470\n",
      "    Validation loss did not improve. Current: 0.0470, Best: 0.0450, Delta: -0.002037\n",
      "    Early stopping patience: 2/5\n",
      "\n",
      "=== Epoch 10/20 ===\n",
      "  Training...\n",
      "  Epoch 9 Average Validation Loss: 0.0470\n",
      "    Validation loss did not improve. Current: 0.0470, Best: 0.0450, Delta: -0.002037\n",
      "    Early stopping patience: 2/5\n",
      "\n",
      "=== Epoch 10/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0026\n",
      "    Step 10: current batch loss = 0.0026\n",
      "    Step 20: current batch loss = 0.0026\n",
      "    Step 20: current batch loss = 0.0026\n",
      "    Step 30: current batch loss = 0.0025\n",
      "    Step 30: current batch loss = 0.0025\n",
      "    Step 40: current batch loss = 0.0022\n",
      "    Step 40: current batch loss = 0.0022\n",
      "    Step 50: current batch loss = 0.0052\n",
      "    Step 50: current batch loss = 0.0052\n",
      "    Step 60: current batch loss = 0.0071\n",
      "    Step 60: current batch loss = 0.0071\n",
      "    Step 70: current batch loss = 0.0023\n",
      "    Step 70: current batch loss = 0.0023\n",
      "    Step 80: current batch loss = 0.0025\n",
      "    Step 80: current batch loss = 0.0025\n",
      "    Step 90: current batch loss = 0.0023\n",
      "    Step 90: current batch loss = 0.0023\n",
      "    Step 100: current batch loss = 0.0034\n",
      "    Step 100: current batch loss = 0.0034\n",
      "    Step 110: current batch loss = 0.0109\n",
      "    Step 110: current batch loss = 0.0109\n",
      "    Step 120: current batch loss = 0.0051\n",
      "    Step 120: current batch loss = 0.0051\n",
      "    Step 130: current batch loss = 0.0104\n",
      "    Step 130: current batch loss = 0.0104\n",
      "    Step 140: current batch loss = 0.0042\n",
      "    Step 140: current batch loss = 0.0042\n",
      "    Step 150: current batch loss = 0.0185\n",
      "    Step 150: current batch loss = 0.0185\n",
      "    Step 160: current batch loss = 0.0143\n",
      "    Step 160: current batch loss = 0.0143\n",
      "    Step 170: current batch loss = 0.0126\n",
      "    Step 170: current batch loss = 0.0126\n",
      "    Step 180: current batch loss = 0.0084\n",
      "    Step 180: current batch loss = 0.0084\n",
      "    Step 190: current batch loss = 0.0057\n",
      "    Step 190: current batch loss = 0.0057\n",
      "    Step 200: current batch loss = 0.0103\n",
      "    Step 200: current batch loss = 0.0103\n",
      "    Step 210: current batch loss = 0.0028\n",
      "    Step 210: current batch loss = 0.0028\n",
      "    Step 220: current batch loss = 0.0128\n",
      "    Step 220: current batch loss = 0.0128\n",
      "    Step 230: current batch loss = 0.0042\n",
      "    Step 230: current batch loss = 0.0042\n",
      "    Step 240: current batch loss = 0.0071\n",
      "    Step 240: current batch loss = 0.0071\n",
      "    Step 250: current batch loss = 0.0018\n",
      "  Epoch 10 Average Train Loss: 0.0075\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0018\n",
      "  Epoch 10 Average Train Loss: 0.0075\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0504\n",
      "    Validation Step 10: current batch loss = 0.0504\n",
      "    Validation Step 20: current batch loss = 0.0310\n",
      "    Validation Step 20: current batch loss = 0.0310\n",
      "    Validation Step 30: current batch loss = 0.0454\n",
      "    Validation Step 30: current batch loss = 0.0454\n",
      "    Validation Step 40: current batch loss = 0.0963\n",
      "    Validation Step 40: current batch loss = 0.0963\n",
      "    Validation Step 50: current batch loss = 0.0460\n",
      "    Validation Step 50: current batch loss = 0.0460\n",
      "    Validation Step 60: current batch loss = 0.0009\n",
      "    Validation Step 60: current batch loss = 0.0009\n",
      "  Epoch 10 Average Validation Loss: 0.0480\n",
      "    Validation loss did not improve. Current: 0.0480, Best: 0.0450, Delta: -0.003048\n",
      "    Early stopping patience: 3/5\n",
      "\n",
      "=== Epoch 11/20 ===\n",
      "  Training...\n",
      "  Epoch 10 Average Validation Loss: 0.0480\n",
      "    Validation loss did not improve. Current: 0.0480, Best: 0.0450, Delta: -0.003048\n",
      "    Early stopping patience: 3/5\n",
      "\n",
      "=== Epoch 11/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0027\n",
      "    Step 10: current batch loss = 0.0027\n",
      "    Step 20: current batch loss = 0.0029\n",
      "    Step 20: current batch loss = 0.0029\n",
      "    Step 30: current batch loss = 0.0023\n",
      "    Step 30: current batch loss = 0.0023\n",
      "    Step 40: current batch loss = 0.0031\n",
      "    Step 40: current batch loss = 0.0031\n",
      "    Step 50: current batch loss = 0.0011\n",
      "    Step 50: current batch loss = 0.0011\n",
      "    Step 60: current batch loss = 0.0024\n",
      "    Step 60: current batch loss = 0.0024\n",
      "    Step 70: current batch loss = 0.0020\n",
      "    Step 70: current batch loss = 0.0020\n",
      "    Step 80: current batch loss = 0.0016\n",
      "    Step 80: current batch loss = 0.0016\n",
      "    Step 90: current batch loss = 0.0062\n",
      "    Step 90: current batch loss = 0.0062\n",
      "    Step 100: current batch loss = 0.0025\n",
      "    Step 100: current batch loss = 0.0025\n",
      "    Step 110: current batch loss = 0.0109\n",
      "    Step 110: current batch loss = 0.0109\n",
      "    Step 120: current batch loss = 0.0081\n",
      "    Step 120: current batch loss = 0.0081\n",
      "    Step 130: current batch loss = 0.0026\n",
      "    Step 130: current batch loss = 0.0026\n",
      "    Step 140: current batch loss = 0.0035\n",
      "    Step 140: current batch loss = 0.0035\n",
      "    Step 150: current batch loss = 0.0031\n",
      "    Step 150: current batch loss = 0.0031\n",
      "    Step 160: current batch loss = 0.0039\n",
      "    Step 160: current batch loss = 0.0039\n",
      "    Step 170: current batch loss = 0.0166\n",
      "    Step 170: current batch loss = 0.0166\n",
      "    Step 180: current batch loss = 0.0088\n",
      "    Step 180: current batch loss = 0.0088\n",
      "    Step 190: current batch loss = 0.0046\n",
      "    Step 190: current batch loss = 0.0046\n",
      "    Step 200: current batch loss = 0.0065\n",
      "    Step 200: current batch loss = 0.0065\n",
      "    Step 210: current batch loss = 0.0053\n",
      "    Step 210: current batch loss = 0.0053\n",
      "    Step 220: current batch loss = 0.0022\n",
      "    Step 220: current batch loss = 0.0022\n",
      "    Step 230: current batch loss = 0.0022\n",
      "    Step 230: current batch loss = 0.0022\n",
      "    Step 240: current batch loss = 0.0036\n",
      "    Step 240: current batch loss = 0.0036\n",
      "    Step 250: current batch loss = 0.0031\n",
      "  Epoch 11 Average Train Loss: 0.0056\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0031\n",
      "  Epoch 11 Average Train Loss: 0.0056\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0509\n",
      "    Validation Step 10: current batch loss = 0.0509\n",
      "    Validation Step 20: current batch loss = 0.0322\n",
      "    Validation Step 20: current batch loss = 0.0322\n",
      "    Validation Step 30: current batch loss = 0.0447\n",
      "    Validation Step 30: current batch loss = 0.0447\n",
      "    Validation Step 40: current batch loss = 0.0857\n",
      "    Validation Step 40: current batch loss = 0.0857\n",
      "    Validation Step 50: current batch loss = 0.0481\n",
      "    Validation Step 50: current batch loss = 0.0481\n",
      "    Validation Step 60: current batch loss = 0.0007\n",
      "    Validation Step 60: current batch loss = 0.0007\n",
      "  Epoch 11 Average Validation Loss: 0.0473\n",
      "    Validation loss did not improve. Current: 0.0473, Best: 0.0450, Delta: -0.002269\n",
      "    Early stopping patience: 4/5\n",
      "\n",
      "=== Epoch 12/20 ===\n",
      "  Training...\n",
      "  Epoch 11 Average Validation Loss: 0.0473\n",
      "    Validation loss did not improve. Current: 0.0473, Best: 0.0450, Delta: -0.002269\n",
      "    Early stopping patience: 4/5\n",
      "\n",
      "=== Epoch 12/20 ===\n",
      "  Training...\n",
      "    Step 10: current batch loss = 0.0092\n",
      "    Step 10: current batch loss = 0.0092\n",
      "    Step 20: current batch loss = 0.0015\n",
      "    Step 20: current batch loss = 0.0015\n",
      "    Step 30: current batch loss = 0.0022\n",
      "    Step 30: current batch loss = 0.0022\n",
      "    Step 40: current batch loss = 0.0034\n",
      "    Step 40: current batch loss = 0.0034\n",
      "    Step 50: current batch loss = 0.0065\n",
      "    Step 50: current batch loss = 0.0065\n",
      "    Step 60: current batch loss = 0.0031\n",
      "    Step 60: current batch loss = 0.0031\n",
      "    Step 70: current batch loss = 0.0026\n",
      "    Step 70: current batch loss = 0.0026\n",
      "    Step 80: current batch loss = 0.0017\n",
      "    Step 80: current batch loss = 0.0017\n",
      "    Step 90: current batch loss = 0.0033\n",
      "    Step 90: current batch loss = 0.0033\n",
      "    Step 100: current batch loss = 0.0042\n",
      "    Step 100: current batch loss = 0.0042\n",
      "    Step 110: current batch loss = 0.0016\n",
      "    Step 110: current batch loss = 0.0016\n",
      "    Step 120: current batch loss = 0.0035\n",
      "    Step 120: current batch loss = 0.0035\n",
      "    Step 130: current batch loss = 0.0041\n",
      "    Step 130: current batch loss = 0.0041\n",
      "    Step 140: current batch loss = 0.0098\n",
      "    Step 140: current batch loss = 0.0098\n",
      "    Step 150: current batch loss = 0.0025\n",
      "    Step 150: current batch loss = 0.0025\n",
      "    Step 160: current batch loss = 0.0098\n",
      "    Step 160: current batch loss = 0.0098\n",
      "    Step 170: current batch loss = 0.0008\n",
      "    Step 170: current batch loss = 0.0008\n",
      "    Step 180: current batch loss = 0.0027\n",
      "    Step 180: current batch loss = 0.0027\n",
      "    Step 190: current batch loss = 0.0147\n",
      "    Step 190: current batch loss = 0.0147\n",
      "    Step 200: current batch loss = 0.0394\n",
      "    Step 200: current batch loss = 0.0394\n",
      "    Step 210: current batch loss = 0.0121\n",
      "    Step 210: current batch loss = 0.0121\n",
      "    Step 220: current batch loss = 0.0031\n",
      "    Step 220: current batch loss = 0.0031\n",
      "    Step 230: current batch loss = 0.0197\n",
      "    Step 230: current batch loss = 0.0197\n",
      "    Step 240: current batch loss = 0.0026\n",
      "    Step 240: current batch loss = 0.0026\n",
      "    Step 250: current batch loss = 0.0014\n",
      "  Epoch 12 Average Train Loss: 0.0050\n",
      "  Validating...\n",
      "    Step 250: current batch loss = 0.0014\n",
      "  Epoch 12 Average Train Loss: 0.0050\n",
      "  Validating...\n",
      "    Validation Step 10: current batch loss = 0.0462\n",
      "    Validation Step 10: current batch loss = 0.0462\n",
      "    Validation Step 20: current batch loss = 0.0328\n",
      "    Validation Step 20: current batch loss = 0.0328\n",
      "    Validation Step 30: current batch loss = 0.0443\n",
      "    Validation Step 30: current batch loss = 0.0443\n",
      "    Validation Step 40: current batch loss = 0.1046\n",
      "    Validation Step 40: current batch loss = 0.1046\n",
      "    Validation Step 50: current batch loss = 0.0493\n",
      "    Validation Step 50: current batch loss = 0.0493\n",
      "    Validation Step 60: current batch loss = 0.0003\n",
      "    Validation Step 60: current batch loss = 0.0003\n",
      "  Epoch 12 Average Validation Loss: 0.0474\n",
      "    Validation loss did not improve. Current: 0.0474, Best: 0.0450, Delta: -0.002361\n",
      "    Early stopping patience: 5/5\n",
      "\n",
      "--- Early Stopping triggered at epoch 12 ---\n",
      "--- Best validation loss: 0.0450 achieved at epoch 7 ---\n",
      "\n",
      "--- Training Finished ---\n",
      "  Epoch 12 Average Validation Loss: 0.0474\n",
      "    Validation loss did not improve. Current: 0.0474, Best: 0.0450, Delta: -0.002361\n",
      "    Early stopping patience: 5/5\n",
      "\n",
      "--- Early Stopping triggered at epoch 12 ---\n",
      "--- Best validation loss: 0.0450 achieved at epoch 7 ---\n",
      "\n",
      "--- Training Finished ---\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAcxpJREFUeJzt3Xd4VGXexvH7zKR3agqEKkhNqLKAIihdURQVWVSwF1ARy1oRsIu+olhZXbEsig10FYSAIIKoIBJAAUV6CT2dJJOZ8/6RZMiQQApJTpL5fq5rdjKn/maewc2d5zzPMUzTNAUAAAAAOCWb1QUAAAAAQHVHcAIAAACAEhCcAAAAAKAEBCcAAAAAKAHBCQAAAABKQHACAAAAgBIQnAAAAACgBAQnAAAAACgBwQkAAAAASkBwAlBjjR07Vs2aNSvXvpMnT5ZhGBVbUDWzY8cOGYahWbNmVfm5DcPQ5MmT3a9nzZolwzC0Y8eOEvdt1qyZxo4dW6H1nMl3BSgvwzA0fvx4q8sAUEEITgAqnGEYpXosW7bM6lK93l133SXDMLR169ZTbvPII4/IMAytX7++Cisru3379mny5Mlat26d1aW4FYTXF154wepSSmXXrl267bbb1KxZM/n7+6thw4YaPny4Vq5caXVpxTrdf19uu+02q8sDUMv4WF0AgNrngw8+8Hj9/vvvKyEhocjytm3bntF5/v3vf8vlcpVr30cffVQPPvjgGZ2/Nhg9erRmzJih2bNna9KkScVu89FHH6ljx46Ki4sr93muvfZaXX311fL39y/3MUqyb98+TZkyRc2aNVOnTp081p3Jd8VbrFy5UkOHDpUk3XTTTWrXrp2SkpI0a9YsnXfeeXr55Zd15513WlxlUQMGDNB1111XZHnr1q0tqAZAbUZwAlDhrrnmGo/XP/30kxISEoosP1lmZqaCgoJKfR5fX99y1SdJPj4+8vHhP4E9evTQWWedpY8++qjY4LRq1Spt375dzz777Bmdx263y263n9ExzsSZfFe8wbFjx3TFFVcoMDBQK1euVMuWLd3rJk6cqEGDBmnChAnq2rWrevXqVWV1ZWVlyc/PTzbbqS+Qad26dYn/bQGAisClegAs0bdvX3Xo0EG//vqr+vTpo6CgID388MOSpC+//FIXXXSRYmJi5O/vr5YtW+qJJ56Q0+n0OMbJ41YKXxY1c+ZMtWzZUv7+/urevbtWr17tsW9xY5wKxiPMmzdPHTp0kL+/v9q3b69vv/22SP3Lli1Tt27dFBAQoJYtW+qtt94q9bipH374QVdeeaWaNGkif39/xcbG6p577tHx48eLvL+QkBDt3btXw4cPV0hIiBo0aKD77ruvyGeRnJyssWPHKjw8XBERERozZoySk5NLrEXK63XavHmz1q5dW2Td7NmzZRiGRo0apZycHE2aNEldu3ZVeHi4goODdd5552np0qUlnqO4MU6maerJJ59U48aNFRQUpH79+un3338vsu/Ro0d13333qWPHjgoJCVFYWJiGDBmixMRE9zbLli1T9+7dJUnXX3+9+3KtgvFdxY1xysjI0L333qvY2Fj5+/vr7LPP1gsvvCDTND22K8v3orwOHjyoG2+8UZGRkQoICFB8fLzee++9Itt9/PHH6tq1q0JDQxUWFqaOHTvq5Zdfdq93OByaMmWKWrVqpYCAANWrV0/nnnuuEhISTnv+t956S0lJSZo2bZpHaJKkwMBAvffeezIMQ1OnTpUkrVmzRoZhFFvjwoULZRiGvv76a/eyvXv36oYbblBkZKT78/vPf/7jsd+yZctkGIY+/vhjPfroo2rUqJGCgoKUmppa8gdYgsL/venVq5cCAwPVvHlzvfnmm0W2LW1buFwuvfzyy+rYsaMCAgLUoEEDDR48WGvWrCmybUnfnbS0NE2YMMHjEskBAwYU+28SgHX4cysAyxw5ckRDhgzR1VdfrWuuuUaRkZGS8n7JDgkJ0cSJExUSEqLvvvtOkyZNUmpqqqZNm1bicWfPnq20tDTdeuutMgxDzz//vC6//HJt27atxJ6HFStW6IsvvtAdd9yh0NBQvfLKKxoxYoR27dqlevXqSZJ+++03DR48WNHR0ZoyZYqcTqemTp2qBg0alOp9f/rpp8rMzNTtt9+uevXq6ZdfftGMGTO0Z88effrppx7bOp1ODRo0SD169NALL7ygxYsX68UXX1TLli11++23S8oLIJdeeqlWrFih2267TW3bttXcuXM1ZsyYUtUzevRoTZkyRbNnz1aXLl08zv3JJ5/ovPPOU5MmTXT48GG9/fbbGjVqlG6++WalpaXpnXfe0aBBg/TLL78UuTyuJJMmTdKTTz6poUOHaujQoVq7dq0GDhyonJwcj+22bdumefPm6corr1Tz5s114MABvfXWWzr//PP1xx9/KCYmRm3bttXUqVM1adIk3XLLLTrvvPMk6ZS9I6Zp6pJLLtHSpUt14403qlOnTlq4cKHuv/9+7d27Vy+99JLH9qX5XpTX8ePH1bdvX23dulXjx49X8+bN9emnn2rs2LFKTk7W3XffLUlKSEjQqFGjdOGFF+q5556TJG3atEkrV650bzN58mQ988wzuummm3TOOecoNTVVa9as0dq1azVgwIBT1vC///1PAQEBuuqqq4pd37x5c5177rn67rvvdPz4cXXr1k0tWrTQJ598UuR7NmfOHNWpU0eDBg2SJB04cED/+Mc/3AG0QYMGWrBggW688UalpqZqwoQJHvs/8cQT8vPz03333afs7Gz5+fmd9vPLysrS4cOHiywPCwvz2PfYsWMaOnSorrrqKo0aNUqffPKJbr/9dvn5+emGG26QVPq2kKQbb7xRs2bN0pAhQ3TTTTcpNzdXP/zwg3766Sd169bNvV1pvju33XabPvvsM40fP17t2rXTkSNHtGLFCm3atMnj3yQAi5kAUMnGjRtnnvyfm/PPP9+UZL755ptFts/MzCyy7NZbbzWDgoLMrKws97IxY8aYTZs2db/evn27KcmsV6+eefToUffyL7/80pRk/u9//3Mve/zxx4vUJMn08/Mzt27d6l6WmJhoSjJnzJjhXjZs2DAzKCjI3Lt3r3vZX3/9Zfr4+BQ5ZnGKe3/PPPOMaRiGuXPnTo/3J8mcOnWqx7adO3c2u3bt6n49b948U5L5/PPPu5fl5uaa5513ninJfPfdd0usqXv37mbjxo1Np9PpXvbtt9+aksy33nrLfczs7GyP/Y4dO2ZGRkaaN9xwg8dySebjjz/ufv3uu++akszt27ebpmmaBw8eNP38/MyLLrrIdLlc7u0efvhhU5I5ZswY97KsrCyPukwzr639/f09PpvVq1ef8v2e/F0p+MyefPJJj+2uuOIK0zAMj+9Aab8XxSn4Tk6bNu2U20yfPt2UZH744YfuZTk5OWbPnj3NkJAQMzU11TRN07z77rvNsLAwMzc395THio+PNy+66KLT1lSciIgIMz4+/rTb3HXXXaYkc/369aZpmuZDDz1k+vr6evxby87ONiMiIjy+DzfeeKMZHR1tHj582ON4V199tRkeHu7+97B06VJTktmiRYti/40UR9IpHx999JF7u4L/3rz44osetXbq1Mls2LChmZOTY5pm6dviu+++MyWZd911V5GaCn+fS/vdCQ8PN8eNG1eq9wzAOlyqB8Ay/v7+uv7664ssDwwMdP+clpamw4cP67zzzlNmZqY2b95c4nFHjhypOnXquF8X9D5s27atxH379+/vcalSXFycwsLC3Ps6nU4tXrxYw4cPV0xMjHu7s846S0OGDCnx+JLn+8vIyNDhw4fVq1cvmaap3377rcj2J88Odt5553m8l/nz58vHx8fdAyXljSkqy0D+a665Rnv27NHy5cvdy2bPni0/Pz9deeWV7mMW/AXf5XLp6NGjys3NVbdu3cp8SdHixYuVk5OjO++80+PyxpN7H6S870nBGBen06kjR44oJCREZ599drkvZZo/f77sdrvuuusuj+X33nuvTNPUggULPJaX9L04E/Pnz1dUVJRGjRrlXubr66u77rpL6enp+v777yVJERERysjIOO1ldxEREfr999/1119/lamGtLQ0hYaGnnabgvUFl86NHDlSDodDX3zxhXubRYsWKTk5WSNHjpSU17P3+eefa9iwYTJNU4cPH3Y/Bg0apJSUlCJtOGbMGI9/IyW59NJLlZCQUOTRr18/j+18fHx06623ul/7+fnp1ltv1cGDB/Xrr79KKn1bfP755zIMQ48//niRek6+XLc0352IiAj9/PPP2rdvX6nfN4CqR3ACYJlGjRoVexnO77//rssuu0zh4eEKCwtTgwYN3IO/U1JSSjxukyZNPF4XhKhjx46Ved+C/Qv2PXjwoI4fP66zzjqryHbFLSvOrl27NHbsWNWtW9c9bun888+XVPT9FYydOFU9krRz505FR0crJCTEY7uzzz67VPVI0tVXXy273a7Zs2dLyrv8ae7cuRoyZIhHCH3vvfcUFxfnHj/ToEEDffPNN6Vql8J27twpSWrVqpXH8gYNGnicT8oLaS+99JJatWolf39/1a9fXw0aNND69evLfN7C54+JiSkSFgpmeiyor0BJ34szsXPnTrVq1arIBAgn13LHHXeodevWGjJkiBo3bqwbbrihyFiZqVOnKjk5Wa1bt1bHjh11//33l2oa+dDQUKWlpZ12m4L1BZ9ZfHy82rRpozlz5ri3mTNnjurXr68LLrhAknTo0CElJydr5syZatCggcej4I8mBw8e9DhP8+bNS6y3sMaNG6t///5FHgWX/haIiYlRcHCwx7KCmfcKxt6Vti3+/vtvxcTEqG7duiXWV5rvzvPPP6+NGzcqNjZW55xzjiZPnlwhoRxAxSI4AbBMcX9VTk5O1vnnn6/ExERNnTpV//vf/5SQkOAe01GaKaVPNXubedKg/4retzScTqcGDBigb775Rv/61780b948JSQkuCcxOPn9VdVMdAWD0T///HM5HA7973//U1pamkaPHu3e5sMPP9TYsWPVsmVLvfPOO/r222+VkJCgCy64oFKn+n766ac1ceJE9enTRx9++KEWLlyohIQEtW/fvsqmGK/s70VpNGzYUOvWrdNXX33lHp81ZMgQjzFGffr00d9//63//Oc/6tChg95++2116dJFb7/99mmP3bZtW23ZskXZ2dmn3Gb9+vXy9fX1CLsjR47U0qVLdfjwYWVnZ+urr77SiBEj3DNWFrTPNddcU2yvUEJCgnr37u1xnrL0NtUEpfnuXHXVVdq2bZtmzJihmJgYTZs2Te3bty/S8wnAWkwOAaBaWbZsmY4cOaIvvvhCffr0cS/fvn27hVWd0LBhQwUEBBR7w9jT3US2wIYNG/Tnn3/qvffe87j3TEmznp1O06ZNtWTJEqWnp3v0Om3ZsqVMxxk9erS+/fZbLViwQLNnz1ZYWJiGDRvmXv/ZZ5+pRYsW+uKLLzwuRyrucqXS1CxJf/31l1q0aOFefujQoSK9OJ999pn69eund955x2N5cnKy6tev735dmhkNC59/8eLFRS5RK7gUtKC+qtC0aVOtX79eLpfLo6ejuFr8/Pw0bNgwDRs2TC6XS3fccYfeeustPfbYY+4ez7p16+r666/X9ddfr/T0dPXp00eTJ0/WTTfddMoaLr74Yq1atUqffvppsVN779ixQz/88IP69+/vEWxGjhypKVOm6PPPP1dkZKRSU1N19dVXu9c3aNBAoaGhcjqd6t+/f/k/pAqwb98+ZWRkePQ6/fnnn5LknnGxtG3RsmVLLVy4UEePHi1Vr1NpREdH64477tAdd9yhgwcPqkuXLnrqqadKfQkwgMpHjxOAaqXgr7OF/xqbk5Oj119/3aqSPNjtdvXv31/z5s3zGI+wdevWUv11uLj3Z5qmx5TSZTV06FDl5ubqjTfecC9zOp2aMWNGmY4zfPhwBQUF6fXXX9eCBQt0+eWXKyAg4LS1//zzz1q1alWZa+7fv798fX01Y8YMj+NNnz69yLZ2u71Iz86nn36qvXv3eiwr+IW4NNOwDx06VE6nU6+++qrH8pdeekmGYVTpL6tDhw5VUlKSxyVvubm5mjFjhkJCQtyXcR45csRjP5vN5r4pcUFP0cnbhISE6KyzzjptT5Ik3XrrrWrYsKHuv//+IpeIZWVl6frrr5dpmkXu9dW2bVt17NhRc+bM0Zw5cxQdHe3xBw+73a4RI0bo888/18aNG4uc99ChQ6etqyLl5ubqrbfecr/OycnRW2+9pQYNGqhr166SSt8WI0aMkGmamjJlSpHzlLUX0ul0FrnktGHDhoqJiSmx3QBULXqcAFQrvXr1Up06dTRmzBjdddddMgxDH3zwQZVeElWSyZMna9GiRerdu7duv/129y/gHTp00Lp16067b5s2bdSyZUvdd9992rt3r8LCwvT555+f0ViZYcOGqXfv3nrwwQe1Y8cOtWvXTl988UWZx/+EhIRo+PDh7nFOhS/Tk/J6Jb744gtddtlluuiii7R9+3a9+eabateundLT08t0roL7UT3zzDO6+OKLNXToUP32229asGCBRy9SwXmnTp2q66+/Xr169dKGDRv03//+16OnSsrrBYiIiNCbb76p0NBQBQcHq0ePHsWOmRk2bJj69eunRx55RDt27FB8fLwWLVqkL7/8UhMmTChyL6MztWTJEmVlZRVZPnz4cN1yyy166623NHbsWP36669q1qyZPvvsM61cuVLTp09394jddNNNOnr0qC644AI1btxYO3fu1IwZM9SpUyf3GJx27dqpb9++6tq1q+rWras1a9a4p7k+nXr16umzzz7TRRddpC5duuimm25Su3btlJSUpFmzZmnr1q16+eWXi53efeTIkZo0aZICAgJ04403Fhkf9Oyzz2rp0qXq0aOHbr75ZrVr105Hjx7V2rVrtXjxYh09erS8H6ukvF6jDz/8sMjyyMhIjynYY2Ji9Nxzz2nHjh1q3bq15syZo3Xr1mnmzJnu2xSUti369euna6+9Vq+88or++usvDR48WC6XSz/88IP69etX4uddWFpamho3bqwrrrhC8fHxCgkJ0eLFi7V69Wq9+OKLZ/TZAKhgVT2NHwDvc6rpyNu3b1/s9itXrjT/8Y9/mIGBgWZMTIz5wAMPmAsXLjQlmUuXLnVvd6rpyIub+lknTY99qunIi5sSuGnTph7TY5umaS5ZssTs3Lmz6efnZ7Zs2dJ8++23zXvvvdcMCAg4xadwwh9//GH279/fDAkJMevXr2/efPPN7imKC0+lPWbMGDM4OLjI/sXVfuTIEfPaa681w8LCzPDwcPPaa681f/vtt1JPR17gm2++MSWZ0dHRRaYAd7lc5tNPP202bdrU9Pf3Nzt37mx+/fXXRdrBNEuejtw0TdPpdJpTpkwxo6OjzcDAQLNv377mxo0bi3zeWVlZ5r333uvernfv3uaqVavM888/3zz//PM9zvvll1+a7dq1c08NX/Dei6sxLS3NvOeee8yYmBjT19fXbNWqlTlt2jSP6aQL3ktpvxcnK/hOnurxwQcfmKZpmgcOHDCvv/56s379+qafn5/ZsWPHIu322WefmQMHDjQbNmxo+vn5mU2aNDFvvfVWc//+/e5tnnzySfOcc84xIyIizMDAQLNNmzbmU0895Z5uuyTbt283b775ZrNJkyamr6+vWb9+ffOSSy4xf/jhh1Pu89dff7nfz4oVK4rd5sCBA+a4cePM2NhY09fX14yKijIvvPBCc+bMme5tCqYj//TTT0tVq2mefjrywt+Ngv/erFmzxuzZs6cZEBBgNm3a1Hz11VeLrbWktjDNvOn5p02bZrZp08b08/MzGzRoYA4ZMsT89ddfPeor6buTnZ1t3n///WZ8fLwZGhpqBgcHm/Hx8ebrr79e6s8BQNUwTLMa/RkXAGqw4cOHl2sqaACVq2/fvjp8+HCxlwsCQGkxxgkAyuH48eMer//66y/Nnz9fffv2taYgAABQqRjjBADl0KJFC40dO1YtWrTQzp079cYbb8jPz08PPPCA1aUBAIBKQHACgHIYPHiwPvroIyUlJcnf3189e/bU008/XeSGrgAAoHZgjBMAAAAAlIAxTgAAAABQAoITAAAAAJTA68Y4uVwu7du3T6GhoTIMw+pyAAAAAFjENE2lpaUpJiamyA28T+Z1wWnfvn2KjY21ugwAAAAA1cTu3bvVuHHj027jdcEpNDRUUt6HExYWZnE1ksPh0KJFizRw4ED5+vpaXQ6qGO3v3Wh/70b7ezfa37vR/tVHamqqYmNj3RnhdLwuOBVcnhcWFlZtglNQUJDCwsL4h+OFaH/vRvt7N9rfu9H+3o32r35KM4SHySEAAAAAoAQEJwAAAAAoAcEJAAAAAErgdWOcAAAAUP2Ypqnc3Fw5nU6rS6l0DodDPj4+ysrK8or3azVfX1/Z7fYzPg7BCQAAAJbKycnR/v37lZmZaXUpVcI0TUVFRWn37t3cV7QKGIahxo0bKyQk5IyOQ3ACAACAZVwul7Zv3y673a6YmBj5+fnV+jDhcrmUnp6ukJCQEm+6ijNjmqYOHTqkPXv2qFWrVmfU80RwAgAAgGVycnLkcrkUGxuroKAgq8upEi6XSzk5OQoICCA4VYEGDRpox44dcjgcZxScaCkAAABYjgCBylJRPZh8QwEAAACgBAQnAAAAACgBwQkAAACoBpo1a6bp06dbXQZOgeAEAAAAlIFhGKd9TJ48uVzHXb16tW655ZYzqq1v376aMGHCGR0DxWNWvWrANK2uAAAAAKW1f/9+989z5szRpEmTtGXLFveywvcLMk1TTqdTPj4l/9rdoEGDii0UFYoeJwv9e/k2DXp5hVYeqN33KgAAACgt0zSVmZNrycMs5V+zo6Ki3I/w8HAZhuF+vXnzZoWGhmrBggXq2rWr/P39tWLFCv3999+69NJLFRkZqbCwMF1wwQVavHixx3FPvlTPMAy9/fbbuuyyyxQUFKRWrVrpq6++OqPP9/PPP1f79u3l7++vZs2a6cUXX/RY//rrr6tVq1YKCAhQZGSkrrjiCve6zz77TB07dlRgYKDq1aun/v37KyMj44zqqUnocbJQWnauth3OVP1afpM3AACA0jrucKrdpIWWnPuPqYMU5Fcxvx4/+OCDeuGFF9SiRQvVqVNHu3fv1tChQ/XUU0/J19dXb7/9ti699FJt2bJFTZo0OeVxpkyZoueff17Tpk3TjBkzNHr0aO3cuVN169Ytc02//vqrrrrqKk2ePFkjR47Ujz/+qDvuuEP16tXT2LFjtWbNGt1111364IMP1KtXLx09elQ//PCDpLxetlGjRun555/XZZddprS0NP3www+lDpu1AcHJQvGNwyVJO9MJTgAAALXJ1KlTNWDAAPfrunXrKj4+XlLeDXAfeeQRLViwQF999ZXGjx9/yuOMHTtWo0aNkiQ9/fTTeuWVV/TLL79o8ODBZa7p//7v/3ThhRfqsccekyS1bt1af/zxh6ZNm6axY8dq165dCg4O1sUXX6zQ0FA1bdpUnTt3lpQXnHJzc3X55ZeradOmkqSOHTuWuYaajOBkobjGEZKkg8el9Oxc1fH1tbYgAAAAiwX62vXH1EGWnbuidOvWzeN1enq6Jk+erG+++cYdQo4fP65du3ad9jhxcXHun4ODgxUWFqaDBw+Wq6ZNmzbp0ksv9VjWu3dvTZ8+XU6nUwMGDFDTpk3VokULDR48WIMHD3ZfJhgfH68LL7xQHTt21KBBgzRw4EBdccUVqlOnTrlqqYkY42ShBqH+igkPkClDv+9LtbocAAAAyxmGoSA/H0seRgUOnwgODvZ4fd9992nu3Ll6+umn9f3332v58uXq2LGjcnJyTnsc35P+sG4YhlwuV4XVWVhoaKjWrl2rjz76SNHR0Zo0aZLi4+OVnJwsu92uhIQELViwQO3atdOMGTN09tlna/v27ZVSS3VEcLJYx0ZhkqT1e1MsrgQAAACVZeXKlRo7dqwuu+wydezYUQ0bNtSOHTuqtIa2bdtq5cqVRepq3bq17Pa83jYfHx/1799fzz//vNavX68dO3bou+++k5QX2nr37q0pU6bot99+k5+fn+bOnVul78FKXKpnsY6NwrXwj4Nav4ceJwAAgNqqVatW+uKLLzRs2DCZpqmHH3640nqODh06pHXr1nksi46O1r333qvu3bvriSee0MiRI7Vq1Sq9+uqrev311yVJX3/9tbZt26Y+ffqoTp06mj9/vlwul84++2z9/PPPWrJkiQYOHKiGDRvq559/1qFDh9S2bdtKeQ/VEcHJYgUTRGygxwkAAKDW+r//+z/dcMMN6tWrl+rXr68777xTx48fr5RzzZ49W7Nnz/ZY9sQTT+jRRx/VJ598okmTJumJJ55QdHS0pk6dqrFjx0qSIiIi9MUXX2jy5MnKyspSq1at9NFHH6l9+/batGmTli9frunTpys1NVVNmzbViy++qCFDhlTKe6iOCE4Wax8TJkOm9iZn6Uh6tuqF+FtdEgAAAEpp7Nix7uAhSX379i12iu5mzZq5L3lzuVxKTU3VvffeK5vtxMiZky/dK+44ycnJp61n2bJlp10/YsQIjRgxoth155577in3b9u2rb799tvTHru2Y4yTxUIDfNQwMO/n9XvodQIAAACqI4JTNdAkOO+vCet2J1tbCAAAAIBiEZyqgSYhecFp/Z5kawsBAAAAUCyCUzVwIjilFHstKwAAAABrWRqcnnnmGXXv3l2hoaFq2LChhg8fri1btpx2n1mzZskwDI9HQEBAFVVcORoFSz42Q0cycrTnWOXMrgIAAACg/CwNTt9//73GjRunn376SQkJCXI4HBo4cKAyMjJOu19YWJj279/vfuzcubOKKq4cvjbp7KgQSUwQAQAAAFRHlk5HfvKUhrNmzVLDhg3166+/qk+fPqfczzAMRUVFVXZ5VSquUbh+35em9XuSdVFctNXlAAAAACikWt3HKSUlr7elbt26p90uPT1dTZs2lcvlUpcuXfT000+rffv2xW6bnZ2t7Oxs9+vU1FRJksPhkMPhqKDKy6+ghnb5PU7rdh+rFnWhahS0NW3unWh/70b7ezfa/wSHwyHTNOVyueRyuawup0oUjGkveN+oXC6XS6ZpyuFwyG63e6wry79Bw6wmsxG4XC5dcsklSk5O1ooVK0653apVq/TXX38pLi5OKSkpeuGFF7R8+XL9/vvvaty4cZHtJ0+erClTphRZPnv2bAUFBVXoezgT+zKk59b7yN9m6tlznLIZVlcEAABQ+Xx8fBQVFaXY2Fj5+flZXQ5qoZycHO3evVtJSUnKzc31WJeZmal//vOfSklJUVhY2GmPU22C0+23364FCxZoxYoVxQagU3E4HGrbtq1GjRqlJ554osj64nqcYmNjdfjw4RI/nKrgcDiUkJCgfhdeqB7P/aDjDpfm39lLrRqGWF0aqkBB+w8YMEC+vr5Wl4MqRvt7N9rfu9H+J2RlZWn37t1q1qxZjZ/wq7RM01RaWpouvfRSderUSS+99JIkqUWLFrr77rt19913n3Jfu92uzz//XMOHDz+jGirqODVBVlaWduzYodjY2CLfsdTUVNWvX79UwalaXKo3fvx4ff3111q+fHmZQpMk+fr6qnPnztq6dWux6/39/eXv71/sftXpP1SB/v7q2ChCv+w4qj+SMtSuUR2rS0IVqm7fR1Qt2t+70f7ejfaXnE6nDMOQzWaTzVYz7pQzbNgwORyOIuP1JemHH35Qnz59lJiYqLi4uGL3L7g8r2CG6IL3vXr1agUHB5f4OZTls5o8ebLmzZundevWeSzfv3+/6tSpU6mf+axZszRhwgQlJydX2jlKw2azyTCMYv+9leXfn6XfTtM0NX78eM2dO1ffffedmjdvXuZjOJ1ObdiwQdHRNX9ChbjG4ZK4ES4AAEB1duONNyohIUF79uwpsu7dd99Vt27dThmaTqdBgwZVNpQkKiqq2M4FnJqlwWncuHH68MMPNXv2bIWGhiopKUlJSUk6fvzEvYyuu+46PfTQQ+7XU6dO1aJFi7Rt2zatXbtW11xzjXbu3KmbbrrJirdQoeJiIyRJibuTLa0DAADAMqYp5WRY8yjlCJaLL75YDRo00KxZszyWp6en69NPP9WNN96oI0eOaNSoUWrUqJGCgoLUsWNHffTRR6c9brNmzTR9+nT367/++kt9+vRRQECA2rVrp4SEhCL7/Otf/1Lr1q0VFBSkFi1a6LHHHnNPeDBr1ixNmTJFiYmJ7t6tgpoNw9C8efPcx9mwYYMuuOACBQYGql69errllluUnp7uXj927FgNHz5cL7zwgqKjo1WvXj2NGzfujCY42bVrly699FKFhIQoLCxMV111lQ4cOOBen5iYqH79+ik0NFRhYWHq2rWr1qxZI0nauXOnhg0bpjp16ig4OFjt27fX/Pnzy11LaVh6qd4bb7whSerbt6/H8nfffVdjx46VlPeBFu5CPHbsmG6++WYlJSWpTp066tq1q3788Ue1a9euqsquNPH5PU6b9qcpJ9clP5+a0V0NAABQYRyZ0tMx1pz74X2SX3CJm/n4+Oi6667TrFmz9Mgjj8gw8mb1+vTTT+V0OjVq1Cilp6era9eu+te//qWwsDB98803uvbaa9WyZUt169atxHO4XC5dfvnlioyM1M8//6yUlBRNmDChyHahoaGaNWuWYmJitGHDBt18880KDQ3VAw88oJEjR2rjxo369ttvtXjxYklSeHh4kWNkZGRo0KBB6tmzp1avXq2DBw/qpptu0vjx4z3C4dKlSxUdHa2lS5dq69atGjlypDp16qSbb765xPdT3PsrCE3ff/+9cnNzNW7cOI0cOVLLli2TJI0ePVqdO3fWG2+8IbvdrnXr1rkvrRs3bpxycnK0fPlyBQcH648//lBISOXOEWBpcCrNvBQFH1yBl156yT2ArrZpUjdIEUG+Ss50aHNSquIaR1hdEgAAAIpxww03aNq0afr+++/dnQDvvvuuRowYofDwcIWHh+u+++5zb3/nnXdq4cKF+uSTT0oVnBYvXqzNmzdr4cKFionJC5JPP/20hgwZ4rHdo48+6v65WbNmuu+++/Txxx/rgQceUGBgoEJCQtwzF57K7NmzlZWVpffff1/BwXnB8dVXX9WwYcP03HPPKTIyUpJUp04dvfrqq7Lb7WrTpo0uuugiLVmypFzBacmSJdqwYYO2b9+u2NhYSdL777+v9u3ba/Xq1erevbt27dql+++/X23atJEktWrVyr3/rl27NGLECHXs2FFS3sQala1aTA6BPIZhKK5xhJb/eUiJe1IITgAAwPv4BuX1/Fh17lJq06aNevXqpf/85z/q27evtm7dqh9++EFTp06VlDcO/+mnn9Ynn3yivXv3KicnR9nZ2aUew7Rp0ybFxsa6Q5Mk9ezZs8h2c+bM0SuvvKK///5b6enpys3NLfPM0Zs2bVJ8fLw7NElS79695XK5tGXLFndwat++vcd9kKKjo7Vhw4YynavwOWNjY92hSZLatWuniIgIbdq0Sd27d9fEiRN100036YMPPlD//v115ZVXqmXLlpKku+66S7fffrsWLVqk/v37a8SIEeUaV1YWXAtWzRRcrsc4JwAA4JUMI+9yOSseRtlupHnjjTfq888/V1pamt599121bNlS559/viRp2rRpevnll/Wvf/1LS5cu1bp16zRo0CDl5ORU2Ee1atUqjR49WkOHDtXXX3+t3377TY888kiFnqOwk2egMwyjUm/gO3nyZP3++++66KKL9N1336ldu3aaO3euJOmmm27Stm3bdO2112rDhg3q1q2bZsyYUWm1SASnaqegl4mZ9QAAAKq3q666SjabTbNnz9b777+vG264wT3eaeXKlbr00kt1zTXXKD4+Xi1atNCff/5Z6mO3bdtWu3fv1v79+93LfvrpJ49tfvzxRzVt2lSPPPKIunXrplatWmnnzp0e2/j5+cnpdJZ4rsTERGVkZLiXrVy5UjabTWeffXapay6Lgve3e/du97I//vhDycnJHnMXtG7dWvfcc48WLVqkyy+/XO+++657XWxsrG677TZ98cUXuvfee/Xvf/+7UmotQHCqZgp6nLYeTFdGdm4JWwMAAMAqISEhGjlypB566CHt37/fPbmZlDceJyEhQT/++KM2bdqkW2+91WPGuJL0799frVu31pgxY5SYmKgffvhBjzzyiMc2rVq10q5du/Txxx/r77//1iuvvOLukSnQrFkzbd++XevWrdPhw4eVnZ1d5FyjR49WQECAxowZo40bN2rp0qW68847de2117ov0ysvp9OpdevWeTw2bdqk/v37q2PHjho9erTWrl2rX375Rdddd53OP/98devWTcePH9f48eO1bNky7dy5UytXrtTq1avVtm1bSdKECRO0cOFCbd++XWvXrtXSpUvd6yoLwamaaRgWoOjwALlMaePeFKvLAQAAwGnceOONOnbsmAYNGuQxHunRRx9Vly5dNGjQIPXt21dRUVEaPnx4qY9rs9k0d+5cHT9+XOecc45uuukmPfXUUx7bXHLJJbrnnns0fvx4derUST/++KMee+wxj21GjBihwYMHq1+/fmrQoEGxU6IHBQVp4cKFOnr0qLp3764rrrhCF154oV599dWyfRjFSE9PV+fOnT0ew4YNk2EY+vLLL1WnTh316dNH/fv3V4sWLTRnzhxJkt1u15EjR3TdddepdevWuuqqqzRkyBBNmTJFUl4gGzdunNq2bavBgwerdevWev3118+43tMxzNJMbVeLpKamKjw8XCkpKWUeOFcZHA6H5s+fr6FDh7qvG731gzVa+PsBPTy0jW7p09LiClGZimt/eA/a37vR/t6N9j8hKytL27dvV/PmzRUQEGB1OVXC5XIpNTVVYWFhHrfdQeU43XesLNmAlqqG4gtuhLuHHicAAACgOiA4VUPxTBABAAAAVCsEp2qoQ6O8CSJ2Hz2uoxmVM50kAAAAgNIjOFVD4YG+alE/7wZkifQ6AQAAAJYjOFVTBeOc1u9mnBMAAKj9vGy+MlShivpuEZyqqbj8+zkxzgkAANRmBbMKZmZmWlwJaqucnLyhL3a7/YyO41MRxaDixeVPEJG4J1mmabrvQg0AAFCb2O12RURE6ODBg5Ly7ilU23/vcblcysnJUVZWFtORVzKXy6VDhw4pKChIPj5nFn0ITtVU+5gw+dgMHU7P0b6ULDWKCLS6JAAAgEoRFRUlSe7wVNuZpqnjx48rMDCw1ofE6sBms6lJkyZn/FkTnKqpAF+7zo4K1e/7UrV+dzLBCQAA1FqGYSg6OloNGzaUw+GwupxK53A4tHz5cvXp08frb4BcFfz8/CqkZ4/gVI3FNY7Q7/tSlbgnRUM6RltdDgAAQKWy2+1nPA6lJrDb7crNzVVAQADBqQbhospqLD5/gojE3cnWFgIAAAB4OYJTNVYwQcTGvSlyuZiiEwAAALAKwakaax0ZogBfm9Kyc7XtcIbV5QAAAABei+BUjfnYbeoQw/2cAAAAAKsRnKo59/2cGOcEAAAAWIbgVM3Fx+ZPELEnxeJKAAAAAO9FcKrm4vN7nP7Yn6qcXJe1xQAAAABeiuBUzTWtF6TwQF/l5Lr054E0q8sBAAAAvBLBqZozDENx+fdzWsc4JwAAAMASBKcaoCA4MbMeAAAAYA2CUw1QMM5pPRNEAAAAAJYgONUA8bERkqQ/D6QpMyfX2mIAAAAAL0RwqgEiwwIUGeYvlylt3JtqdTkAAACA1yE41RAnLtdLtrQOAAAAwBsRnGqIgsv1uBEuAAAAUPUITjUEM+sBAAAA1iE41RBxjSIkSTuPZOpYRo61xQAAAABehuBUQ4QH+ap5/WBJ0vq9XK4HAAAAVCWCUw3ivlxvd7K1hQAAAABehuBUg8Tlz6yXyDgnAAAAoEoRnGqQ+Pwep8Q9KTJN0+JqAAAAAO9BcKpB2seEy24zdCgtW0mpWVaXAwAAAHgNglMNEuhnV+vIUElS4m4miAAAAACqCsGphjlxuV6ytYUAAAAAXoTgVMMUTBDBjXABAACAqkNwqmHiY/OnJN+TIpeLCSIAAACAqkBwqmFaR4bK38emtKxc7TiSYXU5AAAAgFcgONUwvnab2seESWKcEwAAAFBVCE41kPtGuMysBwAAAFQJglMN1Ck2QhITRAAAAABVheBUA8XlT0n++75UOZwui6sBAAAAaj+CUw3UrF6wQgN8lJ3r0pakNKvLAQAAAGo9glMNZLMZ7l6n9XsY5wQAAABUNoJTDRXPjXABAACAKkNwqqHcM+vR4wQAAABUOoJTDRUfm3ep3p8H0nQ8x2lxNQAAAEDtRnCqoaLCAtQg1F9Ol6nf99HrBAAAAFQmglMNZRiGe5wTl+sBAAAAlYvgVIPFu2fWS7a2EAAAAKCWIzjVYHGxEZKkxN3JltYBAAAA1HaWBqdnnnlG3bt3V2hoqBo2bKjhw4dry5YtJe736aefqk2bNgoICFDHjh01f/78Kqi2+inocdpxJFMpmQ6LqwEAAABqL0uD0/fff69x48bpp59+UkJCghwOhwYOHKiMjIxT7vPjjz9q1KhRuvHGG/Xbb79p+PDhGj58uDZu3FiFlVcPEUF+alovSJK0fm+ytcUAAAAAtZilwenbb7/V2LFj1b59e8XHx2vWrFnatWuXfv3111Pu8/LLL2vw4MG6//771bZtWz3xxBPq0qWLXn311SqsvPpw38+Jy/UAAACASuNjdQGFpaTkzQ5Xt27dU26zatUqTZw40WPZoEGDNG/evGK3z87OVnZ2tvt1amqqJMnhcMjhsP7ytoIayltLh+gQ/S9RWrfrWLV4PyibM21/1Gy0v3ej/b0b7e/daP/qoyxtUG2Ck8vl0oQJE9S7d2916NDhlNslJSUpMjLSY1lkZKSSkpKK3f6ZZ57RlClTiixftGiRgoKCzqzoCpSQkFCu/TJSJclHq/8+6LVjvWqD8rY/agfa37vR/t6N9vdutL/1MjMzS71ttQlO48aN08aNG7VixYoKPe5DDz3k0UOVmpqq2NhYDRw4UGFhYRV6rvJwOBxKSEjQgAED5OvrW+b9M3Ny9eof3ynFYajLuRcoKiygEqpEZTnT9kfNRvt7N9rfu9H+3o32rz4KrkYrjWoRnMaPH6+vv/5ay5cvV+PGjU+7bVRUlA4cOOCx7MCBA4qKiip2e39/f/n7+xdZ7uvrW62+qOWtJ9zXV60jQ7U5KU1/JGUotl5oJVSHylbdvo+oWrS/d6P9vRvt791of+uV5fO3dHII0zQ1fvx4zZ07V999952aN29e4j49e/bUkiVLPJYlJCSoZ8+elVVmtRfHjXABAACASmVpcBo3bpw+/PBDzZ49W6GhoUpKSlJSUpKOHz/u3ua6667TQw895H59991369tvv9WLL76ozZs3a/LkyVqzZo3Gjx9vxVuoFuLzb4S7fk+KtYUAAAAAtZSlwemNN95QSkqK+vbtq+joaPdjzpw57m127dql/fv3u1/36tVLs2fP1syZMxUfH6/PPvtM8+bNO+2EErVdfKEpyU3TtLYYAAAAoBaydIxTaX7JX7ZsWZFlV155pa688spKqKhmOjsqVH4+NqVm5WrHkUw1rx9sdUkAAABArWJpjxMqhq/dpnbReTMEMs4JAAAAqHgEp1qiU/44p8TdjHMCAAAAKhrBqZZgZj0AAACg8hCcaom4/AkiNu5LUa7TZW0xAAAAQC1DcKolWtQPVqi/j7IcLv15IN3qcgAAAIBaheBUS9hshjpyuR4AAABQKQhOtUjB5XqJ3AgXAAAAqFAEp1okPr/HKXF3srWFAAAAALUMwakWicufknzLgTRlOZzWFgMAAADUIgSnWiQmPED1Q/zldJn6fV+q1eUAAAAAtQbBqRYxDIPL9QAAAIBKQHCqZQomiGBmPQAAAKDiEJxqmfjYginJmVkPAAAAqCgEp1qmoMdp2+EMpRx3WFsMAAAAUEsQnGqZusF+iq0bKEnaQK8TAAAAUCEITrXQiRvhJltaBwAAAFBbEJxqoU5MEAEAAABUKIJTLRTXmAkiAAAAgIpEcKqFOjQKl82Q9qdk6WBqltXlAAAAADUewakWCvb30VkNQyRJifQ6AQAAAGeM4FRLxTPOCQAAAKgwBKdaKi42QhI9TgAAAEBFIDjVUvHuCSKSZZqmxdUAAAAANRvBqZZqExUmP7tNyZkO7TqaaXU5AAAAQI1GcKql/HxsahsTJonL9QAAAIAzRXCqxdyX6+1OtrYQAAAAoIYjONVicfkz6yUysx4AAABwRghOtVhBj9PGvanKdbosrgYAAACouQhOtViLBiEK8ffRcYdTWw+lW10OAAAAUGMRnGoxu81Qh0Z5E0Ss380EEQAAAEB5EZxqufj8cU7rGOcEAAAAlBvBqZYrmCBiPcEJAAAAKDeCUy0XH5s3QcTm/WnKcjgtrgYAAAComQhOtVyjiEDVC/ZTrsvUH/tTrS4HAAAAqJEITrWcYRiK40a4AAAAwBkhOHmB+NgISdL6PcysBwAAAJQHwckLFMysl8gEEQAAAEC5EJy8QMGlen8fylBqlsPiagAAAICah+DkBeqF+KtRRKAkaSOX6wEAAABlRnDyEp3yxzklEpwAAACAMiM4eQn3zHqMcwIAAADKjODkJeIKJohgSnIAAACgzAhOXqJj43AZhrQvJUuH0rKtLgcAAACoUQhOXiLE30dnNQiRxOV6AAAAQFkRnLyI+3I9JogAAAAAyoTg5EXiY/MmiGCcEwAAAFA2BCcvUtDjtH5PskzTtLYYAAAAoAYhOHmRttGh8rUbOpbp0J5jx60uBwAAAKgxCE5exN/HrrbRYZKkRCaIAAAAAEqN4ORlCm6EyzgnAAAAoPQITl6GmfUAAACAsiM4eZlOsRGSpI17U+R0MUEEAAAAUBoEJy/TskGIgvzsysxx6u9D6VaXAwAAANQIBCcvY7cZ6tAob5zTOsY5AQAAAKVCcPJC8fkTRKxnZj0AAACgVAhOXig+f5zTeiaIAAAAAErF0uC0fPlyDRs2TDExMTIMQ/PmzTvt9suWLZNhGEUeSUlJVVNwLRGfP7Pepv2pys51WlsMAAAAUANYGpwyMjIUHx+v1157rUz7bdmyRfv373c/GjZsWEkV1k6N6wSqTpCvHE5Tm/anWV0OAAAAUO35WHnyIUOGaMiQIWXer2HDhoqIiKj4gryEYRiKj43Qsi2HtH5PsnuKcgAAAADFszQ4lVenTp2UnZ2tDh06aPLkyerdu/cpt83OzlZ2drb7dWpqqiTJ4XDI4XBUeq0lKaihqmvpEB2qZVsO6bddxzSqW6MqPTdOsKr9UT3Q/t6N9vdutL93o/2rj7K0gWGaZrW4C6phGJo7d66GDx9+ym22bNmiZcuWqVu3bsrOztbbb7+tDz74QD///LO6dOlS7D6TJ0/WlClTiiyfPXu2goKCKqr8GmfjMUP/3mxXZKCphzsxzgkAAADeJzMzU//85z+VkpKisLCw025bo4JTcc4//3w1adJEH3zwQbHri+txio2N1eHDh0v8cKqCw+FQQkKCBgwYIF9f3yo77+H0bPV87nsZhrT2kQsU4l8jOx9rPKvaH9UD7e/daH/vRvt7N9q/+khNTVX9+vVLFZxq/G/L55xzjlasWHHK9f7+/vL39y+y3NfXt1p9Uau6nug6vmoUEai9yce1+UCmerasV2XnRlHV7fuIqkX7ezfa37vR/t6N9rdeWT7/Gn8fp3Xr1ik6OtrqMmqkOG6ECwAAAJSKpT1O6enp2rp1q/v19u3btW7dOtWtW1dNmjTRQw89pL179+r999+XJE2fPl3NmzdX+/btlZWVpbffflvfffedFi1aZNVbqNHiGkdowcYkJRKcAAAAgNOyNDitWbNG/fr1c7+eOHGiJGnMmDGaNWuW9u/fr127drnX5+Tk6N5779XevXsVFBSkuLg4LV682OMYKL34/B6nxN0pFlcCAAAAVG+WBqe+ffvqdHNTzJo1y+P1Aw88oAceeKCSq/IeHRqHyzCkvcnHdSQ9W/VCio4FAwAAAFALxjih/MICfNWifrAkaf0eep0AAACAUyE4ebn4xhGSpHW7ky2tAwAAAKjOCE5ejpn1AAAAgJIRnLxcfGyEpLxL9arJvZABAACAaofg5OXaRofJx2boSEaO9iYft7ocAAAAoFoiOHm5AF+72kSHSmJacgAAAOBUCE5QXP4EEYxzAgAAAIpXruC0e/du7dmzx/36l19+0YQJEzRz5swKKwxVp1N+cEokOAEAAADFKldw+uc//6mlS5dKkpKSkjRgwAD98ssveuSRRzR16tQKLRCVLy42b2a9jXtT5XQxQQQAAABwsnIFp40bN+qcc86RJH3yySfq0KGDfvzxR/33v//VrFmzKrI+VIGzGoQo0Neu9OxcbTuUbnU5AAAAQLVTruDkcDjk7+8vSVq8eLEuueQSSVKbNm20f//+iqsOVcLHblOHRmGSpMQ9TBABAAAAnKxcwal9+/Z688039cMPPyghIUGDBw+WJO3bt0/16tWr0AJRNeKZIAIAAAA4pXIFp+eee05vvfWW+vbtq1GjRik+Pl6S9NVXX7kv4UPNEpd/I9zE3cmW1gEAAABURz7l2alv3746fPiwUlNTVadOHffyW265RUFBQRVWHKpOfOO8CSI27U9TTq5Lfj7MVA8AAAAUKNdvx8ePH1d2drY7NO3cuVPTp0/Xli1b1LBhwwotEFWjSd0gRQT5Ksfp0uakVKvLAQAAAKqVcgWnSy+9VO+//74kKTk5WT169NCLL76o4cOH64033qjQAlE1DMNw3wiXCSIAAAAAT+UKTmvXrtV5550nSfrss88UGRmpnTt36v3339crr7xSoQWi6hRcrsc4JwAAAMBTuYJTZmamQkNDJUmLFi3S5ZdfLpvNpn/84x/auXNnhRaIqhPHzHoAAABAscoVnM466yzNmzdPu3fv1sKFCzVw4EBJ0sGDBxUWFlahBaLqFPQ4bT2YrozsXIurAQAAAKqPcgWnSZMm6b777lOzZs10zjnnqGfPnpLyep86d+5coQWi6jQMC1B0eIBcprRxL+OcAAAAgALlmo78iiuu0Lnnnqv9+/e77+EkSRdeeKEuu+yyCisOVS+ucbj2p2QpcU+yerTgZsYAAACAVM7gJElRUVGKiorSnj17JEmNGzfm5re1QFzjCC38/QAz6wEAAACFlOtSPZfLpalTpyo8PFxNmzZV06ZNFRERoSeeeEIul6uia0QV6hQbIYkJIgAAAIDCytXj9Mgjj+idd97Rs88+q969e0uSVqxYocmTJysrK0tPPfVUhRaJqtOhUd4EEbuPHtfRjBzVDfazuCIAAADAeuUKTu+9957efvttXXLJJe5lcXFxatSoke644w6CUw0WHuirFvWDte1whhL3JKvf2Q2tLgkAAACwXLku1Tt69KjatGlTZHmbNm109OjRMy4K1orLn5Z8/W7GOQEAAABSOYNTfHy8Xn311SLLX331VcXFxZ1xUbBWPOOcAAAAAA/lulTv+eef10UXXaTFixe77+G0atUq7d69W/Pnz6/QAlH14hpHSJIS96TINE0ZhmFtQQAAAIDFytXjdP755+vPP//UZZddpuTkZCUnJ+vyyy/X77//rg8++KCia0QVax8TJh+bocPp2dqXkmV1OQAAAIDlyn0fp5iYmCKTQCQmJuqdd97RzJkzz7gwWCfA167WkaH6Y3+q1u9OVqOIQKtLAgAAACxVrh4n1H4F45y4ES4AAABAcMIpxBfMrMcEEQAAAADBCcUrmCBiw54UuVymtcUAAAAAFivTGKfLL7/8tOuTk5PPpBZUI60jQxTga1Nadq62Hc7QWQ1DrC4JAAAAsEyZglN4eHiJ66+77rozKgjVg4/dpg4x4Vqz85jW70kmOAEAAMCrlSk4vfvuu5VVB6qhuMYRWrPzmBJ3J+vyLo2tLgcAAACwDGOccErxsXk9jMysBwAAAG9HcMIpFUwQ8cf+VOXkuqwtBgAAALAQwQmn1KxekMICfJST69KfB9KsLgcAAACwDMEJp2QYhvtGuOt2J1taCwAAAGAlghNOK44b4QIAAAAEJ5xefP44p/VMEAEAAAAvRnDCaRVcqvfngTRl5uRaWwwAAABgEYITTisyLECRYf5ymdLGvalWlwMAAABYguCEEsW5L9dLtrQOAAAAwCoEJ5SoU/7letwIFwAAAN6K4IQSMbMeAAAAvB3BCSWKaxQhSdp5JFPHMnKsLQYAAACwAMEJJQoP8lWzekGSpPV7uVwPAAAA3ofghFIpmJZ8/e5kS+sAAAAArEBwQqkUzKzHBBEAAADwRgQnlEp8/gQRiXuSZZqmxdUAAAAAVYvghFJpHxMuu83QobRsJaVmWV0OAAAAUKUITiiVQD+7WkeGSpISd3O5HgAAALwLwQmlFs/9nAAAAOClCE4otRMTRCRbWgcAAABQ1SwNTsuXL9ewYcMUExMjwzA0b968EvdZtmyZunTpIn9/f5111lmaNWtWpdeJPHHuHqcUuVxMEAEAAADvYWlwysjIUHx8vF577bVSbb99+3ZddNFF6tevn9atW6cJEybopptu0sKFCyu5UkjS2VGh8vexKS0rVzuOZFhdDgAAAFBlfKw8+ZAhQzRkyJBSb//mm2+qefPmevHFFyVJbdu21YoVK/TSSy9p0KBBlVUm8vnabWofE6a1u5KVuCdZLRqEWF0SAAAAUCUsDU5ltWrVKvXv399j2aBBgzRhwoRT7pOdna3s7Gz369TUVEmSw+GQw+GolDrLoqCG6lBLaXTID06/7TymiztEWl1OjVfT2h8Vi/b3brS/d6P9vRvtX32UpQ1qVHBKSkpSZKTnL+uRkZFKTU3V8ePHFRgYWGSfZ555RlOmTCmyfNGiRQoKCqq0WssqISHB6hJKxXXYkGTX8o07Nd/YZnU5tUZNaX9UDtrfu9H+3o329260v/UyMzNLvW2NCk7l8dBDD2nixInu16mpqYqNjdXAgQMVFhZmYWV5HA6HEhISNGDAAPn6+lpdTonaHs7Qhy+v1P4suwYMGiBfOxMznoma1v6oWLS/d6P9vRvt791o/+qj4Gq00qhRwSkqKkoHDhzwWHbgwAGFhYUV29skSf7+/vL39y+y3NfXt1p9UatbPadyVmS4QgN8lJaVq21HstShUbjVJdUKNaX9UTlof+9G+3s32t+70f7WK8vnX6O6C3r27KklS5Z4LEtISFDPnj0tqsj72GyGx7TkAAAAgDewNDilp6dr3bp1WrdunaS86cbXrVunXbt2Scq7zO66665zb3/bbbdp27ZteuCBB7R582a9/vrr+uSTT3TPPfdYUb7Xis+/Ee56boQLAAAAL2FpcFqzZo06d+6szp07S5ImTpyozp07a9KkSZKk/fv3u0OUJDVv3lzffPONEhISFB8frxdffFFvv/02U5FXsbj84JRIjxMAAAC8hKVjnPr27SvTNE+5ftasWcXu89tvv1ViVShJfGzepXp/HkjT8RynAv3sFlcEAAAAVK4aNcYJ1UNUWIAahPrL6TL1+z56nQAAAFD7EZxQZoZhuMc5cbkeAAAAvAHBCeUS755ZL9naQgAAAIAqQHBCucTFRkiSEncnW1oHAAAAUBUITiiXuPwb3+44kqmUTIfF1QAAAACVi+CEcqkT7Kem9YIkSev3JltbDAAAAFDJCE4otzj3jXCZIAIAAAC1G8EJ5VYwQcQ6xjkBAACgliM4odxO9DglW1oHAAAAUNkITii3Do3CZDOkA6nZSkrJsrocAAAAoNIQnFBuQX4+ah0ZKklKpNcJAAAAtRjBCWckjhvhAgAAwAsQnHBGmFkPAAAA3oDghDPSKTZCkpS4O1mmaVpbDAAAAFBJCE44I2dHhcrPx6bUrFztOJJpdTkAAABApSA44Yz42m1qFx0miXFOAAAAqL0ITjhjBTfCTdzNOCcAAADUTgQnnLH4/HFO9DgBAACgtiI44YwVzKy3cV+Kcp0ua4sBAAAAKgHBCWesRf1ghfr7KMvh0p8H0q0uBwAAAKhwBCecMZvNUIdG3AgXAAAAtRfBCRWiYJxTIjfCBQAAQC1EcEKFODGzXrK1hQAAAACVgOCEChGX3+O05UCashxOa4sBAAAAKhjBCRUiJjxA9UP85XSZ+n1fqtXlAAAAABWK4IQKYRiG+3I9JogAAABAbUNwQoUpuJ8T45wAAABQ2xCcUGHiYgt6nJhZDwAAALULwQkVJj6/x2nb4QylHHdYWwwAAABQgQhOqDB1g/0UWzdQkrRxL71OAAAAqD0ITqhQBeOc1jHOCQAAALUIwQkVipn1AAAAUBsRnFChCsY5MUEEAAAAahOCEypUh0bhshnS/pQsHUzNsrocAAAAoEIQnFChgv19dFbDEElSIr1OAAAAqCUITqhwce7L9ZItrQMAAACoKAQnVLj42AhJ9DgBAACg9iA4ocIVnlnPNE2LqwEAAADOHMEJFa5NVJj87DYlZzq062im1eUAAAAAZ4zghArn52NT2+hQSVyuBwAAgNqB4IRKUTDOaf3uZEvrAAAAACoCwclqtXQMUMHMeonMrAcAAIBagOBkpSN/yz5rkIKz9ltdSYUrmCBi495U5TpdFlcDAAAAnBmCk5Xm3y/bvrU6969npMN/WV1NhWrRIETBfnYddzi19VC61eUAAAAAZ4TgZKXL3pLZsJ0CcpPl8+Gl0sHNVldUYew2Qx0LpiXfzQQRAAAAqNkITlYKaaDc0XOVEthERsZBadZF0oE/rK6qwsTnj3NaxzgnAAAA1HAEJ6sF1dPKsx6UGRUnZR6W3rtYStpgdVUVomCCiPUEJwAAANRwBKdqwOETotx/fiHFdJYyj0jvDZP2J1pd1hmLj827VG/z/jRlOZwWVwMAAACUH8GpugiMkK6dJzXqJh0/lhee9q61uqoz0igiUPWC/ZTrMrVpf6rV5QAAAADlRnCqTgIjpGvnSrE9pKwU6f3h0p5fra6q3AzDUFz+BBGJ3AgXAAAANRjBqboJCJOu+Vxq0lPKTpE+GC7t/sXqqsrtxDgnZtYDAABAzUVwqo78Q6XRn0lNz5WyU6UPLpN2rrK6qnLpFBshSUpkgggAAADUYASn6so/RBr9idS8j5STLn04Qtqx0uqqyqzgUr1thzOUmuWwuBoAAACgfAhO1ZlfsDRqjtSin+TIkP57hbR9udVVlUm9EH81igiUaUobuVwPAAAANRTBqbrzC5JGfSSd1V9yZEr/vUr6e6nVVZVJwbTkiQQnAAAA1FAEp5rAN1Aa+V+p1SAp97j00dXS1sVWV1Vq8dwIFwAAADVctQhOr732mpo1a6aAgAD16NFDv/xy6lnkZs2aJcMwPB4BAQFVWK1FfAOkkR9IZ18k5WZJH42S/lxkdVWlUjCzHlOSAwAAoKayPDjNmTNHEydO1OOPP661a9cqPj5egwYN0sGDB0+5T1hYmPbv3+9+7Ny5swortpCPv3TlLKntMMmZI338T2nLAqurKlHHxuEyDGlfSpYOpWVbXQ4AAABQZpYHp//7v//TzTffrOuvv17t2rXTm2++qaCgIP3nP/855T6GYSgqKsr9iIyMrMKKLebjJ13xrtRuuORySHOulTb9z+qqTivE30ctG4RI4nI9AAAA1Ew+Vp48JydHv/76qx566CH3MpvNpv79+2vVqlPftyg9PV1NmzaVy+VSly5d9PTTT6t9+/bFbpudna3s7BO9HKmpqZIkh8Mhh8P66bELaihzLZe+KbsM2f6YK/PTsXIOnymz7SWVUGHF6NgoTFsPpuu3nUfV56y6VpdTbZS7/VEr0P7ejfb3brS/d6P9q4+ytIFhmqZZibWc1r59+9SoUSP9+OOP6tmzp3v5Aw88oO+//14///xzkX1WrVqlv/76S3FxcUpJSdELL7yg5cuX6/fff1fjxo2LbD958mRNmTKlyPLZs2crKCioYt9QFTNMpzrv/Ldij/0ol2z6tdlt2lfnH1aXVawfkgx9tt2uthEu3dbWZXU5AAAAgDIzM/XPf/5TKSkpCgsLO+22NS44nczhcKht27YaNWqUnnjiiSLri+txio2N1eHDh0v8cKqCw+FQQkKCBgwYIF9f37IfwOWU/Zu7ZVv/sUzDJuclr8vscEXFF3qGEvek6Iq3fladIF/9/GBfGYZhdUnVwhm3P2o02t+70f7ejfb3brR/9ZGamqr69euXKjhZeqle/fr1ZbfbdeDAAY/lBw4cUFRUVKmO4evrq86dO2vr1q3Frvf395e/v3+x+1WnL2r56/GVhr8h2X1l/PaBfL66QzIkdRpV0SWekY6xdeRrN3Qs06ED6bmKrVuze/sqWnX7PqJq0f7ejfb3brS/d6P9rVeWz9/SySH8/PzUtWtXLVmyxL3M5XJpyZIlHj1Qp+N0OrVhwwZFR0dXVpnVn80mDXtF6jpWMl3SvNul3z60uioP/j52tY3OS/GJTBABAACAGsbyWfUmTpyof//733rvvfe0adMm3X777crIyND1118vSbruuus8Jo+YOnWqFi1apG3btmnt2rW65pprtHPnTt10001WvYXqwWaTLnpJ6n6TJFP6cpz06yyrq/IQ1zhcEvdzAgAAQM1j6aV6kjRy5EgdOnRIkyZNUlJSkjp16qRvv/3WPcX4rl27ZLOdyHfHjh3TzTffrKSkJNWpU0ddu3bVjz/+qHbt2ln1FqoPm00a+oJk85F+flP6392Syyl1v9HqyiQV3Ah3lxL3pFhdCgAAAFAmlgcnSRo/frzGjx9f7Lply5Z5vH7ppZf00ksvVUFVNZRhSIOflQy79NNr0jcT88JTj1usrkzxjSMkSRv3psjpMmW3MUEEAAAAagbLL9VDJTAMadBTUq+78l4vuF9a9bq1NUk6q2GIgvzsysxx6u9D6VaXAwAAAJQawam2MgxpwFTp3Il5rxc+JP04w9KS7DZDHRrljXNaxzgnAAAA1CAEp9rMMKQLJ0l9Hsh7vehRaYW1lznG508QsZ6Z9QAAAFCDEJxqO8OQLnhE6vtw3uvFk6Xl0ywrJz42QpK0ngkiAAAAUIMQnLxF339JFzya9/N3T0rLnrWkjIIJIjbtT1V2rtOSGgAAAICyIjh5kz73S/0n5/287Jm8AGWaVVpC4zqBqhPkK4fT1Kb9aVV6bgAAAKC8CE7e5tx7pIFP5f28fJq0ZEqVhifDMPLv58Q4JwAAANQcBCdv1Gu8NPi5vJ9XvCQlPFal4algnFPibsY5AQAAoGYgOHmrf9wmDX0h7+cfZ0gLH66y8MTMegAAAKhpCE7e7JybpYvzpyf/6XVpwQNVEp4KLtXbeihd6dm5lX4+AAAA4EwRnLxdtxukS2ZIMqRfZkrf3Cu5XJV6ygah/ooJD5BpShuYlhwAAAA1AMEJUpfrpEtfk2RIa96Rvp5Q6eHpxP2ckiv1PAAAAEBFIDghT+fR0mVvSYZNWvue9NWdkqvy7rNUcLleIsEJAAAANQDBCSfEj5Qu/3deeFr3ofTluEoLTwUTRDCzHgAAAGoCghM8dbxCGvGOZNilxI+kubdKzoqfwKFDfnDam3xcR9KzK/z4AAAAQEUiOKGoDpdLV74r2XykDZ9KX9xc4eEpLMBXLRsES5LWM0EEAAAAqjmCE4rX7lLpqvclm6/0+xfS5zdITkeFniI+f5zT2yu2aUtSWoUeGwAAAKhIBCecWpuLpJEfSnY/6Y8vpU/HSrk5FXb4YZ1iJEkrtx7RoOnLdf27v+jnbUdkVtGNeAEAAIDSIjjh9M4eLI38r2T3lzZ/LX1ynZRbMWOS+p3dUF+O662hHaNkGNLSLYc0cuZPuvyNH/XtxiS5XAQoAAAAVA8EJ5Ss9UBp1GzJJ0D6c4E05xrJkVUhh46PjdDro7vqu3v76p89msjPx6bfdiXrtg9/Vf+Xvtec1buUnVt506IDAAAApUFwQumc1V8a9bHkEyj9tUiaM1pyHK+wwzevH6ynL+uoFf/qpzv6tlRogI+2HcrQvz7foPOeW6o3v/9bqVkVO8YKAAAAKC2CE0qvZT9p9CeSb5C0dbH00SgpJ7NCT9EwNEAPDG6jVQ9dqEeGtlVUWIAOpmXr2QWb1fuZ7/TMgk06mFoxvV0AAABAaRGcUDbN+0ijP5N8g6VtS6XZV0k5GRV+mhB/H93cp4WWP9BP066I01kNQ5SWnau3vt+mc59bqgc/X6+/D6VX+HkBAACA4hCcUHbNekvXfiH5hUo7fpD+e6WUXTkhxs/Hpiu7xWrRhD56+7pu6ta0jnKcLn28erf6/9/3uvWDNVq761ilnBsAAAAoQHBC+TT5h3TtXMk/TNq5UvpwhJRdefdistkM9W8Xqc9u76XPbuup/m0jZZrSwt8P6PLXf9RVb63S0s0HmcocAAAAlYLghPKL7S5dN08KCJd2/yR9cLmUlVLpp+3WrK7eHtNNCff00ZVdG8vXbuiX7Ud1/azVGjz9B32xdo8cTlel1wEAAADvQXDCmWnUVbruSykgQtrzi/TBZdLx5Co5davIUE27Ml7LH+inm89rrmA/u7YcSNPETxJ1/vNL9c6K7crIzq2SWgAAAFC7EZxw5mI6S2P+JwXWlfb+Kr1/qZR5tMpOHx0eqEcuaqcfH7pQ9w86W/VD/LUvJUtPfP2Hej37nf5v0RYdTq+Ym/YCAADAOxGcUDGi4/LCU1A9af+6Kg9PkhQe6Ktx/c7Sin/109OXdVSzekFKOe7QK99tVe9nv9Nj8zZq15GKnT4dAAAA3oHghIoT1UEa87UU3EBKWi+9N0zKOFzlZQT42vXPHk205N6+emN0F8U3Dld2rksf/LRTfV9YqvGz12rj3sofiwUAAIDag+CEihXZThr7jRQSKR3YmBee0g9ZUordZmhIx2jNG9dbs2/uoT6tG8hlSl+v36+LZ6zQNW//rBV/HWYmPgAAAJSI4ISK1+Ds/PAUJR38Q3rvYintgGXlGIahXi3r6/0bztH8u87TpZ1iZLcZWrH1sK5552cNe3WF/pe4T7nMxAcAAIBTIDihctRvJV0/XwqNkQ5tlmZdJKXut7oqtYsJ08tXd9ay+/pqbK9mCvC1aePeVN350W+64MXv9cFPO5XlcFpdJgAAAKoZghMqT72W0vXfSGGNpSN/5YenfVZXJUmKrRukyZe0148PXqgJ/VupTpCvdh3N1GPzNqr3s99pxpK/lJyZY3WZAAAAqCYITqhcdVvkhafwJtLRv6V3h0ope6yuyq1usJ8m9G+tlQ9eoCmXtFfjOoE6kpGjFxP+VK9nv9PU//2hfcnHrS4TAAAAFiM4ofLVaZYXniKaSse254Wn5F1WV+UhyM9HY3o107L7+urlqzupbXSYMnOc+s/K7erz/FJN/GSdtiSlWV0mAAAALEJwQtWIaJI35qlOcyl5p/TuRdKxHVZXVYSP3aZLOzXS/LvO1Xs3nKOeLeop12Xqi7V7NWj6ct0wa7V+3naEmfgAAAC8DMEJVSe8cV54qttSStmVF56ObrO6qmIZhqHzWzfQR7f8Q1+O662hHaNkGNJ3mw9q5MyfdPkbP2rh70lyuQhQAAAA3oDghKoVFpMXnuq3llL35IWnn96Q/lwoHd4q5Va/CRniYyP0+uiuWnpvX/2zRxP5+dj0265k3frBr+r/0veas3qXsnOZiQ8AAKA287G6AHih0Ki8+zy9NyxvqvJvHzyxzrBJ4bF5k0rUa5n3XPCIaCr5BlhWdrP6wXr6so6a0L+V3vtxhz5YtVPbDmXoX59v0IuL/tQN5zbXP3s0UViAr2U1AgAAoHIQnGCNkIbS2PnSL29JBzdJR7fnXbbnyMgbA5W8U9q29KSdjPxQ1dwzUNVrmTcBhW9glZTeMDRA9w9qo9v7nqWPf9mlt3/YrqTULD27YLNe+26rRv+jqW7o3UwNw6wLeQAAAKhYBCdYJ7ie1O/hE69NU0o/kBegCh5H/j7xc0563tiolF3S9u+LHi+skWegcj+aS37BFV5+iL+Pbjqvha7r2Uxfrturmcu36a+D6Xrz+7/1nxXbdXmXRrq5Twu1bBBS4ecGAABA1SI4ofowjLzL+EKjpKa9PNeZppRxOO9eUMUFq+xUKXVv3mPHD0WPHRp9IkTVbZE3QUXBa//QMyrbz8emK7vFakSXxvpu80G9+f3fWrPzmD5evVtz1uzWwHaRuu38lurcpM4ZnQcAAADWITihZjAMKaRB3qPJPzzXmaaUebRQoDopXB0/JqXtz3vsXFn02MENC42pOukywIDwUpdosxnq3y5S/dtFas2Oo3rz+21avOmAFv6e9zineV3dfn5L9T27gQzDOMMPBAAAAFWJ4ISazzDyLvsLrifFdi+6PvPoiTFUJ4erzCNSxsG8x+6fiu4bVL/oeKqCcBV46h6kbs3q6u1mdfXXgTTNXL5N89bt1S/bj+qX7Ud1dmSobj2/hYbFx1TghwAAAIDKRHBC7RdUN+/RuGvRdceTpWP5oerINs9wlXFQyjyc99jzS9F9A+sUuuTvpHAVWEcyDLWKDNW0K+M1cWBrvbtyh/77005tOZCmiZ8k6sVFf2pMzyZyZEhHMnIUGe5DTxQAAEA1RXCCdwuMkAI7SzGdi67LSj0Rqk4OVulJeZcA7l2T9zhZQHihMNVS0XVb6OEOLTT+nDh9sD5D7/64U3uTj+vpBVsk+Wja+mXy87EpKixAUeEBig7Pe44KK/g5UNHhAaof4i+7jXAFAABQ1QhOwKkEhEnR8XmPk2WnS8d2nDSeanveZBVp+6SsFGnfb3mPQsIkjfMP0+31m2t3/SitTgnXjgwfHXUGKsPlr8zkAKUnB2qH6a/fFaAMM1AZ8lemApQrH9lthiJD/RVZEK7CAk+ErPygFRkWID8f7m0NAABQkQhOQHn4h0hRHfIeJ8vJzA9VJ09UsV1K2SNlp8qWlKimSlRTSbLlP0qQbfoqQ/7KOB6ojOMBykzyV7oZqEwFKEP+2mQG6lcFKMP0l/xD5RcYKr/gMAUFhyskLFzh4XUUUaeO6tepqwb16ikwOEyyEbAAAABKg+AEVDS/ICmyXd7jZI6sQqFqm5xHtmnfts1q1CBcttxMKScjrzcrJ0PKSct7duZIkvwNh/zlUF0jveQaXJIy8h+nkSV/ZduClOsTJKdvsAy/ENkDQuQbFCb/oDD5BobI8A/Nuw+WX0j+IzgvOBb8XHi5b2DeZB0AAAC1DMEJqEq+AVLDNnkPSS6HQ2vnz1fU0KGy+foWv09uTt7Nf3My8h/pJ15np3u8NrPTlZ2ZqqyMVDmOp8mZlSZlp8twZMjXmSk/53EFmpmyG6YkKUDZCnBlSznHpByVGLRKZNhOClT5z/7FhKyCZTZfyeaT/7DnP3xOPAyb5+uTtzFO3sfu+WwUXkcPGwAAKB+CE1Dd+fhJPvkzA5bAkBSQ/zgV0+VSaka6Dh05qsNHj+jYsaNKTUlRetoxZaalKiszVY7MVNkcmQo2jitYWQpS9omfjWyF6LiClKVgI1vBOq5gI7vg4Hk3I85OrYh3XgmMoiHLOCmo2WwVENxOXl9MoLPZZTMNnXXgT9lWbc2vxfCsVSp+mcfy4paVZX+dYlkVnb/g8zNsJ4XdguX2k5bbPD/TkrY9uY0MG72iAIByITgBXsaw2RQWGqaw0DC1bNbslNsdz3EqKTVL+1OOKyklSztTspSUkqX9KVk6kJr3fDg9LzAZcilQOfkhKkvBysoPWVkKUZZCbNmK8s9VwwCH6vk5VMfHoQh7tkJs2fIznPKRS3a55COn7HLJlv9suHIll1Ny5eY9zIKfT/FcsN50neJdmSeOVQ3YJbWXpH0WF+JtjNKEr1IEsSLb+pQpBNoktd+7Q7bFP1X/3lDTrIyD5h+3PM/F7G+6ynAMleFcrjPY99TPPqZLfVPT5LPnmbww7/6MC59Dnuctbl2J26oM2xb3uhTnLLLtac7p8bqQ0/4xxiiyqugfZ0r7h53T/FGn3Mcqe10+MnVhRoZ8dk4++QBnqDL+raqS/hsg6abFUnD9yjl2JSA4AShWoJ9dzesHq3n94FNuk5Pr0oHUrPyAlaWklONKSslWUupx7U/J0pb8kOXKlZSrMl8K6OdjU5CfXUG+dgX5++T9HGBXkJ+PAv3sCvbL+znIz64gP7sC/XwU7GdXoK+hEF9DQb6GgnxMBftKgXZTgT5SkI/kb3PJMJ35gatQMHM5C4WzgofL87VZzD7FBrzTbJN/DleuQ3v27Fbjxo1lK/x/4if/kuGxrNDy4pZV6P46xbIKPL/pOhF2PT5/54l17uXOk34u+CxdJ+3nLFq/x3tx5R/bcfrtKpld0lmSdNDSMmARQ1K4JGVZXAgsYUgKkaRsiwuxmquE/15XMwQnAOXm52NTbN0gxdYNOuU2uU6XDqfnKCk1L1jtL9RzlZSapYzsXGXmOJWZk6vMbKcyHU45XXm/WOfkupST61KyKvYXXJshd/gKOil85T2CCv3sU/TngLznQF+7gvMDXWB+wPOxl77nwOlw6Lf58xV9ujFuKJ+CwOsRsooLZ6cKZOXYtvD5ShH6nE6Htv29TS1atpC9wnucKuFyxEq5xNHIP25pnk+zvWErxTFUhnMVPNsq4BjF1Wso1+nSz7/8oh7n9JCPj89Jn/Epei0Kvy7LtoWePNeXdt/iXpd1XxW/XtJpe6JK+0ebMi8r7R97SlpW5IdS7Zub69CqVavUs2evE+1fYSrpcuTK+G9AYJ2KP2YlIjgBqFQ+dpv7PlOKjShxe9M0leN0uUPU8ZxcZWQ7T4SrHKeO5ziVkZNbzLJC2xfa97gjb7ssR94lfC5TSs/OVXp2xV+y5+djc/eEFfSK5T0XDWr+dkPb9hk68ONO+fvm3afLx2bkPdsN2W02+Z702r0+/9nXbvPcz2aT3V5o/UmvfWyGDG8Y42OzSTY/q6s4LZfDoT+y5qvZBUNlJzh7HdPh0OFNGTKb95Fof69jOhw6uuGozNgetH8NUi2C02uvvaZp06YpKSlJ8fHxmjFjhs4555xTbv/pp5/qscce044dO9SqVSs999xzGjp0aBVWDKCyGIYhfx+7/H3squi/QzldZl6IcvdynQheJ55P8XMxYSwjO9cd4vI7ydy9ZMcyS9tLZteXO7dU8Ds9PZuhvICVH8jyQpXtpNCWt9zHZvN47Q5npwtv9pO2K+4cJz3bbTbZbfJ8NgrW5W1nK3g28o5pM07sX9wyj4dhyG43ihzTK0IkAKBCWB6c5syZo4kTJ+rNN99Ujx49NH36dA0aNEhbtmxRw4YNi2z/448/atSoUXrmmWd08cUXa/bs2Ro+fLjWrl2rDh2KuRkpAOSz2wyF+PsoxL9i/9Nnmqayc13uEHU8P3QV/FzQE3ZyGEvPcmjHrt2Kio6RS4acTlO5LpdyXaacLlO5zvxnlyv/OW9Z4deFnx1Oz+UFlzyezGVKOU6X5JQq+CrIGscwdCKMFQpnpwtcBSGt8DJ74QBonAiAPjabZ+ArdA7J1K6dNiUu2CK73SabkRfkbIZky3/Oe23IMOTxumAb46TnU21jFLNP4e0L71/cNipmH1uhZflXn51ym6J1njiuofz9lV9LfrsY+ZfnuWsstLzwVXMFr22Ft/GSQOxymXKZppymKZdLcpp5/+7N/OfCy93bup/l/vnk5a787Z2Flhc+zonzqNB5zELnUaHj5S13b1toueTZhkb+gpPb+uR2LXadx/ei8DFOfMdLPH7+d67Id/G0x/c8hk71fT7p+C6nU5uTDYVtPSIfH7vy93S3beGvcJFvs8e60u1T+N/EqbYr+s+mNPt47nSq452qzrOjQuVbhkvcrWaYZmVNk1E6PXr0UPfu3fXqq69Kklwul2JjY3XnnXfqwQcfLLL9yJEjlZGRoa+//tq97B//+Ic6deqkN998s8TzpaamKjw8XCkpKQoLC6u4N1JODodD8+fP19ChQ+VLV63Xof29W2W3f8EvT7kFYeqUocslh9Pzda6z8L6er0+7n/s8+WHO5fJ4XXi7XGf+L2DOE7+IFXmYefu4XIWfXXKZynt2Kf89Sc7891b4F8aCbeGdThXITv5luPA2Hr84nxTIPH8pLmb/Uh43PzcrNS1NQcEhMuUZKgoHGKerUCg5KSABNd3qR/qrQai/pTWUJRtY2uOUk5OjX3/9VQ899JB7mc1mU//+/bVq1api91m1apUmTpzosWzQoEGaN29esdtnZ2crO/vElCWpqXn3l3E4HHI4rP9Ta0EN1aEWVD3a37tVVfvbJdltkmwq+B+vcuKv7wXh6sRfyU8OacWGuJJe5/+F3SPgnRz4zLygWHh5Tq5T27ZtV9Nmzdx/gnWZ+X/tN/PqPvFzweu89Z4/5613eWyf/yzP1wXbF/dcZD+X534F600V3f5EHUXPc6r9nK68ZZX551vTzB+Kf6oZIC1nSMfP9M7jpzl6oZ7Egh5UW/6zUWSZZLMZ7h5Bu03udTbDkC3/dUGvq92Wf4xCx8xbl39O9zIVOqfncSXPNjLdr0338rym83xdeL3M0607+Vie3znzFOcsul/eaxVznILXOul18fudOK7LNJWWmqaQ0NC8XptC31GPb6vHPBTmKZYXv71KsX15zlG6WgsvL357SXI6c+VwWPv/S2X5/2BLg9Phw4fldDoVGRnpsTwyMlKbN28udp+kpKRit09KSip2+2eeeUZTpkwpsnzRokUKCjr1TGBVLSEhweoSYCHa37vR/jWfIamgz7Asfztt31SSua3obxP5l7F5E/cvrSr8y2vhX57Ltr7IskLbq4zrTz7f6dYXPn/Reg2P9YYh2WSeuBRTyg8eJ3q1bCctK247m+G5bcHPNeKKReOkZ6+TbHUBlvr5+8VWl6DMzMxSb2v5GKfK9tBDD3n0UKWmpio2NlYDBw6sNpfqJSQkaMCAAVyq5YVof+9G+3s32t+70f7ejfavPgquRisNS4NT/fr1ZbfbdeDAAY/lBw4cUFRUVLH7REVFlWl7f39/+fsX/fufr69vtfqiVrd6ULVof+9G+3s32t+70f7ejfa3Xlk+f0svKvTz81PXrl21ZMkS9zKXy6UlS5aoZ8+exe7Ts2dPj+2lvMtcTrU9AAAAAJwpyy/VmzhxosaMGaNu3brpnHPO0fTp05WRkaHrr79eknTdddepUaNGeuaZZyRJd999t84//3y9+OKLuuiii/Txxx9rzZo1mjlzppVvAwAAAEAtZnlwGjlypA4dOqRJkyYpKSlJnTp10rfffuueAGLXrl2y2U50jPXq1UuzZ8/Wo48+qocfflitWrXSvHnzuIcTAAAAgEpjeXCSpPHjx2v8+PHFrlu2bFmRZVdeeaWuvPLKSq4KAAAAAPJ43w09AAAAAKCMCE4AAAAAUAKCEwAAAACUgOAEAAAAACUgOAEAAABACQhOAAAAAFACghMAAAAAlIDgBAAAAAAlIDgBAAAAQAl8rC6gqpmmKUlKTU21uJI8DodDmZmZSk1Nla+vr9XloIrR/t6N9vdutL93o/29G+1ffRRkgoKMcDpeF5zS0tIkSbGxsRZXAgAAAKA6SEtLU3h4+Gm3MczSxKtaxOVyad++fQoNDZVhGFaXo9TUVMXGxmr37t0KCwuzuhxUMdrfu9H+3o329260v3ej/asP0zSVlpammJgY2WynH8XkdT1ONptNjRs3trqMIsLCwviH48Vof+9G+3s32t+70f7ejfavHkrqaSrA5BAAAAAAUAKCEwAAAACUgOBkMX9/fz3++OPy9/e3uhRYgPb3brS/d6P9vRvt791o/5rJ6yaHAAAAAICyoscJAAAAAEpAcAIAAACAEhCcAAAAAKAEBCcAAAAAKAHByUKvvfaamjVrpoCAAPXo0UO//PKL1SWhCjzzzDPq3r27QkND1bBhQw0fPlxbtmyxuixY5Nlnn5VhGJowYYLVpaCK7N27V9dcc43q1aunwMBAdezYUWvWrLG6LFQBp9Opxx57TM2bN1dgYKBatmypJ554QszTVXstX75cw4YNU0xMjAzD0Lx58zzWm6apSZMmKTo6WoGBgerfv7/++usva4pFiQhOFpkzZ44mTpyoxx9/XGvXrlV8fLwGDRqkgwcPWl0aKtn333+vcePG6aefflJCQoIcDocGDhyojIwMq0tDFVu9erXeeustxcXFWV0KqsixY8fUu3dv+fr6asGCBfrjjz/04osvqk6dOlaXhirw3HPP6Y033tCrr76qTZs26bnnntPzzz+vGTNmWF0aKklGRobi4+P12muvFbv++eef1yuvvKI333xTP//8s4KDgzVo0CBlZWVVcaUoDaYjt0iPHj3UvXt3vfrqq5Ikl8ul2NhY3XnnnXrwwQctrg5V6dChQ2rYsKG+//579enTx+pyUEXS09PVpUsXvf7663ryySfVqVMnTZ8+3eqyUMkefPBBrVy5Uj/88IPVpcACF198sSIjI/XOO++4l40YMUKBgYH68MMPLawMVcEwDM2dO1fDhw+XlNfbFBMTo3vvvVf33XefJCklJUWRkZGaNWuWrr76agurRXHocbJATk6Ofv31V/Xv39+9zGazqX///lq1apWFlcEKKSkpkqS6detaXAmq0rhx43TRRRd5/HcAtd9XX32lbt266corr1TDhg3VuXNn/fvf/7a6LFSRXr16acmSJfrzzz8lSYmJiVqxYoWGDBlicWWwwvbt25WUlOTx/wPh4eHq0aMHvw9WUz5WF+CNDh8+LKfTqcjISI/lkZGR2rx5s0VVwQoul0sTJkxQ79691aFDB6vLQRX5+OOPtXbtWq1evdrqUlDFtm3bpjfeeEMTJ07Uww8/rNWrV+uuu+6Sn5+fxowZY3V5qGQPPvigUlNT1aZNG9ntdjmdTj311FMaPXq01aXBAklJSZJU7O+DBetQvRCcAAuNGzdOGzdu1IoVK6wuBVVk9+7duvvuu5WQkKCAgACry0EVc7lc6tatm55++mlJUufOnbVx40a9+eabBCcv8Mknn+i///2vZs+erfbt22vdunWaMGGCYmJiaH+gBuBSPQvUr19fdrtdBw4c8Fh+4MABRUVFWVQVqtr48eP19ddfa+nSpWrcuLHV5aCK/Prrrzp48KC6dOkiHx8f+fj46Pvvv9crr7wiHx8fOZ1Oq0tEJYqOjla7du08lrVt21a7du2yqCJUpfvvv18PPvigrr76anXs2FHXXnut7rnnHj3zzDNWlwYLFPzOx++DNQfByQJ+fn7q2rWrlixZ4l7mcrm0ZMkS9ezZ08LKUBVM09T48eM1d+5cfffdd2revLnVJaEKXXjhhdqwYYPWrVvnfnTr1k2jR4/WunXrZLfbrS4Rlah3795Fbj/w559/qmnTphZVhKqUmZkpm83zVy+73S6Xy2VRRbBS8+bNFRUV5fH7YGpqqn7++Wd+H6ymuFTPIhMnTtSYMWPUrVs3nXPOOZo+fboyMjJ0/fXXW10aKtm4ceM0e/ZsffnllwoNDXVfxxweHq7AwECLq0NlCw0NLTKeLTg4WPXq1WOcmxe455571KtXLz399NO66qqr9Msvv2jmzJmaOXOm1aWhCgwbNkxPPfWUmjRpovbt2+u3337T//3f/+mGG26wujRUkvT0dG3dutX9evv27Vq3bp3q1q2rJk2aaMKECXryySfVqlUrNW/eXI899phiYmLcM++hemE6cgu9+uqrmjZtmpKSktSpUye98sor6tGjh9VloZIZhlHs8nfffVdjx46t2mJQLfTt25fpyL3I119/rYceekh//fWXmjdvrokTJ+rmm2+2uixUgbS0ND322GOaO3euDh48qJiYGI0aNUqTJk2Sn5+f1eWhEixbtkz9+vUrsnzMmDGaNWuWTNPU448/rpkzZyo5OVnnnnuuXn/9dbVu3dqCalESghMAAAAAlIAxTgAAAABQAoITAAAAAJSA4AQAAAAAJSA4AQAAAEAJCE4AAAAAUAKCEwAAAACUgOAEAAAAACUgOAEAAABACQhOAACchmEYmjdvntVlAAAsRnACAFRbY8eOlWEYRR6DBw+2ujQAgJfxsboAAABOZ/DgwXr33Xc9lvn7+1tUDQDAW9HjBACo1vz9/RUVFeXxqFOnjqS8y+jeeOMNDRkyRIGBgWrRooU+++wzj/03bNigCy64QIGBgapXr55uueUWpaene2zzn//8R+3bt5e/v7+io6M1fvx4j/WHDx/WZZddpqCgILVq1UpfffWVe92xY8c0evRoNWjQQIGBgWrVqlWRoAcAqPkITgCAGu2xxx7TiBEjlJiYqNGjR+vqq6/Wpk2bJEkZGRkaNGiQ6tSpo9WrV+vTTz/V4sWLPYLRG2+8oXHjxumWW27Rhg0b9NVXX+mss87yOMeUKVN01VVXaf369Ro6dKhGjx6to0ePus//xx9/aMGCBdq0aZPeeOMN1a9fv+o+AABAlTBM0zStLgIAgOKMHTtWH374oQICAjyWP/zww3r44YdlGIZuu+02vfHGG+51//jHP9SlSxe9/vrr+ve//61//etf2r17t4KDgyVJ8+fP17Bhw7Rv3z5FRkaqUaNGuv766/Xkk08WW4NhGHr00Uf1xBNPSMoLYyEhIVqwYIEGDx6sSy65RPXr19d//vOfSvoUAADVAWOcAADVWr9+/TyCkSTVrVvX/XPPnj091vXs2VPr1q2TJG3atEnx8fHu0CRJvXv3lsvl0pYtW2QYhvbt26cLL7zwtDXExcW5fw4ODlZYWJgOHjwoSbr99ts1YsQIrV27VgMHDtTw4cPVq1evcr1XAED1RXACAFRrwcHBRS6dqyiBgYGl2s7X19fjtWEYcrlckqQhQ4Zo586dmj9/vhISEnThhRdq3LhxeuGFFyq8XgCAdRjjBACo0X766acir9u2bStJatu2rRITE5WRkeFev3LlStlsNp199tkKDQ1Vs2bNtGTJkjOqoUGDBhozZow+/PBDTZ8+XTNnzjyj4wEAqh96nAAA1Vp2draSkpI8lvn4+LgnYPj000/VrVs3nXvuufrvf/+rX375Re+8844kafTo0Xr88cc1ZswYTZ48WYcOHdKdd96pa6+9VpGRkZKkyZMn67bbblPDhg01ZMgQpaWlaeXKlbrzzjtLVd+kSZPUtWtXtW/fXtnZ2fr666/dwQ0AUHsQnAAA1dq3336r6Ohoj2Vnn322Nm/eLClvxruPP/5Yd9xxh6Kjo/XRRx+pXbt2kqSgoCAtXLhQd999t7p3766goCCNGDFC//d//+c+1pgxY5SVlaWXXnpJ9913n+rXr68rrrii1PX5+fnpoYce0o4dOxQYGKjzzjtPH3/8cQW8cwBAdcKsegCAGsswDM2dO1fDhw+3uhQAQC3HGCcAAAAAKAHBCQAAAABKwBgnAECNxdXmAICqQo8TAAAAAJSA4AQAAAAAJSA4AQAAAEAJCE4AAAAAUAKCEwAAAACUgOAEAAAAACUgOAEAAABACQhOAAAAAFCC/wfX9UMM836XFAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loss plot saved as 'training_validation_loss.png'\n",
      "[INFO] Basic HTML report started in 'report.html'\n",
      "Transformer model saved to best_transformer_model.pth\n",
      "[INFO] Loaded 10020 samples from food_predictions.csv.\n",
      "[INFO] Tokenizer set for the dataset.\n",
      "--- Computing Metrics (Precision, Recall, F1) ---\n",
      "[ERROR] Failed to compute or append Transformer metrics: object of type 'int' has no len()\n",
      "Training and evaluation completed.\n",
      "[INFO] Restored standard output/error streams.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pilchj\\AppData\\Local\\Temp\\ipykernel_53220\\511654419.py\", line 35, in train_transformer\n",
      "    precision, recall, f1 = compute_metrics(model, eval_dataset, tokenizer, device=device, batch_size=8)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pilchj\\AppData\\Local\\Temp\\ipykernel_53220\\3095488796.py\", line 116, in compute_metrics\n",
      "    for batch in eval_loader_func():\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pilchj\\AppData\\Local\\Temp\\ipykernel_53220\\3095488796.py\", line 111, in eval_loader_func\n",
      "    collated_batch = collate_fn(batch_data, tokenizer)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pilchj\\AppData\\Local\\Temp\\ipykernel_53220\\218196207.py\", line 112, in collate_fn\n",
      "    lengths = [len(x) for x in batch]\n",
      "               ^^^^^^\n",
      "TypeError: object of type 'int' has no len()\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Logic ---\n",
    "\n",
    "# Define filenames\n",
    "log_filename = \"log.txt\"\n",
    "csv_filename = \"food_predictions.csv\" # Assumed to be created or exist\n",
    "report_filename = \"report.html\"\n",
    "loss_plot_filename = \"training_validation_loss.png\"\n",
    "conf_matrix_filename = \"confusion_matrix.png\"\n",
    "model_save_filename = \"trained_model.pth\"\n",
    "\n",
    "# Clear log file at the start\n",
    "try:\n",
    "    with open(log_filename, \"w\") as f:\n",
    "        f.write(\"--- Log Start ---\\n\")\n",
    "    print(f\"[INFO] Cleared log file: {log_filename}\")\n",
    "except IOError as e:\n",
    "     print(f\"[WARN] Could not clear log file {log_filename}: {e}\")\n",
    "\n",
    "# Keep original stdout/stderr\n",
    "original_stdout = sys.stdout\n",
    "original_stderr = sys.stderr\n",
    "\n",
    "# Open log file in append mode and start Tee redirection\n",
    "try:\n",
    "    log_file = open(log_filename, \"a\", encoding='utf-8')\n",
    "    sys.stdout = Tee(original_stdout, log_file)\n",
    "    sys.stderr = Tee(original_stderr, log_file)\n",
    "\n",
    "    print(\"\\n--- Starting Main Process ---\")\n",
    "\n",
    "    # Load Dataset\n",
    "    print(f\"\\n[Phase 1] Loading dataset from '{csv_filename}'...\")\n",
    "    # Define dataset parameters\n",
    "    MAX_SEQ_LEN = 128 # Maximum sequence length for truncation\n",
    "    dset = FoodDataset(csv_filename, max_len=MAX_SEQ_LEN)\n",
    "\n",
    "    # Test Dataset Length (early check)\n",
    "    test_dataset_length(dset)\n",
    "    if len(dset) == 0:\n",
    "        raise ValueError(\"Dataset is empty. Cannot proceed. Check CSV file and path.\")\n",
    "\n",
    "    # Initialize and Train Tokenizer\n",
    "    print(\"\\n[Phase 2] Initializing and training tokenizer...\")\n",
    "    # Get raw text samples for tokenizer training\n",
    "    texts_for_tokenizer = [dset.samples[i] for i in range(len(dset))]\n",
    "    tokenizer = BPETokenizer(texts_for_tokenizer)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    print(f\"  Tokenizer vocabulary size: {vocab_size}\")\n",
    "\n",
    "    # Set Tokenizer for Dataset\n",
    "    dset.set_tokenizer(tokenizer)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dset, [int(len(dset) * 0.8), len(dset) - int(len(dset) * 0.8)])\n",
    "\n",
    "    # Create data loaders with collate_fn that includes the tokenizer\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, \n",
    "                                              collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, \n",
    "                                            collate_fn=lambda batch: collate_fn(batch, tokenizer))\n",
    "\n",
    "    # Calculate vocabulary size\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # Get maximum sequence length\n",
    "    max_length = max(len(sample) for sample in dset)\n",
    "    \n",
    "    # Define device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train the RNN model\n",
    "    train_rnn(train_loader, val_loader, vocab_size, max_length, tokenizer)\n",
    "\n",
    "    # Train the Transformer model\n",
    "    train_transformer(lambda: train_loader, lambda: val_loader, vocab_size, tokenizer)  # Pass loader functions\n",
    "\n",
    "    print(\"Training and evaluation completed.\")\n",
    "\n",
    "finally:\n",
    "    # Cleanup Logging: Always restore original stdout/stderr\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "    if 'log_file' in locals() and log_file:\n",
    "        log_file.close()\n",
    "    print(\"[INFO] Restored standard output/error streams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KVUpdQeAB8r-",
   "metadata": {
    "id": "KVUpdQeAB8r-"
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "puadYRhlB8r-",
   "metadata": {
    "id": "puadYRhlB8r-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cleanup] Skipping removal of 'food_predictions.csv' as it might contain real data.\n",
      "\n",
      "--- Cleaning up generated files ---\n",
      "Removed: log.txt\n",
      "Removed: report.html\n",
      "Removed: training_validation_loss.png\n",
      "Skipped (Not Found): confusion_matrix.png\n",
      "Skipped (Not Found): trained_model.pth\n",
      "--- Cleanup Finished --- \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files_to_remove = [\n",
    "    log_filename,\n",
    "    report_filename,\n",
    "    loss_plot_filename,\n",
    "    conf_matrix_filename,\n",
    "    model_save_filename,\n",
    "    # csv_filename # Be careful removing the CSV\n",
    "]\n",
    "\n",
    "# For simplicity, we'll just check if it's the default name and small size\n",
    "csv_check_path = \"food_predictions.csv\"\n",
    "if os.path.exists(csv_check_path):\n",
    "     # A simple check, might need refinement\n",
    "     if os.path.getsize(csv_check_path) < 1024:\n",
    "          print(f\"[Cleanup] Identified '{csv_check_path}', adding to removal list.\")\n",
    "          files_to_remove.append(csv_check_path)\n",
    "     else:\n",
    "           print(f\"[Cleanup] Skipping removal of '{csv_check_path}' as it might contain real data.\")\n",
    "\n",
    "print(\"\\n--- Cleaning up generated files ---\")\n",
    "for filename in files_to_remove:\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "            print(f\"Removed: {filename}\")\n",
    "        else:\n",
    "            print(f\"Skipped (Not Found): {filename}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error removing {filename}: {e}\")\n",
    "\n",
    "print(\"--- Cleanup Finished --- \")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
