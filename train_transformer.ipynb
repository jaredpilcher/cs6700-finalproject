{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KUMi2gukB8r3",
   "metadata": {
    "id": "KUMi2gukB8r3"
   },
   "source": [
    "# Food Description Transformer Model\n",
    "\n",
    "This notebook trains a Decoder-Only Transformer model to learn patterns from food descriptions and associated data stored in a CSV file. It performs the following steps:\n",
    "\n",
    "1.  **Imports & Setup:** Imports necessary libraries and checks for the availability of `torch` and `tokenizers`.\n",
    "2.  **Data Creation:** Creates a placeholder `food_predictions.csv` if it doesn't exist, allowing the notebook to run initially.\n",
    "3.  **Tokenizer:** Defines and trains a Byte-Pair Encoding (BPE) tokenizer on the input text data.\n",
    "4.  **Dataset:** Defines a custom dataset class to load, preprocess, and tokenize the data from the CSV.\n",
    "5.  **Model Definition:** Defines the `DecoderOnlyTransformer` architecture and a wrapper class (`DecoderOnlyModelWrapper`) that includes the model, optimizer, and loss function.\n",
    "6.  **Training Loop:** Implements the training process, including validation, loss calculation, early stopping, and plotting of training/validation loss.\n",
    "7.  **Testing:** Includes several test functions to verify basic model functionality (forward pass, handling different inputs).\n",
    "8.  **Evaluation:** Defines functions to compute and visualize a confusion matrix and calculate precision, recall, and F1-score.\n",
    "9.  **Main Execution:** Orchestrates the entire workflow: loading data, training the tokenizer, initializing the model, running tests, training the model, evaluating performance, and saving the trained model and reports.\n",
    "10. **Logging:** Redirects output to both the console and a `log.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EIDhxhvYB8r4",
   "metadata": {
    "id": "EIDhxhvYB8r4"
   },
   "source": [
    "## Imports and Setup\n",
    "\n",
    "Import necessary libraries. We check if `torch` and `tokenizers` are available and set flags accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MA7mwQdCB8r5",
   "metadata": {
    "id": "MA7mwQdCB8r5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import json # For writing the notebook itself\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "WJRvLBKjnEXf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJRvLBKjnEXf",
    "outputId": "dd375bfc-8879-4fb3-bbe9-2c93c69910f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.1.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.6.1)\n",
      "Requirement already satisfied: torch in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.21.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: optuna in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.2.1)\n",
      "Requirement already satisfied: oauth2client in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.1.3)\n",
      "Requirement already satisfied: gdown in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (5.2.0)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.9.5)\n",
      "Requirement already satisfied: lightning-bolts in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tokenizers) (0.27.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (1.14.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (2.0.36)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (0.22.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (0.4.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pytorch-lightning) (1.7.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pytorch-lightning) (0.14.3)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from lightning-bolts) (0.20.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from lightning-bolts) (2.19.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.11.11)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (5.29.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard>=2.9.1->lightning-bolts) (2.3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->optuna) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\pilchj\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.18.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib scikit-learn torch tokenizers seaborn optuna oauth2client gdown pytorch-lightning lightning-bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xh4XkPHgB8r5",
   "metadata": {
    "id": "Xh4XkPHgB8r5"
   },
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "_MS0nLBwB8r6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MS0nLBwB8r6",
    "outputId": "ae45bc96-ac1b-4d6f-dfc4-d0578c122c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found existing 'food_predictions.csv' locally. Using this file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import io\n",
    "\n",
    "csv_file_path = \"food_predictions.csv\"\n",
    "\n",
    "# Check if running in Colab\n",
    "is_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if is_colab:\n",
    "    # Colab-specific code\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        from google.colab import auth\n",
    "        from oauth2client.client import GoogleCredentials\n",
    "        \n",
    "        # Mount Google Drive\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Authenticate with Google Drive\n",
    "        auth.authenticate_user()\n",
    "        gauth = GoogleCredentials.get_application_default()\n",
    "        \n",
    "        # File ID from Google Drive URL\n",
    "        file_id = '1RFhhiSFwP0s6Y7y4yWCkegXlVaEYqH0B'  # Replace with the actual file ID\n",
    "        drive_file_path = f'/content/drive/MyDrive/food_predictions.csv'  # Update the path if necessary\n",
    "        \n",
    "        # Check if CSV exists locally, or download from Google Drive\n",
    "        if not os.path.exists(csv_file_path):\n",
    "            try:\n",
    "                # Download file from Google Drive\n",
    "                !gdown --id $file_id -O $csv_file_path\n",
    "                print(f\"[INFO] Downloaded '{csv_file_path}' from Google Drive.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to download from Google Drive: {e}\")\n",
    "                raise FileNotFoundError(f\"Could not download or find '{csv_file_path}'. Please ensure the file ID and path are correct.\")\n",
    "        else:\n",
    "            print(f\"[INFO] Found existing '{csv_file_path}' in Colab. Using this file.\")\n",
    "    except ImportError:\n",
    "        print(\"[WARN] Google Colab modules not available but detected in Colab environment.\")\n",
    "else:\n",
    "    # Local environment (VS Code, etc.)\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"[INFO] Found existing '{csv_file_path}' locally. Using this file.\")\n",
    "    else:\n",
    "        # Create a simple placeholder CSV if it doesn't exist\n",
    "        try:\n",
    "            with open(csv_file_path, 'w', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['food_description', 'contains_allergen', 'sweetener', 'fats_oils'])\n",
    "                writer.writerow(['Sample food item 1', 'True', 'sugar', 'olive oil'])\n",
    "                writer.writerow(['Sample food item 2', 'False', 'none', 'none'])\n",
    "            print(f\"[INFO] Created placeholder '{csv_file_path}' for local development.\")\n",
    "            print(\"[NOTE] Replace this with your actual data file for meaningful results.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to create placeholder CSV: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F7je9NvONofE",
   "metadata": {
    "id": "F7je9NvONofE"
   },
   "source": [
    "# Data Exploration\n",
    "\n",
    "Let's explore the data to understand the distribution and structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "B_wPg0XWNm_n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "B_wPg0XWNm_n",
    "outputId": "16fe13cc-2bcb-47bf-e199-e757897b0b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF Head\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_description</th>\n",
       "      <th>main_ingredient</th>\n",
       "      <th>sweetener</th>\n",
       "      <th>fat_or_oil</th>\n",
       "      <th>seasoning</th>\n",
       "      <th>allergens</th>\n",
       "      <th>contains_allergen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Creamy scrambled eggs, crispy bacon, and toast...</td>\n",
       "      <td>Eggs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bacon</td>\n",
       "      <td>Dairy, Eggs</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>omg best pizza i ever had: gooey melted mozzar...</td>\n",
       "      <td>Mozzarella</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tomato sauce, Crispy crust</td>\n",
       "      <td>Dairy, Wheat</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Warm, flaky croissants filled with buttery, ga...</td>\n",
       "      <td>Spinach</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Butter</td>\n",
       "      <td>Garlic</td>\n",
       "      <td>Almond, Dairy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decadent chocolate cake, moist and rich, serve...</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fresh catch of the day: pan-seared salmon with...</td>\n",
       "      <td>salmon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lemon, herb</td>\n",
       "      <td>Fish</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    food_description main_ingredient  \\\n",
       "0  Creamy scrambled eggs, crispy bacon, and toast...            Eggs   \n",
       "1  omg best pizza i ever had: gooey melted mozzar...      Mozzarella   \n",
       "2  Warm, flaky croissants filled with buttery, ga...         Spinach   \n",
       "3  Decadent chocolate cake, moist and rich, serve...       Chocolate   \n",
       "4  Fresh catch of the day: pan-seared salmon with...          salmon   \n",
       "\n",
       "  sweetener fat_or_oil                   seasoning      allergens  \\\n",
       "0       NaN        NaN                       Bacon    Dairy, Eggs   \n",
       "1       NaN        NaN  Tomato sauce, Crispy crust   Dairy, Wheat   \n",
       "2       NaN     Butter                      Garlic  Almond, Dairy   \n",
       "3     Sugar        NaN                         NaN          Dairy   \n",
       "4       NaN        NaN                 lemon, herb           Fish   \n",
       "\n",
       "  contains_allergen  \n",
       "0              true  \n",
       "1              true  \n",
       "2              True  \n",
       "3              true  \n",
       "4              true  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "print(\"DF Head\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "DIPOH1nLO-qi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "DIPOH1nLO-qi",
    "outputId": "55e28eca-215e-48ef-be8f-2000fead8e2b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title contains_allergen\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "df.groupby('contains_allergen').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
    "plt.gca().spines[['top', 'right',]].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zafgFwmrQm9C",
   "metadata": {
    "id": "zafgFwmrQm9C"
   },
   "source": [
    "Let's look at the factor data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kREqd0AVQjZT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kREqd0AVQjZT",
    "outputId": "e73fbadc-d5a9-4236-f819-68d59dfdf22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10020 entries, 0 to 10019\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   food_description   10020 non-null  object\n",
      " 1   main_ingredient    9865 non-null   object\n",
      " 2   sweetener          3666 non-null   object\n",
      " 3   fat_or_oil         3501 non-null   object\n",
      " 4   seasoning          7891 non-null   object\n",
      " 5   allergens          7548 non-null   object\n",
      " 6   contains_allergen  10020 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 548.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZOwWsWsfQ5lt",
   "metadata": {
    "id": "ZOwWsWsfQ5lt"
   },
   "source": [
    "Let's look at the number of unique values for each factor. Most noteworthy, the food_descriptions are almost all unique, which is expected. And all of the samples have a value in the contains_allergen column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "VEel3q1jQssf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "VEel3q1jQssf",
    "outputId": "6db628a7-89c5-405a-e30e-e4c74b9a4c80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_description</th>\n",
       "      <th>main_ingredient</th>\n",
       "      <th>sweetener</th>\n",
       "      <th>fat_or_oil</th>\n",
       "      <th>seasoning</th>\n",
       "      <th>allergens</th>\n",
       "      <th>contains_allergen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10020</td>\n",
       "      <td>9865</td>\n",
       "      <td>3666</td>\n",
       "      <td>3501</td>\n",
       "      <td>7891</td>\n",
       "      <td>7548</td>\n",
       "      <td>10020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9988</td>\n",
       "      <td>988</td>\n",
       "      <td>578</td>\n",
       "      <td>469</td>\n",
       "      <td>2778</td>\n",
       "      <td>1587</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Fried chicken tenders with honey mustard dippi...</td>\n",
       "      <td>chicken</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>Butter</td>\n",
       "      <td>none</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>895</td>\n",
       "      <td>497</td>\n",
       "      <td>721</td>\n",
       "      <td>285</td>\n",
       "      <td>1547</td>\n",
       "      <td>5821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         food_description main_ingredient  \\\n",
       "count                                               10020            9865   \n",
       "unique                                               9988             988   \n",
       "top     Fried chicken tenders with honey mustard dippi...         chicken   \n",
       "freq                                                    2             895   \n",
       "\n",
       "       sweetener fat_or_oil seasoning allergens contains_allergen  \n",
       "count       3666       3501      7891      7548             10020  \n",
       "unique       578        469      2778      1587                 5  \n",
       "top        Sugar     Butter      none     Dairy              true  \n",
       "freq         497        721       285      1547              5821  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tUX-yNtbSqRA",
   "metadata": {
    "id": "tUX-yNtbSqRA"
   },
   "source": [
    "There appears to be a majority of items that either don't have a predicted sweetner or fat/oil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "zOA15cyJRL5V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "zOA15cyJRL5V",
    "outputId": "309cbef9-8984-4111-ae8f-e2ceabe5924f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food_description        0\n",
       "main_ingredient       155\n",
       "sweetener            6354\n",
       "fat_or_oil           6519\n",
       "seasoning            2129\n",
       "allergens            2472\n",
       "contains_allergen       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tls0fdxQB8r6",
   "metadata": {
    "id": "tls0fdxQB8r6"
   },
   "source": [
    "## BPE Tokenizer\n",
    "\n",
    "This class handles tokenization. It uses the `tokenizers` library to train a Byte-Pair Encoding (BPE) model on the provided text data. BPE is effective at handling unknown words by breaking them down into subword units.\n",
    "\n",
    "If the `tokenizers` library is unavailable, it falls back to a simple character-level tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ksE9ApGLB8r6",
   "metadata": {
    "id": "ksE9ApGLB8r6"
   },
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\" Wrapper for BPE Tokenizer\"\"\"\n",
    "    def __init__(self, texts):\n",
    "        \"\"\" Initializes and trains the tokenizer.\n",
    "\n",
    "        Args:\n",
    "            texts (iterable): An iterable of strings to train the tokenizer on.\n",
    "        \"\"\"\n",
    "        # Use Hugging Face tokenizers library\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "        # Define special tokens, ensuring <pad> is handled correctly (often ID 0 by convention)\n",
    "        trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"])\n",
    "        # Train the tokenizer\n",
    "        self.tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "        # Ensure pad token ID is 0 if possible (it usually is by default with BpeTrainer)\n",
    "        pad_token_id = self.tokenizer.token_to_id(\"<pad>\")\n",
    "        if pad_token_id is None:\n",
    "              print(\"[WARN] <pad> token not found after training!\")\n",
    "              # Handle this case if necessary, maybe re-train or add manually\n",
    "        elif pad_token_id != 0:\n",
    "              print(f\"[WARN] <pad> token ID is {pad_token_id}, not 0. CrossEntropyLoss might need ignore_index adjustment if not using 0.\")\n",
    "        print(f\"[INFO] Trained BPE tokenizer. Vocab size: {self.tokenizer.get_vocab_size()}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Encode with BOS and EOS tokens implicitly handled via format string during encoding\n",
    "        bos_token = self.tokenizer.token_to_id(\"<bos>\")\n",
    "        eos_token = self.tokenizer.token_to_id(\"<eos>\")\n",
    "\n",
    "        encoded = self.tokenizer.encode(text) # Encode the main text\n",
    "\n",
    "        # Manually add BOS and EOS if not added automatically or if specific placement is needed\n",
    "        output_ids = []\n",
    "        if bos_token is not None:\n",
    "            output_ids.append(bos_token)\n",
    "        output_ids.extend(encoded.ids)\n",
    "        if eos_token is not None:\n",
    "              output_ids.append(eos_token)\n",
    "        return output_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\" Decodes a list of token IDs back into a string. \"\"\"\n",
    "        # Use the tokenizer's decode method\n",
    "        return self.tokenizer.decode(ids, skip_special_tokens=False) # Keep special tokens for clarity if needed\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\" Returns the size of the vocabulary. \"\"\"\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\" Converts a token string to its ID.\"\"\"\n",
    "        return self.tokenizer.token_to_id(token)\n",
    "\n",
    "    def id_to_token(self, id):\n",
    "        \"\"\" Converts a token ID to its string representation.\"\"\"\n",
    "        return self.tokenizer.id_to_token(id)\n",
    "\n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        \"\"\" Returns the ID of the padding token.\"\"\"\n",
    "        return self.token_to_id(\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z91DvXm3B8r6",
   "metadata": {
    "id": "z91DvXm3B8r6"
   },
   "source": [
    "## Dataset and Collation\n",
    "\n",
    "### `VectorizedFoodDataset` Class\n",
    "Reads the `food_prediction.csv` file. It takes the `food_description` column and vectorizes it. The `contains_allergen` is then encoded to use 1 and 0 instead of true and false.\n",
    "\n",
    "### `FoodDataset` Class\n",
    "Reads the `food_predictions.csv` file. It assumes the first column is `food_description` and concatenates all other columns into a structured `OUTPUT:` section. It then tokenizes this combined text.\n",
    "\n",
    "### `collate_fn` Function\n",
    "Takes a batch of sequences (lists of token IDs) from the dataset and pads them to the length of the longest sequence in the batch. It creates an attention mask to indicate which tokens are real and which are padding. This is necessary for batch processing in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uRy601ptB8r7",
   "metadata": {
    "id": "uRy601ptB8r7"
   },
   "outputs": [],
   "source": [
    "class VectorizedFoodDataset:\n",
    "    def __init__(self, csv_path, vectorizer):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "        descriptions = df[\"food_description\"].tolist()\n",
    "        bool_array = np.char.lower(df[\"contains_allergen\"].tolist()) == \"true\"\n",
    "\n",
    "        self.targets = np.array(bool_array, dtype=int)\n",
    "        self.features = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "    def __clean_text(text):\n",
    "        \"\"\"Cleans the input text by removing irrelevant characters and converting to lowercase.\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        return text\n",
    "\n",
    "\n",
    "class FoodDataset:\n",
    "    \"\"\" Loads and preprocesses data from the food CSV file.\"\"\"\n",
    "    def __init__(self, csv_path, max_len=128):\n",
    "        \"\"\" Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the input CSV file.\n",
    "            max_len (int): Maximum sequence length after tokenization. Longer sequences will be truncated.\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self._tokenizer = None # Tokenizer will be set later\n",
    "        self.max_len = max_len\n",
    "\n",
    "        try:\n",
    "            with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                rows = list(csv.DictReader(f))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[ERROR] CSV file not found at {csv_path}. Please ensure it exists.\")\n",
    "            rows = [] # Initialize with empty list to prevent further errors\n",
    "        except Exception as e:\n",
    "             print(f\"[ERROR] Failed to read CSV file {csv_path}: {e}\")\n",
    "             rows = []\n",
    "\n",
    "        for row in rows:\n",
    "            desc = row.get(\"food_description\", \"\") # Get food description, default to empty string if missing\n",
    "            other_cols = []\n",
    "            for k, v in row.items():\n",
    "                if k == \"food_description\": # Skip the description itself\n",
    "                    continue\n",
    "                other_cols.append(f\"{k}: {v}\") # Format other columns as 'key: value'\n",
    "\n",
    "            # Combine description and other info into a single string\n",
    "            output_section = \"\\n\".join(other_cols)\n",
    "            # Using a separator like ' OUTPUT:' helps the model distinguish input from target\n",
    "            self.samples.append(desc.strip() + \"\\nOUTPUT:\\n\" + output_section.strip())\n",
    "\n",
    "        if not self.samples:\n",
    "            print(\"[WARN] No samples loaded from the CSV. The dataset is empty.\")\n",
    "        else:\n",
    "             print(f\"[INFO] Loaded {len(self.samples)} samples from {csv_path}.\")\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        \"\"\" Sets the tokenizer to be used for encoding samples. \"\"\"\n",
    "        self._tokenizer = tokenizer\n",
    "        print(\"[INFO] Tokenizer set for the dataset.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the number of samples in the dataset. \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Retrieves a single sample by index.\n",
    "\n",
    "        If a tokenizer is set, it returns the tokenized and truncated sequence.\n",
    "        Otherwise, it returns the raw text sample.\n",
    "        \"\"\"\n",
    "        text = self.samples[idx]\n",
    "        if not self._tokenizer:\n",
    "            # Return raw text if tokenizer is not set (e.g., during tokenizer training)\n",
    "            return text\n",
    "\n",
    "        # Encode the text using the tokenizer\n",
    "        enc = self._tokenizer.encode(text)\n",
    "\n",
    "        # Truncate if the encoded sequence exceeds max_len\n",
    "        if len(enc) > self.max_len:\n",
    "            # Truncate, but ensure EOS token is preserved if it was originally included\n",
    "            eos_id = self._tokenizer.token_to_id(\"<eos>\")\n",
    "            enc = enc[:self.max_len -1 ] + [eos_id]\n",
    "\n",
    "        return enc\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\" Collates a batch of tokenized sequences into padded tensors. \"\"\"\n",
    "    if not batch:\n",
    "        # Handle empty batch case\n",
    "        return {\"input_ids\": torch.empty((0, 0), dtype=torch.long),\n",
    "                \"attention_mask\": torch.empty((0, 0), dtype=torch.long)}\n",
    "\n",
    "    # Check if the batch contains raw strings (shouldn't happen if used after tokenization)\n",
    "    if isinstance(batch[0], str):\n",
    "        print(\"[WARN] collate_fn received strings, expected token IDs.\")\n",
    "        return {\"input_ids\": batch, \"attention_mask\": [None]*len(batch)} # Basic handling for unexpected strings\n",
    "\n",
    "    # Determine the maximum length in the batch\n",
    "    lengths = [len(x) for x in batch]\n",
    "    max_batch_len = max(lengths) if lengths else 0\n",
    "\n",
    "    # Get the padding token ID from the first item's potential tokenizer (assuming uniform tokenizer)\n",
    "    # This is a bit indirect; ideally, pad_id would be passed explicitly.\n",
    "    # For this script structure, let's assume PAD ID is 0, which is common.\n",
    "    pad_token_id = 0 # Assuming PAD ID is 0, consistent with BPETrainer default and CrossEntropyLoss ignore_index\n",
    "\n",
    "    # Create padded tensors initialized with the padding token ID\n",
    "    padded = torch.full((len(batch), max_batch_len), pad_token_id, dtype=torch.long)\n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    mask = torch.zeros((len(batch), max_batch_len), dtype=torch.long)\n",
    "\n",
    "    # Fill the tensors with data from the batch\n",
    "    for i, seq in enumerate(batch):\n",
    "        seqlen = len(seq)\n",
    "        padded[i, :seqlen] = torch.tensor(seq, dtype=torch.long)\n",
    "        mask[i, :seqlen] = 1 # Mark the actual tokens in the mask\n",
    "\n",
    "    return {\"input_ids\": padded, \"attention_mask\": mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qyhZ1PgyB8r7",
   "metadata": {
    "id": "qyhZ1PgyB8r7"
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "### `DecoderOnlyTransformer`\n",
    "Implements a standard Transformer Decoder stack. It includes:\n",
    "-   An embedding layer (`nn.Embedding`) to convert token IDs into vectors.\n",
    "-   A stack of Transformer Decoder Layers (`nn.TransformerDecoderLayer`, `nn.TransformerDecoder`).\n",
    "-   A final linear layer (`nn.Linear`) to project the decoder output back to the vocabulary size, producing logits.\n",
    "-   It uses a causal mask (`generate_square_subsequent_mask`) to ensure that predictions for a position can only depend on previous positions.\n",
    "\n",
    "### `DecoderOnlyModelWrapper`\n",
    "A wrapper class that contains the `DecoderOnlyTransformer` model, the Adam optimizer, and the cross-entropy loss function (`nn.CrossEntropyLoss`). It provides methods for:\n",
    "-   Running the forward pass.\n",
    "-   Calculating the loss (using teacher forcing: predicting the next token based on the ground truth previous tokens).\n",
    "-   Accessing the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7vfVMbRqB8r7",
   "metadata": {
    "id": "7vfVMbRqB8r7"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    \"\"\" Simple Decoder-Only Transformer model. \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=5, dim_feedforward=512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # Embedding layer: maps token IDs to dense vectors\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        # Positional Encoding (Add this for better performance, simple example omits it)\n",
    "        self.pos_encoder = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Standard Transformer Decoder Layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                    dim_feedforward=dim_feedforward,\n",
    "                                                    batch_first=True) # Use batch_first=True\n",
    "        # Stack multiple decoder layers\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output layer: maps decoder output back to vocabulary size (logits)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\" Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len).\n",
    "            attention_mask (Tensor, optional): Mask for padding tokens. Shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch_size, seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        positions = torch.arange(0, x.size(1), dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        # Add positional encoding here if implemented\n",
    "        pos_emb = self.pos_encoder(positions)\n",
    "        emb = self.emb(x) + pos_emb\n",
    "\n",
    "        # 2. Generate Causal Mask\n",
    "        seq_len = x.size(1)\n",
    "        # Mask to prevent attention to future tokens\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "\n",
    "        # 3. Generate Padding Mask from attention_mask\n",
    "        # TransformerDecoderLayer expects mask where True indicates masking\n",
    "        # Our `attention_mask` is 1 for tokens, 0 for padding. Need to invert it.\n",
    "        if attention_mask is not None:\n",
    "            # Shape: (batch_size, seq_len)\n",
    "            padding_mask = (attention_mask == 0)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        # 4. Pass through Decoder\n",
    "        # Note: TransformerDecoder uses target (tgt) and memory. For decoder-only, memory is the same as target.\n",
    "        # `batch_first=True` means input shape is (batch, seq, feature)\n",
    "        dec_output = self.decoder(tgt=emb, memory=emb,\n",
    "                                tgt_mask=tgt_mask,\n",
    "                                tgt_key_padding_mask=padding_mask,\n",
    "                                memory_key_padding_mask=padding_mask) # Apply padding mask to memory as well\n",
    "\n",
    "        # 5. Final Linear Layer (Output Logits)\n",
    "        # Output shape: (batch_size, seq_len, vocab_size)\n",
    "        logits = self.fc(dec_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\" Counts the total number of trainable parameters in a PyTorch model. \"\"\"\n",
    "    # Ensure we are counting parameters of the actual nn.Module\n",
    "    actual_model = model.model if isinstance(model, DecoderOnlyModelWrapper) else model\n",
    "    if isinstance(actual_model, nn.Module):\n",
    "        return sum(p.numel() for p in actual_model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return 0 # Should not happen with real model\n",
    "\n",
    "class DecoderOnlyModelWrapper(nn.Module):\n",
    "    \"\"\" Wraps the Transformer model, optimizer, and loss function. \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=5, dim_feedforward=512, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.lr = lr\n",
    "        # Adam optimizer for training\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        # Cross Entropy Loss, ignoring padding token (assuming ID 0)\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        print(\"[INFO] Initialized PyTorch DecoderOnlyModelWrapper.\")\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\" Forward pass through the underlying model. \"\"\"\n",
    "        # Ensure input tensor is on the same device as the model\n",
    "        # device = next(self.model.parameters()).device # Get device from model parameters\n",
    "        # x = x.to(device)\n",
    "        # if attention_mask is not None:\n",
    "        #     attention_mask = attention_mask.to(device)\n",
    "        return self.model(x, attention_mask)\n",
    "\n",
    "    def compute_loss(self, batch):\n",
    "        \"\"\" Computes the loss for a given batch. \"\"\"\n",
    "        inp = batch[\"input_ids\"] # Shape: (batch_size, seq_len)\n",
    "        attn_mask = batch.get(\"attention_mask\") # Shape: (batch_size, seq_len) or None\n",
    "\n",
    "        device = next(self.model.parameters()).device\n",
    "        inp = inp.to(device)\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "        # Get model predictions (logits)\n",
    "        # Input `inp` has shape (batch, seq_len)\n",
    "        logits = self(inp, attention_mask=attn_mask) # Shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Prepare for loss calculation:\n",
    "        # Predict the token at step `t` based on tokens `0..t-1`\n",
    "        # Logits for prediction need to exclude the last token's output\n",
    "        # Target labels need to exclude the first token (BOS)\n",
    "        pred_logits = logits[:, :-1, :].contiguous() # Shape: (batch, seq_len-1, vocab_size)\n",
    "        target_ids = inp[:, 1:].contiguous()       # Shape: (batch, seq_len-1)\n",
    "\n",
    "        # Flatten logits and targets for CrossEntropyLoss\n",
    "        # Input shape for loss: (N*C), Target shape: (N)\n",
    "        # N = batch_size * (seq_len - 1), C = vocab_size\n",
    "        loss = self.crit(pred_logits.view(-1, pred_logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\" Returns the optimizer instance. \"\"\"\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IZH1gC8upsaK",
   "metadata": {
    "id": "IZH1gC8upsaK"
   },
   "source": [
    "### Adding Optuna hyperparameter tuning for the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "UhtF7LsYpj6R",
   "metadata": {
    "id": "UhtF7LsYpj6R"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial, train_loader, val_loader):\n",
    "    # Hyperparameters to tune\n",
    "    d_model = trial.suggest_categorical(\"d_model\", [64, 128, 256])\n",
    "    nhead = trial.suggest_categorical(\"nhead\", [2, 4, 8])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "    dim_feedforward = trial.suggest_categorical(\"dim_feedforward\", [256, 512, 1024])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)  # Logarithmic scale for learning rate\n",
    "\n",
    "    # Create model and data loaders\n",
    "    model = DecoderOnlyModelWrapper(vocab_size, d_model, nhead, num_layers, dim_feedforward, lr)\n",
    "\n",
    "    # Train the model and get validation loss\n",
    "    train_losses, val_losses = train_loop(model, train_loader, val_loader, epochs=10, device=device)\n",
    "\n",
    "    # Return the validation loss (or other metric) to minimize\n",
    "    return val_losses[-1]  # Last validation loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "djSjSBJqzoG_",
   "metadata": {
    "id": "djSjSBJqzoG_"
   },
   "source": [
    "## Create an RNN Model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "XqIS7ll2zVJf",
   "metadata": {
    "id": "XqIS7ll2zVJf"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RNNModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_units=128, dropout_rate=0.2, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Automatically saves the hyperparameters\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_units, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_units, 1) # For binary classification\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x) # Get the last hidden state\n",
    "        x = self.dropout(x[:, -1, :]) # Apply dropout to the last hidden state\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(logits.squeeze(), y.float()) # Binary cross-entropy loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(logits.squeeze(), y.float())\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2uI3gA-F2qTk",
   "metadata": {
    "id": "2uI3gA-F2qTk"
   },
   "source": [
    "## Create optuna objective function for the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "CO0mFOpK2plj",
   "metadata": {
    "id": "CO0mFOpK2plj"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "\n",
    "def rnn_objective(trial, train_loader, val_loader):\n",
    "    # Hyperparameters to tune\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [64, 128, 256])\n",
    "    hidden_units = trial.suggest_categorical(\"hidden_units\", [64, 128, 256])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # Create the model\n",
    "    model = RNNModel(vocab_size, embedding_dim, hidden_units, dropout_rate, lr)\n",
    "\n",
    "    # Create PyTorch Lightning Trainer with early stopping\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=False, mode=\"min\")\n",
    "    trainer = Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0, callbacks=[early_stop_callback], logger=False) # Disable default logger\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # Return the best validation loss (or other metric) to minimize\n",
    "    return trainer.callback_metrics[\"val_loss\"].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1D-Vp6rnB8r8",
   "metadata": {
    "id": "1D-Vp6rnB8r8"
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "The `train_loop` function orchestrates the training process over multiple epochs:\n",
    "-   Iterates through the training data loader.\n",
    "-   Computes the loss for each batch.\n",
    "-   Performs backpropagation and updates model weights using the optimizer.\n",
    "-   Optionally, evaluates the model on a validation set after each epoch.\n",
    "-   Implements early stopping: training halts if the validation loss doesn't improve for a specified number of `patience` epochs.\n",
    "-   Tracks and prints training and validation losses.\n",
    "-   Plots the losses and saves the plot as `training_validation_loss.png`.\n",
    "-   Generates a basic `report.html` containing the loss plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "yx-fhuEzB8r8",
   "metadata": {
    "id": "yx-fhuEzB8r8"
   },
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader_func, val_loader_func, epochs=30, device=\"cpu\", patience=5, report_filename='report.html', loss_plot_filename='training_validation_loss.png'):\n",
    "    \"\"\" Trains the model, performs validation, and handles early stopping. \"\"\"\n",
    "\n",
    "    # Move model to the specified device (CPU or GPU)\n",
    "    model.to(device)\n",
    "    print(f\"[INFO] Model moved to {device}.\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"--- Starting Training --- Epochs: {epochs}, Device: {device}, Patience: {patience} ---\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set model to training mode\n",
    "        total_train_loss = 0.0\n",
    "        train_steps = 0\n",
    "        train_loader = train_loader_func() # Get fresh iterator for the epoch\n",
    "\n",
    "        print(\"  Training...\")\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # training step\n",
    "            try:\n",
    "                # Calculate loss\n",
    "                loss = model.compute_loss(batch)\n",
    "                loss_item = loss.item()\n",
    "                total_train_loss += loss_item\n",
    "\n",
    "                # Backpropagation\n",
    "                model.get_optimizer().zero_grad() # Clear previous gradients\n",
    "                loss.backward()                   # Compute gradients\n",
    "                model.get_optimizer().step()      # Update weights\n",
    "            except Exception as e:\n",
    "                  print(f\"[ERROR] Exception during training step {i}: {e}\")\n",
    "                  # Optionally skip batch or break\n",
    "                  continue\n",
    "\n",
    "            train_steps += 1\n",
    "            if (i + 1) % 10 == 0: # Print progress every 10 steps\n",
    "                 print(f\"    Step {i+1}: current batch loss = {loss_item:.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / train_steps if train_steps > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"  Epoch {epoch+1} Average Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        val_loader = val_loader_func() # Get validation loader\n",
    "        if val_loader:\n",
    "            print(\"  Validating...\")\n",
    "            model.eval() # Set model to evaluation mode\n",
    "\n",
    "            total_val_loss = 0.0\n",
    "            val_steps = 0\n",
    "\n",
    "            # Use torch.no_grad() for validation to save memory and computation\n",
    "            maybe_no_grad = torch.no_grad()\n",
    "\n",
    "            with maybe_no_grad:\n",
    "                for i, val_batch in enumerate(val_loader):\n",
    "                    try:\n",
    "                      vloss = model.compute_loss(val_batch)\n",
    "                      vloss_item = vloss.item()\n",
    "                    except Exception as e:\n",
    "                        print(f\"[ERROR] Exception during validation step {i}: {e}\")\n",
    "                        vloss_item = float('nan') # Indicate error\n",
    "                        continue\n",
    "\n",
    "                    if not math.isnan(vloss_item):\n",
    "                         total_val_loss += vloss_item\n",
    "                         val_steps += 1\n",
    "                         if (i + 1) % 10 == 0:\n",
    "                             print(f\"    Validation Step {i+1}: current batch loss = {vloss_item:.4f}\")\n",
    "\n",
    "            avg_val_loss = total_val_loss / val_steps if val_steps > 0 else float('inf') # Handle case with no validation steps\n",
    "            val_losses.append(avg_val_loss)\n",
    "            print(f\"  Epoch {epoch+1} Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            # --- Early Stopping Check ---\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0 # Reset patience counter\n",
    "                print(f\"    New best validation loss: {best_val_loss:.4f}. Patience reset.\")\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(\"    Best model checkpoint saved.\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"    Validation loss did not improve. Patience: {patience_counter}/{patience}.\")\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\n--- Early Stopping triggered at epoch {epoch+1} ---\")\n",
    "                    break # Stop training\n",
    "        else:\n",
    "             print(\"  No validation loader provided, skipping validation.\")\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "    # --- Plotting and Reporting ---\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        if val_losses: # Only plot validation loss if it was calculated\n",
    "            plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(loss_plot_filename)\n",
    "        plt.show() # Display the plot in the notebook\n",
    "        print(f\"[INFO] Loss plot saved as '{loss_plot_filename}'\")\n",
    "\n",
    "        # Generate HTML Report (overwrite or create)\n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write('<html><head><title>Training Report</title></head><body>\\n')\n",
    "            f.write('<h1>Training Report</h1>\\n')\n",
    "            f.write('<h2>Training and Validation Loss</h2>\\n')\n",
    "            # Use relative path for image source\n",
    "            f.write(f'<img src=\"{os.path.basename(loss_plot_filename)}\" alt=\"Training and Validation Loss\"><br>\\n')\n",
    "            # Table for losses (optional)\n",
    "            f.write('<h3>Loss Values per Epoch</h3>\\n')\n",
    "            f.write('<table border=\"1\"><tr><th>Epoch</th><th>Train Loss</th>')\n",
    "            if val_losses:\n",
    "                f.write('<th>Validation Loss</th>')\n",
    "            f.write('</tr>\\n')\n",
    "            for i in range(len(train_losses)):\n",
    "                 f.write(f'<tr><td>{i+1}</td><td>{train_losses[i]:.4f}</td>')\n",
    "                 if i < len(val_losses):\n",
    "                     f.write(f'<td>{val_losses[i]:.4f}</td>')\n",
    "                 elif val_losses: # If val exists but stopped early\n",
    "                      f.write('<td>N/A</td>')\n",
    "                 f.write('</tr>\\n')\n",
    "            f.write('</table>\\n')\n",
    "            # Placeholder for closing tags, will be added to by evaluation metrics\n",
    "            # f.write('</body></html>')\n",
    "        print(f\"[INFO] Basic HTML report started in '{report_filename}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"[ERROR] Failed to generate plot or report: {e}\")\n",
    "\n",
    "    return train_losses, val_losses # Return recorded losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mbES9GnwB8r9",
   "metadata": {
    "id": "mbES9GnwB8r9"
   },
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "These functions evaluate the trained model's performance on a dataset (typically the validation or a separate test set).\n",
    "\n",
    "### `compute_confusion_matrix`\n",
    "-   Iterates through the evaluation dataset.\n",
    "-   Gets model predictions (logits) for each batch.\n",
    "-   Determines the predicted token ID (argmax) for each position.\n",
    "-   Compares predicted IDs against the true next token IDs (gold labels).\n",
    "-   Aggregates these comparisons into a confusion matrix (tensor).\n",
    "-   Visualizes the confusion matrix using `matplotlib` and saves it as `confusion_matrix.png`.\n",
    "\n",
    "### `compute_metrics`\n",
    "-   Similar to the confusion matrix computation, it iterates through the data and gets predictions vs. gold labels.\n",
    "-   Flattens the predictions and labels across all batches (ignoring padding).\n",
    "-   Uses `scikit-learn`'s `precision_score`, `recall_score`, and `f1_score` functions to calculate weighted metrics across all token classes.\n",
    "-   Returns the computed precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "uk6d4hZUB8r9",
   "metadata": {
    "id": "uk6d4hZUB8r9"
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(model, eval_dataset, tokenizer, device=\"cpu\", batch_size=8, conf_matrix_filename='confusion_matrix.png'):\n",
    "    \"\"\" Computes and saves the confusion matrix for token predictions. \"\"\"\n",
    "    print(\"--- Computing Confusion Matrix ---\")\n",
    "\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "    model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "    vocab_size = model.model.fc.out_features\n",
    "    # Initialize confusion matrix on CPU to avoid potential GPU memory issues for large vocabs\n",
    "    confusion = torch.zeros((vocab_size, vocab_size), dtype=torch.long, device='cpu')\n",
    "\n",
    "    # Create a simple data loader for the evaluation dataset\n",
    "    def eval_loader_func():\n",
    "        for i in range(0, len(eval_dataset), batch_size):\n",
    "            batch_data = eval_dataset[i : i + batch_size]\n",
    "            # Collate the batch manually or using the collate_fn\n",
    "            collated_batch = collate_fn(batch_data)\n",
    "            yield collated_batch\n",
    "\n",
    "    processed_tokens = 0\n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for batch in eval_loader_func():\n",
    "            inp = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch.get(\"attention_mask\", None)\n",
    "            if attn_mask is not None:\n",
    "                 attn_mask = attn_mask.to(device)\n",
    "\n",
    "            if inp.numel() == 0: continue # Skip empty batches\n",
    "\n",
    "            # Get model predictions\n",
    "            logits = model(inp, attention_mask=attn_mask) # (batch, seq_len, vocab_size)\n",
    "\n",
    "            # Get predicted token IDs (argmax along the vocab dimension)\n",
    "            # We predict the next token, so compare logits[:, :-1, :] with targets[:, 1:]\n",
    "            pred_logits = logits[:, :-1, :]\n",
    "            predicted_ids = pred_logits.argmax(dim=-1) # (batch, seq_len-1)\n",
    "\n",
    "            # Get gold standard (actual) token IDs\n",
    "            gold_ids = inp[:, 1:] # (batch, seq_len-1)\n",
    "\n",
    "            # Create a mask to ignore padding tokens in the gold standard\n",
    "            # Assuming pad_id is 0\n",
    "            mask = (gold_ids != tokenizer.pad_id) # (batch, seq_len-1)\n",
    "\n",
    "            # Flatten tensors and apply mask\n",
    "            gold_flat = torch.masked_select(gold_ids, mask)\n",
    "            pred_flat = torch.masked_select(predicted_ids, mask)\n",
    "\n",
    "            # Move tensors to CPU for confusion matrix update\n",
    "            gold_flat_cpu = gold_flat.cpu()\n",
    "            pred_flat_cpu = pred_flat.cpu()\n",
    "\n",
    "            # Update confusion matrix\n",
    "            for gold_tok, pred_tok in zip(gold_flat_cpu, pred_flat_cpu):\n",
    "                # Ensure indices are within bounds (should be guaranteed by vocab size)\n",
    "                if 0 <= gold_tok.item() < vocab_size and 0 <= pred_tok.item() < vocab_size:\n",
    "                     confusion[gold_tok.item(), pred_tok.item()] += 1\n",
    "                     processed_tokens += 1\n",
    "                else:\n",
    "                     print(f\"[WARN] Token ID out of bounds: Gold={gold_tok.item()}, Pred={pred_tok.item()}. Vocab size={vocab_size}. Skipping.\")\n",
    "            print(f\"  Processed batch. Total tokens considered so far: {processed_tokens}\")\n",
    "\n",
    "    print(f\"--- Confusion Matrix Calculation Complete. Total tokens analyzed: {processed_tokens} ---\")\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        # Display a subset if the vocab is too large\n",
    "        matrix_to_plot = confusion\n",
    "        max_display_size = 50 # Limit display size for readability\n",
    "        if vocab_size > max_display_size:\n",
    "            print(f\"[INFO] Vocab size ({vocab_size}) is large, plotting only top {max_display_size}x{max_display_size} part of the matrix.\")\n",
    "            matrix_to_plot = confusion[:max_display_size, :max_display_size]\n",
    "\n",
    "        plt.imshow(matrix_to_plot.log1p(), interpolation='nearest', cmap='Blues') # Use log scale for better visibility\n",
    "        plt.title(f'Confusion Matrix (Log Scale) - First {matrix_to_plot.shape[0]} Tokens')\n",
    "        plt.xlabel('Predicted Token ID')\n",
    "        plt.ylabel('Actual Token ID')\n",
    "        plt.colorbar()\n",
    "        # Add ticks if the matrix is small enough\n",
    "        if matrix_to_plot.shape[0] <= 20:\n",
    "             tick_marks = torch.arange(matrix_to_plot.shape[0])\n",
    "             plt.xticks(tick_marks, tick_marks)\n",
    "             plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(conf_matrix_filename)\n",
    "        plt.show()\n",
    "        print(f\"[INFO] Confusion matrix plot saved as '{conf_matrix_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to plot confusion matrix: {e}\")\n",
    "\n",
    "    # Optionally, return the matrix itself\n",
    "    return confusion\n",
    "\n",
    "\n",
    "def compute_metrics(model, eval_dataset, tokenizer, device=\"cpu\", batch_size=8):\n",
    "    \"\"\" Computes precision, recall, and F1 score for token predictions. \"\"\"\n",
    "    print(\"--- Computing Metrics (Precision, Recall, F1) ---\")\n",
    "\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "    model.to(device) # Ensure model is on the correct device\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Create a simple data loader for the evaluation dataset\n",
    "    def eval_loader_func():\n",
    "        for i in range(0, len(eval_dataset), batch_size):\n",
    "            batch_data = eval_dataset[i : i + batch_size]\n",
    "            collated_batch = collate_fn(batch_data)\n",
    "            yield collated_batch\n",
    "\n",
    "    processed_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader_func():\n",
    "            inp = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch.get(\"attention_mask\", None)\n",
    "            if attn_mask is not None:\n",
    "                 attn_mask = attn_mask.to(device)\n",
    "\n",
    "            if inp.numel() == 0: continue # Skip empty batches\n",
    "\n",
    "            logits = model(inp, attention_mask=attn_mask)\n",
    "            pred_logits = logits[:, :-1, :]\n",
    "            predicted_ids = pred_logits.argmax(dim=-1)\n",
    "            gold_ids = inp[:, 1:]\n",
    "            mask = (gold_ids != tokenizer.pad_id)\n",
    "\n",
    "            gold_flat = torch.masked_select(gold_ids, mask)\n",
    "            pred_flat = torch.masked_select(predicted_ids, mask)\n",
    "\n",
    "            # Append flattened results (move to CPU list for scikit-learn)\n",
    "            all_labels.extend(gold_flat.cpu().tolist())\n",
    "            all_preds.extend(pred_flat.cpu().tolist())\n",
    "            processed_tokens += len(gold_flat)\n",
    "            print(f\"  Processed batch. Total tokens considered so far: {processed_tokens}\")\n",
    "\n",
    "    print(f\"--- Metrics Calculation Complete. Total tokens analyzed: {processed_tokens} ---\")\n",
    "\n",
    "    if not all_labels: # Handle case where no valid tokens were processed\n",
    "        print(\"[WARN] No valid tokens found for metric calculation. Returning zero metrics.\")\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    # Compute metrics using scikit-learn\n",
    "    # 'weighted' average accounts for label imbalance\n",
    "    # `zero_division=0` handles cases where a class might have no predictions/labels\n",
    "    try:\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "        print(f\"  Calculated Metrics - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    except Exception as e:\n",
    "         print(f\"[ERROR] Failed to compute metrics using sklearn: {e}\")\n",
    "         precision, recall, f1 = 0.0, 0.0, 0.0 # Default to zero on error\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nwFZ_9VpB8r9",
   "metadata": {
    "id": "nwFZ_9VpB8r9"
   },
   "source": [
    "## Logging Utility\n",
    "\n",
    "The `Tee` class redirects `stdout` and `stderr` streams. Any output printed to the console will also be written to a specified log file (`log.txt` in this case). This is useful for keeping a persistent record of the entire process, including print statements, warnings, and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ULw2HiO9B8r9",
   "metadata": {
    "id": "ULw2HiO9B8r9"
   },
   "outputs": [],
   "source": [
    "class Tee:\n",
    "    \"\"\" Utility class to redirect stdout/stderr to both console and a file.\"\"\"\n",
    "    def __init__(self, console, logfile):\n",
    "        self.console = console\n",
    "        self.logfile = logfile\n",
    "\n",
    "    def write(self, data):\n",
    "        self.console.write(data)\n",
    "        self.logfile.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        # This flush method is needed for compatibility with sys.stdout\n",
    "        self.console.flush()\n",
    "        self.logfile.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HUBTQeEiB8r-",
   "metadata": {
    "id": "HUBTQeEiB8r-"
   },
   "source": [
    "## Main Execution Block\n",
    "\n",
    "This is the main part of the notebook that orchestrates the entire process:\n",
    "\n",
    "1.  **Setup Logging:** Redirects output using the `Tee` class to `log.txt`.\n",
    "2.  **Load Dataset:** Creates an instance of `FoodDataset` using `food_predictions.csv`.\n",
    "3.  **Test Dataset:** Runs `test_dataset_length`.\n",
    "4.  **Initialize & Train Tokenizer:** Creates `BPETokenizer` and trains it on the dataset samples.\n",
    "5.  **Set Tokenizer for Dataset:** Assigns the trained tokenizer to the dataset instance.\n",
    "6.  **Data Split:** Splits dataset indices into training and validation sets (using a simple 90/10 split here).\n",
    "7.  **Define Data Loaders:** Creates functions (`train_loader`, `val_loader`) that generate batches of data using the specified indices and the `collate_fn`.\n",
    "8.  **Initialize Model:** Creates an instance of `DecoderOnlyModelWrapper` with the vocabulary size from the tokenizer and hyperparameters.\n",
    "9.  **Set Device:** Determines whether to use CUDA (GPU) if available, otherwise CPU.\n",
    "10. **Count Parameters & Run Tests:** Prints the number of trainable parameters and runs the basic model tests.\n",
    "11. **Train Model:** Calls the `train_loop` function to train the model.\n",
    "12. **Evaluate Model:** After training, calls `compute_confusion_matrix` and `compute_metrics` on the validation set (used here as a test set).\n",
    "13. **Save Model & Report:** Saves the trained model's state dictionary to `trained_model.pth` and appends the evaluation metrics to `report.html`.\n",
    "14. **Cleanup Logging:** Restores the original `stdout` and `stderr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "M32LQqt4ngVT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M32LQqt4ngVT",
    "outputId": "74f96f28-7463-41d0-d3f4-e368e3659c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cleared log file: log.txt\n"
     ]
    }
   ],
   "source": [
    "# Define filenames\n",
    "log_filename = \"log.txt\"\n",
    "csv_filename = \"food_predictions.csv\" # Assumed to be created or exist\n",
    "report_filename = \"report.html\"\n",
    "loss_plot_filename = \"training_validation_loss.png\"\n",
    "conf_matrix_filename = \"confusion_matrix.png\"\n",
    "model_save_filename = \"trained_model.pth\"\n",
    "\n",
    "# Clear log file at the start\n",
    "try:\n",
    "    with open(log_filename, \"w\") as f:\n",
    "        f.write(\"--- Log Start ---\\n\")\n",
    "    print(f\"[INFO] Cleared log file: {log_filename}\")\n",
    "except IOError as e:\n",
    "     print(f\"[WARN] Could not clear log file {log_filename}: {e}\")\n",
    "\n",
    "# Keep original stdout/stderr\n",
    "original_stdout = sys.stdout\n",
    "original_stderr = sys.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cAOwJam9d6o_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAOwJam9d6o_",
    "outputId": "c9d3e483-fb1e-4440-da8f-a2a477b40d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Main Process ---\n",
      "\n",
      "[Phase 1] Loading dataset from 'food_predictions.csv'...\n",
      "\n",
      "[Phase 2] Vectorizing dataset...\n",
      "[INFO] Dataset length: 10020\n",
      "\n",
      "[Phase 3] Check vectorized data\n",
      "  Features: <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 13 stored elements and shape (1, 2214)>\n",
      "  Coords\tValues\n",
      "  (0, 486)\t0.18906573466080354\n",
      "  (0, 1692)\t0.36280099125122656\n",
      "  (0, 656)\t0.34251137829537215\n",
      "  (0, 499)\t0.18004423698562447\n",
      "  (0, 103)\t0.2591121087630078\n",
      "  (0, 52)\t0.10731739598299057\n",
      "  (0, 2003)\t0.25985567632738704\n",
      "  (0, 106)\t0.49475648317838145\n",
      "  (0, 2162)\t0.10547004900525823\n",
      "  (0, 1742)\t0.2961612146933974\n",
      "  (0, 1338)\t0.1997704180635649\n",
      "  (0, 798)\t0.22337156390945978\n",
      "  (0, 1875)\t0.3213970858820761\n",
      "  Target: 1\n",
      "\n",
      "[Phase 4a] Splitting data into training and testing sets...\n",
      "  Training Features shape: (8016, 2214)\n",
      "  Training Targets shape: (8016,)\n",
      "  Testing Features shape: (2004, 2214)\n",
      "  Testing Targets shape: (2004,)\n",
      "\n",
      "[Phase 4b] Scaling features for kNN...\n",
      "\n",
      "[Phase 5a] Hyperparameter tuning for Random Forest...\n"
     ]
    }
   ],
   "source": [
    "def test_dataset_length(dataset):\n",
    "    \"\"\" A simple test function to check the length of the dataset. \"\"\"\n",
    "    print(f\"[INFO] Dataset length: {len(dataset)}\")\n",
    "\n",
    "# Open log file in append mode and start Tee redirection\n",
    "try:\n",
    "    log_file = open(log_filename, \"a\", encoding='utf-8')\n",
    "    sys.stdout = Tee(original_stdout, log_file)\n",
    "    sys.stderr = Tee(original_stderr, log_file)\n",
    "\n",
    "    print(\"\\n--- Starting Main Process ---\")\n",
    "\n",
    "    # 1. Load Dataset\n",
    "    print(f\"\\n[Phase 1] Loading dataset from '{csv_filename}'...\")\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vector_dset = VectorizedFoodDataset(csv_filename, vectorizer)\n",
    "\n",
    "    # 2. Test Vectorized Dataset Length\n",
    "    print(f\"\\n[Phase 2] Vectorizing dataset...\")\n",
    "    test_dataset_length(vector_dset)\n",
    "    if len(vector_dset) == 0:\n",
    "        raise ValueError(\"Dataset is empty. Cannot proceed. Check CSV file and path.\")\n",
    "\n",
    "    # 3. Print Features and Targets for first sample\n",
    "    print(f\"\\n[Phase 3] Check vectorized data\")\n",
    "    features, target = vector_dset[0]\n",
    "    print(f\"  Features: {features}\")\n",
    "    print(f\"  Target: {target}\")\n",
    "\n",
    "    # 4a. Split data into training and testing sets\n",
    "    print(\"\\n[Phase 4a] Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        vector_dset.features, vector_dset.targets, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"  Training Features shape: {X_train.shape}\")\n",
    "    print(f\"  Training Targets shape: {y_train.shape}\")\n",
    "    print(f\"  Testing Features shape: {X_test.shape}\")\n",
    "    print(f\"  Testing Targets shape: {y_test.shape}\")\n",
    "\n",
    "    # 4b. Scale features for kNN\n",
    "    print(\"\\n[Phase 4b] Scaling features for kNN...\")\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 5a. Initialize and train Random Forest\n",
    "    random_forest_param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    print(\"\\n[Phase 5a] Hyperparameter tuning for Random Forest...\")\n",
    "    random_forest_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), random_forest_param_grid, cv=5, scoring='accuracy')\n",
    "    random_forest_grid_search.fit(X_train, y_train)\n",
    "    random_forest_model = random_forest_grid_search.best_estimator_\n",
    "    print(f\"Best Random Forest parameters: {random_forest_grid_search.best_params_}\")\n",
    "\n",
    "    # 5b. Initialize and train kNN\n",
    "    kNN_param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "\n",
    "    print(\"\\n[Phase 5b] Hyperparameter tuning for K-Nearest Neighbors...\")\n",
    "    kNN_grid_search = GridSearchCV(KNeighborsClassifier(), kNN_param_grid, cv=5, scoring='accuracy')\n",
    "    kNN_grid_search.fit(X_train_scaled, y_train)\n",
    "    kNN_model = kNN_grid_search.best_estimator_\n",
    "    print(f\"Best kNN parameters: {kNN_grid_search.best_params_}\")\n",
    "\n",
    "    # 6a. Make predictions on testing set for Random Forest\n",
    "    print(\"\\n[Phase 6a] Running prediction on Random Forest...\")\n",
    "    random_forest_y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "    # 6b. Make predictions on testing set for kNN\n",
    "    print(\"\\n[Phase 6b] Running prediction on K-Nearest Neighbors...\")\n",
    "    kNN_y_pred = kNN_model.predict(X_test_scaled)\n",
    "\n",
    "    # 7a. Evaluate performance of Random Forest\n",
    "    print(\"\\n[Phase 7a] Evaluate performance of Random Forest...\")\n",
    "    random_forest_accuracy = accuracy_score(y_test, random_forest_y_pred)\n",
    "    print(f\"Random Forest Accuracy: {random_forest_accuracy}\")\n",
    "    print(classification_report(y_test, random_forest_y_pred))\n",
    "\n",
    "    # 7b. Evaluate performance of kNN\n",
    "    print(\"\\n[Phase 7b] Evaluate performance of K-Nearest Neighbors...\")\n",
    "    kNN_accuracy = accuracy_score(y_test, kNN_y_pred)\n",
    "    print(f\"K-Nearest Neighbor Accuracy: {kNN_accuracy}\")\n",
    "    print(classification_report(y_test, kNN_y_pred))\n",
    "\n",
    "finally:\n",
    "    # 13. Cleanup Logging: Always restore original stdout/stderr\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "    if 'log' in locals() and log_file:\n",
    "        log_file.close()\n",
    "    print(\"[INFO] Restored standard output/error streams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rxHDwvjB7OvK",
   "metadata": {
    "id": "rxHDwvjB7OvK"
   },
   "source": [
    "## Function for training the RNN, including hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "no0KBrz-65v6",
   "metadata": {
    "id": "no0KBrz-65v6"
   },
   "outputs": [],
   "source": [
    "def train_rnn(train_loader, val_loader, vocab_size, max_length):\n",
    "    \"\"\"Trains the RNN model, saves the best model, and evaluates its performance.\"\"\"\n",
    "\n",
    "    # Perform hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: rnn_objective(trial, train_loader, val_loader), n_trials=100)\n",
    "    best_params = study.best_params\n",
    "    print(f\"Best hyperparameters for RNN: {best_params}\")\n",
    "\n",
    "    # Train the final RNN model with the best hyperparameters\n",
    "    best_rnn_model = RNNModel(vocab_size, **best_params)\n",
    "    trainer = Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0)\n",
    "    trainer.fit(best_rnn_model, train_loader, val_loader)\n",
    "\n",
    "    # Save the best RNN model\n",
    "    torch.save(best_rnn_model.state_dict(), 'best_rnn_model.pth')\n",
    "    print(\"Best RNN model saved to best_rnn_model.pth\")\n",
    "\n",
    "    # Evaluate the RNN model\n",
    "    best_rnn_model.eval()  # Set to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x, y = batch\n",
    "            logits = best_rnn_model(x)\n",
    "            preds = torch.round(torch.sigmoid(logits)).squeeze()  # Round predictions to 0/1\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1-score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"RNN Evaluation Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Append evaluation metrics to HTML report\n",
    "    try:\n",
    "        with open('report.html', 'a') as f:\n",
    "            f.write('<h2>RNN Evaluation Metrics</h2>\\n')\n",
    "            f.write(f'<p>Accuracy: {accuracy:.4f}</p>\\n')\n",
    "            f.write(f'<p>Precision: {precision:.4f}</p>\\n')\n",
    "            f.write(f'<p>Recall: {recall:.4f}</p>\\n')\n",
    "            f.write(f'<p>F1-score: {f1:.4f}</p>\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to append RNN metrics to report: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zURs9Zde7ayx",
   "metadata": {
    "id": "zURs9Zde7ayx"
   },
   "source": [
    "## Function for training the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ao_MzbdZ672R",
   "metadata": {
    "id": "Ao_MzbdZ672R"
   },
   "outputs": [],
   "source": [
    "def train_transformer(train_loader_func, val_loader_func, vocab_size):\n",
    "    \"\"\"Trains the Transformer model, saves the best model, and evaluates its performance.\"\"\"\n",
    "\n",
    "    # Optuna Integration for transformer\n",
    "    study = optuna.create_study(direction=\"minimize\")  # Create study\n",
    "    study.optimize(lambda trial: objective(trial, train_loader_func, val_loader_func), n_trials=100)  # Optimize\n",
    "    best_params = study.best_params  # Get best params\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "    best_model = DecoderOnlyModelWrapper(vocab_size, **best_params)  # Create model with best params\n",
    "    best_model.to(device)  # Move model to device\n",
    "\n",
    "    # Create and train the Transformer model\n",
    "    model = DecoderOnlyModelWrapper(vocab_size)  # Use default or tuned hyperparameters\n",
    "    train_losses, val_losses = train_loop(model, train_loader_func, val_loader_func, epochs=30, device=device, patience=5)\n",
    "\n",
    "    # Save the best Transformer model\n",
    "    torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "    print(\"Best Transformer model saved to best_transformer_model.pth\")\n",
    "\n",
    "    # Evaluate the Transformer model (using compute_metrics)\n",
    "    eval_dataset = FoodDataset(csv_file_path)  # Create an evaluation dataset\n",
    "    eval_dataset.set_tokenizer(tokenizer)\n",
    "\n",
    "    precision, recall, f1 = compute_metrics(model, eval_dataset, tokenizer, device=device, batch_size=8)\n",
    "\n",
    "    print(f\"Transformer Evaluation Metrics:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-score: {f1:.4f}\")\n",
    "\n",
    "    # Append evaluation metrics to HTML report\n",
    "    try:\n",
    "        with open('report.html', 'a') as f:\n",
    "            f.write('<h2>Transformer Evaluation Metrics</h2>\\n')\n",
    "            f.write(f'<p>Precision: {precision:.4f}</p>\\n')\n",
    "            f.write(f'<p>Recall: {recall:.4f}</p>\\n')\n",
    "            f.write(f'<p>F1-score: {f1:.4f}</p>\\n')\n",
    "            f.write('</body></html>')  # Close the HTML tags\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to append Transformer metrics to report: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RN-Ev7O4_PHc",
   "metadata": {
    "id": "RN-Ev7O4_PHc"
   },
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jpr4ww0sB8r-",
   "metadata": {
    "id": "Jpr4ww0sB8r-"
   },
   "outputs": [],
   "source": [
    "# --- Main Execution Logic ---\n",
    "\n",
    "# Define filenames\n",
    "log_filename = \"log.txt\"\n",
    "csv_filename = \"food_predictions.csv\" # Assumed to be created or exist\n",
    "report_filename = \"report.html\"\n",
    "loss_plot_filename = \"training_validation_loss.png\"\n",
    "conf_matrix_filename = \"confusion_matrix.png\"\n",
    "model_save_filename = \"trained_model.pth\"\n",
    "\n",
    "# Clear log file at the start\n",
    "try:\n",
    "    with open(log_filename, \"w\") as f:\n",
    "        f.write(\"--- Log Start ---\\n\")\n",
    "    print(f\"[INFO] Cleared log file: {log_filename}\")\n",
    "except IOError as e:\n",
    "     print(f\"[WARN] Could not clear log file {log_filename}: {e}\")\n",
    "\n",
    "# Keep original stdout/stderr\n",
    "original_stdout = sys.stdout\n",
    "original_stderr = sys.stderr\n",
    "\n",
    "# Open log file in append mode and start Tee redirection\n",
    "try:\n",
    "    log_file = open(log_filename, \"a\", encoding='utf-8')\n",
    "    sys.stdout = Tee(original_stdout, log_file)\n",
    "    sys.stderr = Tee(original_stderr, log_file)\n",
    "\n",
    "    print(\"\\n--- Starting Main Process ---\")\n",
    "\n",
    "    # Load Dataset\n",
    "    print(f\"\\n[Phase 1] Loading dataset from '{csv_filename}'...\")\n",
    "    # Define dataset parameters\n",
    "    MAX_SEQ_LEN = 128 # Maximum sequence length for truncation\n",
    "    dset = FoodDataset(csv_filename, max_len=MAX_SEQ_LEN)\n",
    "\n",
    "    # Test Dataset Length (early check)\n",
    "    test_dataset_length(dset)\n",
    "    if len(dset) == 0:\n",
    "        raise ValueError(\"Dataset is empty. Cannot proceed. Check CSV file and path.\")\n",
    "\n",
    "    # Initialize and Train Tokenizer\n",
    "    print(\"\\n[Phase 2] Initializing and training tokenizer...\")\n",
    "    # Get raw text samples for tokenizer training\n",
    "    texts_for_tokenizer = [dset.samples[i] for i in range(len(dset))]\n",
    "    tokenizer = BPETokenizer(texts_for_tokenizer)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    print(f\"  Tokenizer vocabulary size: {vocab_size}\")\n",
    "\n",
    "    # Set Tokenizer for Dataset\n",
    "    dset.set_tokenizer(tokenizer)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dset, [int(len(dset) * 0.8), len(dset) - int(len(dset) * 0.8)])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Calculate vocabulary size\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # Get maximum sequence length\n",
    "    max_length = max(len(sample) for sample in dataset)\n",
    "\n",
    "    # Train the RNN model\n",
    "    train_rnn(train_loader, val_loader, vocab_size, max_length)\n",
    "\n",
    "    # Train the Transformer model\n",
    "    train_transformer(lambda: train_loader, lambda: val_loader, vocab_size)  # Pass loader functions\n",
    "\n",
    "    print(\"Training and evaluation completed.\")\n",
    "\n",
    "finally:\n",
    "    # Cleanup Logging: Always restore original stdout/stderr\n",
    "    sys.stdout = original_stdout\n",
    "    sys.stderr = original_stderr\n",
    "    if 'log_file' in locals() and log_file:\n",
    "        log_file.close()\n",
    "    print(\"[INFO] Restored standard output/error streams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KVUpdQeAB8r-",
   "metadata": {
    "id": "KVUpdQeAB8r-"
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "puadYRhlB8r-",
   "metadata": {
    "id": "puadYRhlB8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_to_remove = [\n",
    "    log_filename,\n",
    "    report_filename,\n",
    "    loss_plot_filename,\n",
    "    conf_matrix_filename,\n",
    "    model_save_filename,\n",
    "    # csv_filename # Be careful removing the CSV\n",
    "]\n",
    "\n",
    "# For simplicity, we'll just check if it's the default name and small size\n",
    "csv_check_path = \"food_predictions.csv\"\n",
    "if os.path.exists(csv_check_path):\n",
    "     # A simple check, might need refinement\n",
    "     if os.path.getsize(csv_check_path) < 1024:\n",
    "          print(f\"[Cleanup] Identified '{csv_check_path}', adding to removal list.\")\n",
    "          files_to_remove.append(csv_check_path)\n",
    "     else:\n",
    "           print(f\"[Cleanup] Skipping removal of '{csv_check_path}' as it might contain real data.\")\n",
    "\n",
    "print(\"\\n--- Cleaning up generated files ---\")\n",
    "for filename in files_to_remove:\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "            print(f\"Removed: {filename}\")\n",
    "        else:\n",
    "            print(f\"Skipped (Not Found): {filename}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error removing {filename}: {e}\")\n",
    "\n",
    "print(\"--- Cleanup Finished --- \")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
